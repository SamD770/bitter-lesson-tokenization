{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have a random model that we counterfactually force to gate at certain times. What does the advantage of doing so look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from clean_code.flexible_bitter_llm import FlexibleBitterLLM, Gemma2RotaryEmbedding\n",
    "from clean_code.bitter_llm import RandomGater, CausalGemmaMiniBitterLLM\n",
    "torch.serialization.add_safe_globals([CausalGemmaMiniBitterLLM, nn.modules.sparse.Embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlexibleBitterLLM(\n",
       "  (embedding): Embedding(256, 512)\n",
       "  (down_layers): ModuleList(\n",
       "    (0-1): 2 x OptimizedModule(\n",
       "      (_orig_mod): Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (down_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_layers): ModuleList(\n",
       "    (0-5): 6 x OptimizedModule(\n",
       "      (_orig_mod): Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_layers): ModuleList(\n",
       "    (0-1): 2 x OptimizedModule(\n",
       "      (_orig_mod): Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (down_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((512,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (down_layer_gate): RandomGater()\n",
       "  (downsampler): AverageTokenDownsampler()\n",
       "  (rotary_emb): Gemma2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = torch.load(\"bitter-llm-exp15.pt\", weights_only=False)\n",
    "my_model.__class__ = FlexibleBitterLLM\n",
    "my_model.attn_implementation = \"flash_attention_2\"\n",
    "my_model.rotary_emb = Gemma2RotaryEmbedding(my_model.byte_layer_config)\n",
    "for l in [*my_model.down_layers, *my_model.mid_layers, *my_model.up_layers]:\n",
    "    l.self_attn.attn_logit_softcapping = 50.0\n",
    "\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.bbc.com/news/articles/crrz44d7v08o\n",
    "test_string = \"\"\"This BBC interview with Prince Harry will become one of those famous moments when television collides with the world of the royals.\n",
    "\n",
    "It was like an emotional avalanche. It began with some stones being kicked over with questions about security and then the interview turned into a spectacular release of what seemed to be a rolling mountain of pent-up frustration and a poignant sense of separation.\n",
    "\n",
    "The starting point was Prince Harry's defeat in the courts as he sought to overturn a downgrading of his security in the UK. He seemed wounded. Had he decided it was time to have his say? And then really say some more?\n",
    "\n",
    "A conversation about security was suddenly becoming about a whole range of insecurities.\n",
    "\"\"\"\n",
    "test_batch = byte5_tokenizer.encode(test_string, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    base_out = my_model(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_down_gate_samples = base_out[\"down_gate_samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 710]), torch.Size([1, 710]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate_of_interest = 100\n",
    "base_down_gate_samples_mask = torch.cat([torch.ones(gate_of_interest, dtype=torch.bool, device=device), torch.zeros(test_batch.shape[1]-gate_of_interest, dtype=torch.bool, device=device)], dim=0).unsqueeze(0)\n",
    "base_down_gate_samples.shape, base_down_gate_samples_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_out = my_model(\n",
    "        test_batch, \n",
    "        prescribed_down_gate_samples=base_down_gate_samples, \n",
    "        down_gate_mask=base_down_gate_samples_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "         0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "         0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_out[\"down_gate_samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "         0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out[\"down_gate_samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_diff = base_out[\"logits\"] - test_out[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.2212e-02, -5.1270e-03,  1.6527e-01,  ...,  3.0318e-01,\n",
       "           4.0684e-01,  3.9837e-01],\n",
       "         [-3.9117e-01, -1.2651e-01,  1.4279e-01,  ...,  2.8289e-01,\n",
       "           3.7215e-01,  4.0109e-01],\n",
       "         [-2.8314e-01, -2.9463e-01, -9.6945e-02,  ..., -3.4163e-02,\n",
       "          -2.7750e-02, -2.9251e-02],\n",
       "         ...,\n",
       "         [ 3.1815e-03,  5.2868e-02,  4.8299e-01,  ...,  6.6721e-01,\n",
       "           7.8633e-01,  5.9012e-01],\n",
       "         [-1.1690e+00, -5.7395e-01, -1.3976e+00,  ..., -4.6978e-01,\n",
       "          -5.3423e-01, -6.2995e-01],\n",
       "         [-1.9269e-04,  1.6357e-01,  3.8826e-02,  ...,  8.8838e-01,\n",
       "           5.7656e-01,  9.8496e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 710, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_diffs = logit_diff.abs().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[250.0041, 230.8676, 268.8690, 368.0974, 368.0240, 288.3623, 264.1929,\n",
       "         314.3453, 344.9648, 373.2635, 361.4180, 263.8871, 324.1987, 161.2671,\n",
       "         339.4470, 345.0281, 335.7305, 302.0316, 278.0880, 386.1330, 299.8571,\n",
       "         329.6399, 272.7560, 353.1989, 307.4991, 381.5919, 301.2749, 300.9753,\n",
       "         384.9588, 343.0367, 293.8155, 347.0078, 303.4677, 383.1190, 252.8161,\n",
       "          89.5313, 211.1520, 109.2413, 178.0293, 240.4632, 172.2382, 151.1092,\n",
       "         134.0942, 196.8838, 174.0321, 147.6389, 162.0296, 146.1143, 192.8975,\n",
       "         132.4105, 218.8984, 161.5545, 169.9106, 143.5901, 141.5398, 166.3748,\n",
       "         178.7725, 174.4449, 187.3694, 186.3413, 158.0880, 155.7456, 155.5697,\n",
       "         197.6676, 173.6640, 163.6866, 199.7448, 211.9181, 192.4732, 243.1473,\n",
       "         161.7686, 220.3329, 158.1173, 172.8017, 287.5665, 309.6010, 178.4384,\n",
       "         163.0322, 207.3238, 198.5522, 169.3605, 244.9174, 159.0432, 162.9296,\n",
       "         210.6855, 161.7034, 171.1038, 150.6186, 214.5444, 180.2531, 154.9842,\n",
       "         154.1735, 224.6966, 202.8674, 277.2184, 145.9865, 171.8397, 141.5150,\n",
       "         222.2508, 120.8239, 126.3561, 138.1458, 129.2836, 154.3311, 120.8638,\n",
       "         154.0307, 123.9671, 237.7370, 120.3575, 156.8469, 151.8768, 126.7994,\n",
       "         127.1189, 112.5250, 112.4938, 120.4259, 178.1173, 103.0557, 100.3833,\n",
       "         103.9894, 134.3127, 120.0948, 180.2980, 159.1929, 157.0790, 222.7146,\n",
       "         137.1359, 137.7796, 204.4455, 165.8877, 237.3578, 204.2716, 167.1995,\n",
       "         183.1733, 187.0216, 178.3632, 142.6428, 149.9695, 215.8626, 145.2386,\n",
       "         225.8576, 171.3779, 181.6720, 148.0672, 156.5864, 179.4656, 160.6569,\n",
       "         181.6893, 170.6056, 161.0435, 216.5105, 255.4293, 167.7321, 177.6802,\n",
       "         157.1936, 156.8087, 142.1378, 142.1014, 164.7551, 190.7688, 185.5100,\n",
       "         191.8675, 239.2698, 225.4185, 193.8485, 160.6902, 191.3232, 176.8310,\n",
       "         179.3006, 228.4897, 175.4079, 146.8369, 173.5148, 152.4483, 180.5323,\n",
       "         198.8443, 216.1856, 192.3724, 165.6432, 148.9220, 200.9363, 188.1196,\n",
       "         178.1945, 181.5027, 159.2341, 202.2898, 205.7664, 141.7978, 164.1057,\n",
       "         195.4328, 166.7693, 206.1606, 140.8056, 185.0213, 151.8388, 290.7731,\n",
       "         341.5871, 189.9502, 225.2943, 214.4080, 204.7544, 295.8148, 232.0664,\n",
       "         182.0068, 280.6340, 339.3790, 266.4901, 279.7312, 217.8337, 246.8719,\n",
       "         279.3322, 149.1695, 188.6481, 208.2204, 217.1887, 209.9685, 250.9376,\n",
       "         189.6158, 168.7031, 243.1509, 179.1160, 219.9351, 169.2162, 191.0987,\n",
       "         263.3589, 335.0214, 360.6478, 321.3083, 292.9965, 145.6004, 240.4794,\n",
       "         195.4053, 171.7624, 179.3441, 227.0711, 174.3450, 170.5461, 174.1329,\n",
       "         151.1342, 146.4508, 185.6670, 169.2981, 157.9838, 319.8098, 334.7547,\n",
       "         323.1265, 235.4814, 317.3912, 352.7362, 369.3070, 317.9490, 310.0074,\n",
       "         314.8848, 287.3758, 317.8323, 262.1725]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_diffs # TODO: figure out why the token diffs are so high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
