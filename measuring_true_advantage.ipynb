{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have a random model that we counterfactually force to gate at certain times. What does the advantage of doing so look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from clean_code.flexible_bitter_llm import FlexibleBitterLLM, Gemma2RotaryEmbedding, IndependentWrapperGater\n",
    "from clean_code.bitter_llm import RandomGater\n",
    "torch.serialization.add_safe_globals([nn.modules.sparse.Embedding])\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import HTML\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_with_filter = False\n",
    "if load_model_with_filter:\n",
    "    my_model = torch.load(\"experiment_28/model_8.pt\", weights_only=False)\n",
    "    filter_vals = [v.item() for v in my_model.down_layer_gate.filter]\n",
    "else:\n",
    "    my_model = torch.load(\"training_random_base_model/random_select_early_output_base_model_42.pt\", weights_only=False)\n",
    "    my_model.down_layer_gate = IndependentWrapperGater(my_model.down_layer_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Quick question: what filters are learned by the models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.001364827156067 [-0.2573898732662201, -0.0959232747554779, 0.003937778528779745, 0.002313619013875723]\n",
      "-1.006346583366394 [-0.2470589131116867, -0.11891937255859375, -0.008503127843141556, 0.04420414939522743]\n",
      "-0.9988336563110352 [-0.26974087953567505, -0.0981111004948616, -0.01271668542176485, 0.009889219887554646]\n"
     ]
    }
   ],
   "source": [
    "inspect_filter_values = False   \n",
    "if inspect_filter_values:\n",
    "    for seed in range(1, 4):\n",
    "        my_model = torch.load(f\"experiment_28/model_{seed}.pt\", weights_only=False)\n",
    "        base_val = my_model.down_layer_gate.base_value.item()\n",
    "    filter_vals = [v.item() for v in my_model.down_layer_gate.filter]\n",
    "    print(base_val, filter_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, supression of gating after just gating _is_ learned, just a very small amount! This is likely due to the Adam optimizer normalizing the gradients passed to these values, and the . For example, if the average size of the gradient to one of these parameters has first moment $E(X) = \\mu$ and second moment $E(X) = v^2$, then adam will normalise this to $\\frac{\\mu}{v^2}$ on avergate and then scale it by `1e-4`. This means that changing the loss balance won't help, but a more flexible model might"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_old_model(file_name):\n",
    "    my_model = torch.load(file_name, weights_only=False)\n",
    "    my_model.__class__ = FlexibleBitterLLM\n",
    "    my_model.rotary_emb = Gemma2RotaryEmbedding(my_model.byte_layer_config)\n",
    "    my_model.attn_implementation = \"eager\" # Use eager attention for higher precision.\n",
    "\n",
    "    my_model = my_model.to(dtype=torch.float32)\n",
    "\n",
    "    for l in [*my_model.down_layers, *my_model.mid_layers, *my_model.up_layers]:\n",
    "        l.self_attn.attn_logit_softcapping = 50.0\n",
    "\n",
    "    return my_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlexibleBitterLLM(\n",
       "  (embedding): Embedding(256, 768)\n",
       "  (down_layers): ModuleList(\n",
       "    (0-1): 2 x OptimizedModule(\n",
       "      (_orig_mod): Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (down_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_layers): ModuleList(\n",
       "    (0-5): 6 x OptimizedModule(\n",
       "      (_orig_mod): Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_layers): ModuleList(\n",
       "    (0-1): 2 x OptimizedModule(\n",
       "      (_orig_mod): Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (down_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((768,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (early_output_layer): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (down_layer_gate): IndependentWrapperGater(\n",
       "    (gater): RandomGater()\n",
       "  )\n",
       "  (downsampler): SelectTokenDownsampler()\n",
       "  (rotary_emb): Gemma2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_by_next_token(vals, token_ids):\n",
    "    # vals is a tensor of shape (batch_size, seq_len, vocab_size)\n",
    "    # token_ids is a tensor of shape (seq_len,)\n",
    "    # We want to index vals by the next token in token_ids\n",
    "    # Return a tensor of shape (batch_size, seq_len-1)\n",
    "    next_token_ids = token_ids[1:]\n",
    "    current_vals = vals[:, :-1, :]\n",
    "    # result = current_vals.index_select(dim=-1, index=next_token_ids)\n",
    "    next_token_ids = next_token_ids.unsqueeze(0).unsqueeze(-1)\n",
    "    next_token_ids = next_token_ids.expand(current_vals.shape[0], -1, 1)\n",
    "    result = current_vals.gather(dim=-1, index=next_token_ids)\n",
    "    \n",
    "    return result.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_stats_batch_independent(a_logits, b_logits):\n",
    "    \"\"\"\n",
    "    a_logits and b_logits are tensors of shape (batch_size, seq_len)\n",
    "    returns the mean and standard error of the difference between a_logits and b_logits, computed as if there is no relation across batch indices between a and b.\n",
    "    \"\"\"\n",
    "    batch_size_a = a_logits.shape[0]\n",
    "    batch_size_b = b_logits.shape[0]\n",
    "\n",
    "    mean_a = a_logits.mean(dim=0)\n",
    "    mean_b = b_logits.mean(dim=0)\n",
    "    diff_mean = mean_a - mean_b\n",
    "\n",
    "    std_a = a_logits.std(dim=0)\n",
    "    std_b = b_logits.std(dim=0)\n",
    "    diff_sterr = torch.sqrt((std_a**2)/batch_size_a + (std_b**2)/batch_size_b)\n",
    "\n",
    "    return diff_mean, diff_sterr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactoring the plotting functions to be more modular:\n",
    "# Helper function: visualize the hard to predict tokens.\n",
    "def plot_character_intensities(txt, intensities, intensity_to_value_fn=lambda x: x, colorbar_label=\"intensity\"):\n",
    "    # Get the token logits (cross-entropy loss for each token)    \n",
    "    # Normalize the logits to a probability-like scale (higher logits = higher probability of being gated)\n",
    "    # We use softmax-like normalization to get values between 0 and 1\n",
    "    # Ensure gate_probs and input_text have the same length\n",
    "    # The first token is not predicted, so we set its intensity to 0\n",
    "    #     \n",
    "    # Create HTML with colored text based on probabilities\n",
    "    colored_text = \"\"\n",
    "    colorbar = \"\"\n",
    "    \n",
    "    # Create a colorbar showing the gradient\n",
    "    for i in range(11):  # 0.0 to 1.0 in steps of 0.1\n",
    "        intensity = i / 10\n",
    "        r = min(1.0, intensity)\n",
    "        b = max(0.0, 1.0 - intensity)\n",
    "        color = f\"rgb({int(r*255)}, 0, {int(b*255)})\"\n",
    "        white = f\"rgb(255, 255, 255)\"\n",
    "        colorbar += f'<span style=\"color:{white}; background-color:{color}; margin-right:2px; padding:0 5px;\">{intensity_to_value_fn(intensity):.2f}</span>'\n",
    "    \n",
    "    # Add a legend for the colorbar\n",
    "    colorbar_html = f'''\n",
    "    <div style=\"margin-bottom:10px;\">\n",
    "        <div style=\"font-family:monospace; font-size:12px; margin-bottom:3px;\">{colorbar_label}</div>\n",
    "        <div style=\"font-family:monospace; font-size:14px;\">{colorbar}</div>\n",
    "    </div>\n",
    "    '''\n",
    "    \n",
    "    # Process the text with colors\n",
    "    for char, intensity in zip(txt, intensities):\n",
    "        # Convert probability to color (blue->red)\n",
    "        r = min(1.0, intensity)  # Red increases with probability\n",
    "        b = max(0.0, 1.0 - intensity)  # Blue decreases with probability\n",
    "        color = f\"rgb({int(r*255)}, 0, {int(b*255)})\"\n",
    "        # Add the colored character to the output\n",
    "        colored_text += f'<span style=\"color:{white}; background-color:{color};\">{char}</span>'\n",
    "    \n",
    "    # Display the colorbar and colored text\n",
    "    display(HTML(f'''\n",
    "    <div>\n",
    "        {colorbar_html}\n",
    "        <div style=\"font-family:monospace; font-size:14px;\">{colored_text}</div>\n",
    "    </div>\n",
    "    '''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.bbc.com/news/articles/crrz44d7v08o\n",
    "test_string = \"\"\"This BBC interview with Prince Harry will become one of those famous moments when television collides with the world of the royals.\n",
    "\n",
    "It was like an emotional avalanche. It began with some stones being kicked over with questions about security and then the interview turned into a spectacular release of what seemed to be a rolling mountain of pent-up frustration and a poignant sense of separation.\n",
    "\n",
    "The starting point was Prince Harry's defeat in the courts as he sought to overturn a downgrading of his security in the UK. He seemed wounded. Had he decided it was time to have his say? And then really say some more?\n",
    "\n",
    "A conversation about security was suddenly becoming about a whole range of insecurities.\n",
    "\"\"\"\n",
    "test_batch = byte5_tokenizer.encode(test_string, return_tensors=\"pt\", padding=True).to(device)\n",
    "token_ids = test_batch[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Analysis 1: keeping the preceding gates fixed to make it the \"true\" advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    base_out = my_model(test_batch.expand(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_of_interest = 23\n",
    "base_down_gate_samples = base_out[\"down_gate_samples\"]\n",
    "gate_samples_mask = torch.cat([torch.ones(gate_of_interest+1, dtype=torch.bool, device=device), torch.zeros(test_batch.shape[1]-gate_of_interest-1, dtype=torch.bool, device=device)], dim=0).unsqueeze(0)\n",
    "\n",
    "gate_gate_samples = base_down_gate_samples.clone()\n",
    "gate_gate_samples[:, gate_of_interest] = 1\n",
    "no_gate_samples = base_down_gate_samples.clone()\n",
    "no_gate_samples[:, gate_of_interest] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want: counter-factual difference between gating at token of interest and not gating at token of interest, itegrated over some large number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_batch_size = 512\n",
    "expanded_gate_samples_mask = gate_samples_mask.expand(forward_batch_size, -1)\n",
    "expanded_test_batch = test_batch.expand(forward_batch_size, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nOutOfMemoryError: CUDA out of memory. Tried to allocate 11.60 GiB. GPU 0 has a total capacity of 47.54 GiB of which 5.21 GiB is free. Including non-PyTorch memory, this process has 42.32 GiB memory in use. Of the allocated memory 41.95 GiB is allocated by PyTorch, and 66.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBackendCompilerFailed\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m ggs = ggs.expand(forward_batch_size, -\u001b[32m1\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     ggo = \u001b[43mmy_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpanded_test_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprescribed_down_gate_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mggs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdown_gate_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgate_samples_mask\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m gate_gate_out.append(ggo)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/itet-stor/sdauncey/net_scratch/VScodeProjects/bitter-lesson-tokenization/clean_code/flexible_bitter_llm.py:374\u001b[39m, in \u001b[36mFlexibleBitterLLM.forward\u001b[39m\u001b[34m(self, input_ids, position_ids, prescribed_down_gate_samples, down_gate_mask, cache_position, past_key_value, past_gate_samples, use_cache)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# Apply down layers to byte tokens        \u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.down_layers:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbyte_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbyte_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    384\u001b[39m early_logits = \u001b[38;5;28mself\u001b[39m.early_output_layer(x)\n\u001b[32m    385\u001b[39m early_logits = F.log_softmax(early_logits, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:663\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    661\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    662\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    666\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1544\u001b[39m, in \u001b[36mOutputGraph._call_user_compiler\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1542\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1544\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\n\u001b[32m   1545\u001b[39m         \u001b[38;5;28mself\u001b[39m.compiler_fn, e, inspect.currentframe()\n\u001b[32m   1546\u001b[39m     ).with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1548\u001b[39m signpost_event(\n\u001b[32m   1549\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdynamo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1550\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOutputGraph.call_user_compiler\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1556\u001b[39m     },\n\u001b[32m   1557\u001b[39m )\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1519\u001b[39m, in \u001b[36mOutputGraph._call_user_compiler\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.verify_correctness:\n\u001b[32m   1518\u001b[39m     compiler_fn = WrapperBackend(compiler_fn)\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m compiled_fn = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m _step_logger()(logging.INFO, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1521\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[33m\"\u001b[39m\u001b[33mcompiler_fn did not return callable\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:150\u001b[39m, in \u001b[36mWrapBackendDebug.__call__\u001b[39m\u001b[34m(self, gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     compiled_gm = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/__init__.py:2347\u001b[39m, in \u001b[36m_TorchCompileInductorWrapper.__call__\u001b[39m\u001b[34m(self, model_, inputs_)\u001b[39m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:2089\u001b[39m, in \u001b[36mcompile_fx\u001b[39m\u001b[34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[39m\n\u001b[32m   2082\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   2083\u001b[39m     V.set_fake_mode(fake_mode),\n\u001b[32m   2084\u001b[39m     torch._guards.tracing(tracing_context),\n\u001b[32m   2085\u001b[39m     compiled_autograd._disable(),\n\u001b[32m   2086\u001b[39m     functorch_config.patch(unlift_effect_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   2087\u001b[39m ):\n\u001b[32m   2088\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2089\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2091\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[43m            \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2094\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2097\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2098\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2099\u001b[39m         \u001b[38;5;66;03m# We will also shorten the traceback inside dynamo.\u001b[39;00m\n\u001b[32m   2100\u001b[39m         \u001b[38;5;66;03m# This is only useful if inductor is called directly with an FX graph.\u001b[39;00m\n\u001b[32m   2101\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:101\u001b[39m, in \u001b[36mAotAutograd.__call__\u001b[39m\u001b[34m(self, gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         cg = \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m         counters[\u001b[33m\"\u001b[39m\u001b[33maot_autograd\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1160\u001b[39m, in \u001b[36maot_module_simplified\u001b[39m\u001b[34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local \u001b[38;5;129;01mor\u001b[39;00m remote:\n\u001b[32m   1159\u001b[39m     set_feature_use(\u001b[33m\"\u001b[39m\u001b[33maot_autograd_remote_cache\u001b[39m\u001b[33m\"\u001b[39m, remote)\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m     compiled_fn = \u001b[43mAOTAutogradCache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdispatch_and_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1170\u001b[39m     compiled_fn = dispatch_and_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:775\u001b[39m, in \u001b[36mAOTAutogradCache.load\u001b[39m\u001b[34m(dispatch_and_compile, mod, args, aot_config, cudagraphs, local, remote)\u001b[39m\n\u001b[32m    771\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cache_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    772\u001b[39m         aot_config.cache_info = AOTAutogradCacheInfo(\n\u001b[32m    773\u001b[39m             cache_key, time.time_ns()\n\u001b[32m    774\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m     compiled_fn = \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    777\u001b[39m cache_info.update(\n\u001b[32m    778\u001b[39m     {\n\u001b[32m    779\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m: cache_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    782\u001b[39m     }\n\u001b[32m    783\u001b[39m )\n\u001b[32m    784\u001b[39m CompileEventLogger.instant(\n\u001b[32m    785\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mautograd_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    786\u001b[39m     metadata=cache_info,\n\u001b[32m    787\u001b[39m     time_ns=cache_event_time,\n\u001b[32m    788\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1145\u001b[39m, in \u001b[36maot_module_simplified.<locals>.dispatch_and_compile\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1143\u001b[39m functional_call = create_functional_call(mod, params_spec, params_len)\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd._disable():\n\u001b[32m-> \u001b[39m\u001b[32m1145\u001b[39m     compiled_fn, _ = \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:570\u001b[39m, in \u001b[36mcreate_aot_dispatcher_function\u001b[39m\u001b[34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[32m    563\u001b[39m     flat_fn,\n\u001b[32m    564\u001b[39m     fake_flat_args: FakifiedFlatArgs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    567\u001b[39m     shape_env: Optional[ShapeEnv],\n\u001b[32m    568\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Callable, ViewAndMutationMeta]:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mcreate_aot_dispatcher_function\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:820\u001b[39m, in \u001b[36m_create_aot_dispatcher_function\u001b[39m\u001b[34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[39m\n\u001b[32m    816\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[32m    818\u001b[39m compiler_fn = choose_dispatcher(needs_autograd, aot_config)\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m compiled_fn, fw_metadata = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:219\u001b[39m, in \u001b[36maot_dispatch_base\u001b[39m\u001b[34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[39m\n\u001b[32m    217\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fw_module, GraphModule)\n\u001b[32m    218\u001b[39m         tensorify_python_scalars(fw_module, fake_mode.shape_env, fake_mode)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     compiled_fw = \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper.needs_post_compile:\n\u001b[32m    222\u001b[39m     fakified_out_wrapper.set_fwd_output_strides(fwd_output_strides)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:479\u001b[39m, in \u001b[36mSerializableAOTDispatchCompiler.__call__\u001b[39m\u001b[34m(self, gm, example_inputs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    476\u001b[39m     gm: torch.fx.GraphModule,\n\u001b[32m    477\u001b[39m     example_inputs: Sequence[InputType],\n\u001b[32m    478\u001b[39m ) -> OutputCode:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1882\u001b[39m, in \u001b[36mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[34m(gm, example_inputs, is_inference)\u001b[39m\n\u001b[32m   1879\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils.dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_inference:\n\u001b[32m   1881\u001b[39m         \u001b[38;5;66;03m# partition_fn won't be called\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1882\u001b[39m         \u001b[43m_recursive_joint_graph_passes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1884\u001b[39m     fixed = torch._inductor.utils.num_fw_fixed_arguments(\n\u001b[32m   1885\u001b[39m         num_example_inputs, \u001b[38;5;28mlen\u001b[39m(example_inputs)\n\u001b[32m   1886\u001b[39m     )\n\u001b[32m   1888\u001b[39m     model_outputs_node = output_node(gm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:367\u001b[39m, in \u001b[36m_recursive_joint_graph_passes\u001b[39m\u001b[34m(gm)\u001b[39m\n\u001b[32m    365\u001b[39m     subgraph = \u001b[38;5;28mgetattr\u001b[39m(gm, subgraph_name)\n\u001b[32m    366\u001b[39m     _recursive_joint_graph_passes(subgraph)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[43mjoint_graph_passes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/joint_graph.py:561\u001b[39m, in \u001b[36mjoint_graph_passes\u001b[39m\u001b[34m(graph)\u001b[39m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.pattern_matcher:\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, patterns \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pass_patterns):\n\u001b[32m    559\u001b[39m         maybe_count = \u001b[43mGraphTransformObserver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpass_pattern_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_graph_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m         count += maybe_count \u001b[38;5;28;01mif\u001b[39;00m maybe_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.fallback_random:\n\u001b[32m    565\u001b[39m     \u001b[38;5;66;03m# not trying into the bisector because decomps may have already affected rng reproducibility\u001b[39;00m\n\u001b[32m    566\u001b[39m     \u001b[38;5;66;03m# we'll instead explicitly turn off the config\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/fx/passes/graph_transform_observer.py:85\u001b[39m, in \u001b[36mGraphTransformObserver.apply_graph_pass\u001b[39m\u001b[34m(self, pass_fn)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_disable_pass():\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpass_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/pattern_matcher.py:1870\u001b[39m, in \u001b[36mPatternMatcherPass.apply\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_PATTERN_MATCH_DEBUG\u001b[39m\u001b[33m\"\u001b[39m) == node.name:\n\u001b[32m   1869\u001b[39m     log.warning(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, node, node.args, m, entry.pattern)\n\u001b[32m-> \u001b[39m\u001b[32m1870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_match(m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextra_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1871\u001b[39m     count += \u001b[32m1\u001b[39m\n\u001b[32m   1872\u001b[39m     entry.apply(m, graph, node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/pattern_matcher.py:1412\u001b[39m, in \u001b[36mregister_replacement.<locals>.check_fn\u001b[39m\u001b[34m(match)\u001b[39m\n\u001b[32m   1409\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1410\u001b[39m specific_pattern_match = specific_pattern.match(node)\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_match(specific_pattern_match) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mextra_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecific_pattern_match\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# trace the pattern using the shapes from the user program\u001b[39;00m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mmatch\u001b[39;00m.replacement_graph = trace_fn(replace_fn, args)\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(match.nodes) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:805\u001b[39m, in \u001b[36mshould_pad_bmm\u001b[39m\u001b[34m(match)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshould_pad_bmm\u001b[39m(match: Match) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    804\u001b[39m     mat1, mat2 = fetch_fake_tensors(match, (\u001b[33m\"\u001b[39m\u001b[33mmat1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmat2\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m should_pad_common(mat1, mat2) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mshould_pad_bench\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43maten\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:396\u001b[39m, in \u001b[36mshould_pad_bench\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshould_pad_bench\u001b[39m(*args: Any, **kwargs: Any) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpad_mm_benchmark\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    393\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    394\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33mcompile_time_autotune_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    395\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_should_pad_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:508\u001b[39m, in \u001b[36m_should_pad_bench\u001b[39m\u001b[34m(match, mat1, mat2, op, input)\u001b[39m\n\u001b[32m    506\u001b[39m fns = []\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mat1_pre_padded \u001b[38;5;129;01mand\u001b[39;00m (m_padded_length \u001b[38;5;129;01mor\u001b[39;00m k_padded_length):\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     mat1_pad = \u001b[43mpad_mat1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmat1_pad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mm_padded_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm_padded_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_padded_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_padded_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_bmm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_bmm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite_pad\u001b[39m():\n\u001b[32m    516\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m is_bmm:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:743\u001b[39m, in \u001b[36mpad_mat1\u001b[39m\u001b[34m(mat1, m_padded_length, k_padded_length, is_bmm)\u001b[39m\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_bmm:\n\u001b[32m    742\u001b[39m         pad_arg.extend((\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maten\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstant_pad_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_arg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mat1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mBackendCompilerFailed\u001b[39m: backend='inductor' raised:\nOutOfMemoryError: CUDA out of memory. Tried to allocate 11.60 GiB. GPU 0 has a total capacity of 47.54 GiB of which 5.21 GiB is free. Including non-PyTorch memory, this process has 42.32 GiB memory in use. Of the allocated memory 41.95 GiB is allocated by PyTorch, and 66.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "gate_gate_out = []\n",
    "\n",
    "for ggs in gate_gate_samples:\n",
    "    ggs = ggs.expand(forward_batch_size, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ggo = my_model(\n",
    "            expanded_test_batch, \n",
    "            prescribed_down_gate_samples=ggs, \n",
    "            down_gate_mask=gate_samples_mask\n",
    "        )\n",
    "    gate_gate_out.append(ggo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gate_out = []\n",
    "for ngs in no_gate_samples:\n",
    "    ngs = ngs.expand(forward_batch_size, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ngo = my_model(\n",
    "            expanded_test_batch, \n",
    "            prescribed_down_gate_samples=ngs, \n",
    "            down_gate_mask=gate_samples_mask\n",
    "        )\n",
    "    no_gate_out.append(ngo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits_gate = [index_by_next_token(ggo[\"logits\"], token_ids) for ggo in gate_gate_out]\n",
    "next_token_logits_no_gate = [index_by_next_token(ngo[\"logits\"], token_ids) for ngo in no_gate_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_stats = [logit_diff_stats_batch_independent(a, b) for a, b in zip(next_token_logits_gate, next_token_logits_no_gate)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the pattern of differences with standard error\n",
    "def visualize_logit_diffs(diff_mean, diff_sterr, **plot_kwargs):\n",
    "    diff_np = diff_mean.cpu().numpy()\n",
    "    stderr_np = diff_sterr.cpu().numpy()\n",
    "\n",
    "    plt.plot(diff_np, **plot_kwargs)\n",
    "\n",
    "    plt.fill_between(\n",
    "        range(len(diff_np)), \n",
    "        diff_np - stderr_np, \n",
    "        diff_np + stderr_np, \n",
    "        alpha=0.3, \n",
    "        label='Standard Error'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation 1: the advantage is very noisy depending on the preceding gating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "visualize_logit_diffs(diff_stats[0][0], diff_stats[0][1], label=f\"preceding gating {0}\")\n",
    "visualize_logit_diffs(diff_stats[1][0], diff_stats[1][1], label=f\"preceding gating {1}\")\n",
    "plt.xlim(0, 100)\n",
    "plt.axvline(x=gate_of_interest, color='r', linestyle='--', label=f'Gate of interest (position {gate_of_interest})')\n",
    "plt.title('Next Token Logit Differences (gate - no gate)')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Logit Difference')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string[:gate_of_interest+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_down_gate_samples[:, :gate_of_interest+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte5_tokenizer.decode(token_ids[gate_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Analysis 2: what does the advantage look like if we marginalise over the preceding gates too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nOutOfMemoryError: CUDA out of memory. Tried to allocate 11.54 GiB. GPU 0 has a total capacity of 47.54 GiB of which 3.99 GiB is free. Including non-PyTorch memory, this process has 43.54 GiB memory in use. Of the allocated memory 43.17 GiB is allocated by PyTorch, and 65.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBackendCompilerFailed\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(n_forward_minibatches):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         full_out = \u001b[43mmy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_minibatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m         full_logits.append(full_out[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     13\u001b[39m         full_gate_samples.append(full_out[\u001b[33m\"\u001b[39m\u001b[33mdown_gate_samples\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/itet-stor/sdauncey/net_scratch/VScodeProjects/bitter-lesson-tokenization/clean_code/flexible_bitter_llm.py:374\u001b[39m, in \u001b[36mFlexibleBitterLLM.forward\u001b[39m\u001b[34m(self, input_ids, position_ids, prescribed_down_gate_samples, down_gate_mask, cache_position, past_key_value, past_gate_samples, use_cache)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# Apply down layers to byte tokens        \u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.down_layers:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbyte_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbyte_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    384\u001b[39m early_logits = \u001b[38;5;28mself\u001b[39m.early_output_layer(x)\n\u001b[32m    385\u001b[39m early_logits = F.log_softmax(early_logits, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:663\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    661\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    662\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    666\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1544\u001b[39m, in \u001b[36mOutputGraph._call_user_compiler\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1542\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1544\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\n\u001b[32m   1545\u001b[39m         \u001b[38;5;28mself\u001b[39m.compiler_fn, e, inspect.currentframe()\n\u001b[32m   1546\u001b[39m     ).with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1548\u001b[39m signpost_event(\n\u001b[32m   1549\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdynamo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1550\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOutputGraph.call_user_compiler\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1556\u001b[39m     },\n\u001b[32m   1557\u001b[39m )\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1519\u001b[39m, in \u001b[36mOutputGraph._call_user_compiler\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.verify_correctness:\n\u001b[32m   1518\u001b[39m     compiler_fn = WrapperBackend(compiler_fn)\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m compiled_fn = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m _step_logger()(logging.INFO, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1521\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[33m\"\u001b[39m\u001b[33mcompiler_fn did not return callable\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:150\u001b[39m, in \u001b[36mWrapBackendDebug.__call__\u001b[39m\u001b[34m(self, gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     compiled_gm = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/__init__.py:2347\u001b[39m, in \u001b[36m_TorchCompileInductorWrapper.__call__\u001b[39m\u001b[34m(self, model_, inputs_)\u001b[39m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:2089\u001b[39m, in \u001b[36mcompile_fx\u001b[39m\u001b[34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[39m\n\u001b[32m   2082\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m   2083\u001b[39m     V.set_fake_mode(fake_mode),\n\u001b[32m   2084\u001b[39m     torch._guards.tracing(tracing_context),\n\u001b[32m   2085\u001b[39m     compiled_autograd._disable(),\n\u001b[32m   2086\u001b[39m     functorch_config.patch(unlift_effect_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m   2087\u001b[39m ):\n\u001b[32m   2088\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2089\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2091\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[43m            \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2094\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2097\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2098\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2099\u001b[39m         \u001b[38;5;66;03m# We will also shorten the traceback inside dynamo.\u001b[39;00m\n\u001b[32m   2100\u001b[39m         \u001b[38;5;66;03m# This is only useful if inductor is called directly with an FX graph.\u001b[39;00m\n\u001b[32m   2101\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:101\u001b[39m, in \u001b[36mAotAutograd.__call__\u001b[39m\u001b[34m(self, gm, example_inputs, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         cg = \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m         counters[\u001b[33m\"\u001b[39m\u001b[33maot_autograd\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1160\u001b[39m, in \u001b[36maot_module_simplified\u001b[39m\u001b[34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local \u001b[38;5;129;01mor\u001b[39;00m remote:\n\u001b[32m   1159\u001b[39m     set_feature_use(\u001b[33m\"\u001b[39m\u001b[33maot_autograd_remote_cache\u001b[39m\u001b[33m\"\u001b[39m, remote)\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m     compiled_fn = \u001b[43mAOTAutogradCache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdispatch_and_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1170\u001b[39m     compiled_fn = dispatch_and_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:775\u001b[39m, in \u001b[36mAOTAutogradCache.load\u001b[39m\u001b[34m(dispatch_and_compile, mod, args, aot_config, cudagraphs, local, remote)\u001b[39m\n\u001b[32m    771\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cache_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    772\u001b[39m         aot_config.cache_info = AOTAutogradCacheInfo(\n\u001b[32m    773\u001b[39m             cache_key, time.time_ns()\n\u001b[32m    774\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m     compiled_fn = \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    777\u001b[39m cache_info.update(\n\u001b[32m    778\u001b[39m     {\n\u001b[32m    779\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m: cache_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    782\u001b[39m     }\n\u001b[32m    783\u001b[39m )\n\u001b[32m    784\u001b[39m CompileEventLogger.instant(\n\u001b[32m    785\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mautograd_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    786\u001b[39m     metadata=cache_info,\n\u001b[32m    787\u001b[39m     time_ns=cache_event_time,\n\u001b[32m    788\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1145\u001b[39m, in \u001b[36maot_module_simplified.<locals>.dispatch_and_compile\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1143\u001b[39m functional_call = create_functional_call(mod, params_spec, params_len)\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd._disable():\n\u001b[32m-> \u001b[39m\u001b[32m1145\u001b[39m     compiled_fn, _ = \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:570\u001b[39m, in \u001b[36mcreate_aot_dispatcher_function\u001b[39m\u001b[34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[32m    563\u001b[39m     flat_fn,\n\u001b[32m    564\u001b[39m     fake_flat_args: FakifiedFlatArgs,\n\u001b[32m   (...)\u001b[39m\u001b[32m    567\u001b[39m     shape_env: Optional[ShapeEnv],\n\u001b[32m    568\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Callable, ViewAndMutationMeta]:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mcreate_aot_dispatcher_function\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:820\u001b[39m, in \u001b[36m_create_aot_dispatcher_function\u001b[39m\u001b[34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[39m\n\u001b[32m    816\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[32m    818\u001b[39m compiler_fn = choose_dispatcher(needs_autograd, aot_config)\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m compiled_fn, fw_metadata = \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:219\u001b[39m, in \u001b[36maot_dispatch_base\u001b[39m\u001b[34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[39m\n\u001b[32m    217\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fw_module, GraphModule)\n\u001b[32m    218\u001b[39m         tensorify_python_scalars(fw_module, fake_mode.shape_env, fake_mode)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     compiled_fw = \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper.needs_post_compile:\n\u001b[32m    222\u001b[39m     fakified_out_wrapper.set_fwd_output_strides(fwd_output_strides)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:479\u001b[39m, in \u001b[36mSerializableAOTDispatchCompiler.__call__\u001b[39m\u001b[34m(self, gm, example_inputs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    476\u001b[39m     gm: torch.fx.GraphModule,\n\u001b[32m    477\u001b[39m     example_inputs: Sequence[InputType],\n\u001b[32m    478\u001b[39m ) -> OutputCode:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1882\u001b[39m, in \u001b[36mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[34m(gm, example_inputs, is_inference)\u001b[39m\n\u001b[32m   1879\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils.dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_inference:\n\u001b[32m   1881\u001b[39m         \u001b[38;5;66;03m# partition_fn won't be called\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1882\u001b[39m         \u001b[43m_recursive_joint_graph_passes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1884\u001b[39m     fixed = torch._inductor.utils.num_fw_fixed_arguments(\n\u001b[32m   1885\u001b[39m         num_example_inputs, \u001b[38;5;28mlen\u001b[39m(example_inputs)\n\u001b[32m   1886\u001b[39m     )\n\u001b[32m   1888\u001b[39m     model_outputs_node = output_node(gm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:367\u001b[39m, in \u001b[36m_recursive_joint_graph_passes\u001b[39m\u001b[34m(gm)\u001b[39m\n\u001b[32m    365\u001b[39m     subgraph = \u001b[38;5;28mgetattr\u001b[39m(gm, subgraph_name)\n\u001b[32m    366\u001b[39m     _recursive_joint_graph_passes(subgraph)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[43mjoint_graph_passes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/joint_graph.py:561\u001b[39m, in \u001b[36mjoint_graph_passes\u001b[39m\u001b[34m(graph)\u001b[39m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.pattern_matcher:\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, patterns \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pass_patterns):\n\u001b[32m    559\u001b[39m         maybe_count = \u001b[43mGraphTransformObserver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpass_pattern_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_graph_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m         count += maybe_count \u001b[38;5;28;01mif\u001b[39;00m maybe_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.fallback_random:\n\u001b[32m    565\u001b[39m     \u001b[38;5;66;03m# not trying into the bisector because decomps may have already affected rng reproducibility\u001b[39;00m\n\u001b[32m    566\u001b[39m     \u001b[38;5;66;03m# we'll instead explicitly turn off the config\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/fx/passes/graph_transform_observer.py:85\u001b[39m, in \u001b[36mGraphTransformObserver.apply_graph_pass\u001b[39m\u001b[34m(self, pass_fn)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_disable_pass():\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpass_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/pattern_matcher.py:1870\u001b[39m, in \u001b[36mPatternMatcherPass.apply\u001b[39m\u001b[34m(self, gm)\u001b[39m\n\u001b[32m   1868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_PATTERN_MATCH_DEBUG\u001b[39m\u001b[33m\"\u001b[39m) == node.name:\n\u001b[32m   1869\u001b[39m     log.warning(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, node, node.args, m, entry.pattern)\n\u001b[32m-> \u001b[39m\u001b[32m1870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_match(m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextra_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1871\u001b[39m     count += \u001b[32m1\u001b[39m\n\u001b[32m   1872\u001b[39m     entry.apply(m, graph, node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/pattern_matcher.py:1412\u001b[39m, in \u001b[36mregister_replacement.<locals>.check_fn\u001b[39m\u001b[34m(match)\u001b[39m\n\u001b[32m   1409\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1410\u001b[39m specific_pattern_match = specific_pattern.match(node)\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_match(specific_pattern_match) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mextra_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecific_pattern_match\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# trace the pattern using the shapes from the user program\u001b[39;00m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mmatch\u001b[39;00m.replacement_graph = trace_fn(replace_fn, args)\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(match.nodes) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:805\u001b[39m, in \u001b[36mshould_pad_bmm\u001b[39m\u001b[34m(match)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshould_pad_bmm\u001b[39m(match: Match) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    804\u001b[39m     mat1, mat2 = fetch_fake_tensors(match, (\u001b[33m\"\u001b[39m\u001b[33mmat1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmat2\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m should_pad_common(mat1, mat2) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mshould_pad_bench\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43maten\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:396\u001b[39m, in \u001b[36mshould_pad_bench\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshould_pad_bench\u001b[39m(*args: Any, **kwargs: Any) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpad_mm_benchmark\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    393\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    394\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33mcompile_time_autotune_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    395\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_should_pad_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:487\u001b[39m, in \u001b[36m_should_pad_bench\u001b[39m\u001b[34m(match, mat1, mat2, op, input)\u001b[39m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    485\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.randn_like(t)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m mat1 = \u001b[43mrealize_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m mat2 = realize_tensor(mat2)\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# since we key on whether or not the inputs can be memory planned, set cache for the\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# original time which is unaffected by whether or not the input can be planned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_inductor/fx_passes/pad_mm.py:482\u001b[39m, in \u001b[36m_should_pad_bench.<locals>.realize_tensor\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m    478\u001b[39m     stride_hint = realize_symbols(t.stride())\n\u001b[32m    479\u001b[39m     real_size = (\n\u001b[32m    480\u001b[39m         \u001b[38;5;28msum\u001b[39m((d - \u001b[32m1\u001b[39m) * s \u001b[38;5;28;01mfor\u001b[39;00m d, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(size_hints, stride_hint)) + \u001b[32m1\u001b[39m\n\u001b[32m    481\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     real_t = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.as_strided(real_t, size_hints, stride_hint)\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mBackendCompilerFailed\u001b[39m: backend='inductor' raised:\nOutOfMemoryError: CUDA out of memory. Tried to allocate 11.54 GiB. GPU 0 has a total capacity of 47.54 GiB of which 3.99 GiB is free. Including non-PyTorch memory, this process has 43.54 GiB memory in use. Of the allocated memory 43.17 GiB is allocated by PyTorch, and 65.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "n_forward_minibatches = 32\n",
    "forward_minibatch_size = 512\n",
    "forward_batch_size = n_forward_minibatches * forward_minibatch_size\n",
    "\n",
    "# Re-sample the gates 1024 times:\n",
    "full_logits = []\n",
    "full_gate_samples = []\n",
    "\n",
    "for i in trange(n_forward_minibatches):\n",
    "    with torch.no_grad():\n",
    "        full_out = my_model(test_batch.expand(forward_minibatch_size, -1))\n",
    "        full_logits.append(full_out[\"logits\"])\n",
    "        full_gate_samples.append(full_out[\"down_gate_samples\"])\n",
    "\n",
    "full_logits = torch.cat(full_logits, dim=0)\n",
    "full_gate_samples = torch.cat(full_gate_samples, dim=0)\n",
    "full_early_logits = full_out[\"early_logits\"][0, :, :] # the early logits do not depend on the gate samples.\n",
    "full_early_logits = full_early_logits.unsqueeze(0) # Put the batch dimension back in.\n",
    "\n",
    "full_next_token_logits = index_by_next_token(full_logits, token_ids)\n",
    "full_early_next_token_logits = index_by_next_token(full_early_logits, token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_next_token_logits.shape, full_early_next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_logits_by_gate_activation(full_gate_samples, full_next_token_logits, gate_of_interest):\n",
    "    \"\"\"\n",
    "    full_gate_samples is a tensor of shape (batch_size, seq_len)\n",
    "    full_next_token_logits is a tensor of shape (batch_size, seq_len, vocab_size)\n",
    "    gate_of_interest is an integer\n",
    "    \"\"\"\n",
    "\n",
    "    gate_samples_of_interest = full_gate_samples[:, gate_of_interest]\n",
    "\n",
    "    # Split logits based on gate activation (1 or 0) for the token of interest\n",
    "    gate_on_mask = gate_samples_of_interest == 1\n",
    "    gate_off_mask = gate_samples_of_interest == 0\n",
    "\n",
    "    # Get logits where gate is activated (1)\n",
    "    gate_on_logits = full_next_token_logits[gate_on_mask]\n",
    "\n",
    "    # Get logits where gate is not activated (0)\n",
    "    gate_off_logits = full_next_token_logits[gate_off_mask]\n",
    "\n",
    "    return gate_on_logits, gate_off_logits\n",
    "\n",
    "gate_of_interest = 25\n",
    "gate_on_logits, gate_off_logits = split_logits_by_gate_activation(full_gate_samples, full_next_token_logits, gate_of_interest)\n",
    "# Print shapes to verify the split\n",
    "print(f\"Gate ON logits shape: {gate_on_logits.shape}\")\n",
    "print(f\"Gate OFF logits shape: {gate_off_logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mean, diff_sterr = logit_diff_stats_batch_independent(gate_on_logits, gate_off_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "visualize_logit_diffs(diff_mean, diff_sterr, label=f\"marginalised over preceding gating\")\n",
    "plt.xlim(0, 100)\n",
    "plt.axvline(x=gate_of_interest, color='r', linestyle='--', label=f'Gate of interest (position {gate_of_interest})')\n",
    "plt.title('Next Token Logit Differences (gate - no gate)')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Logit Difference')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "visualize_logit_diffs(diff_mean, diff_sterr, label=f\"preceding gating {0}\")\n",
    "plt.xlim(0, 600)\n",
    "plt.axvline(x=gate_of_interest, color='r', linestyle='--', label=f'Gate of interest (position {gate_of_interest})')\n",
    "plt.title('Next Token Logit Differences (gate - no gate)')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Logit Difference')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_string[:gate_of_interest+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mean.shape, len(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logit_diffs_intensities(txt, diff_mean, **kwargs):\n",
    "    diff_mean = diff_mean.cpu().numpy()\n",
    "    diff_mean = np.concatenate([np.zeros(1), diff_mean[:-1]]) # Diff means concerns the _next_ token, so we shift by one.\n",
    "    plot_linear_intensity_scale(txt, diff_mean, colorbar_label=\"logit difference\")\n",
    "\n",
    "\n",
    "def plot_linear_intensity_scale(txt, unscaled_intensities, **kwargs):\n",
    "    intensities = (unscaled_intensities - unscaled_intensities.min()) / (unscaled_intensities.max() - unscaled_intensities.min())\n",
    "    intensity_to_value_fn = lambda x: x * (unscaled_intensities.max() - unscaled_intensities.min()) + unscaled_intensities.min()\n",
    "    plot_character_intensities(txt, intensities, intensity_to_value_fn, **kwargs)\n",
    "\n",
    "\n",
    "plot_logit_diffs_intensities(test_string, diff_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mean.min(), diff_mean.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "diff_mean[gate_of_interest:gate_of_interest+window_size].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sequence_length = full_gate_samples.shape\n",
    "window_advantages = []\n",
    "\n",
    "for gate_of_interest_loop in range(1, sequence_length - window_size):\n",
    "\n",
    "    gonl, gnol = split_logits_by_gate_activation(full_gate_samples, full_next_token_logits, gate_of_interest_loop)\n",
    "    d_mean, d_sterr = logit_diff_stats_batch_independent(gonl, gnol)\n",
    "\n",
    "    window_advantage = d_mean[gate_of_interest_loop:gate_of_interest_loop+window_size].sum()\n",
    "    window_advantages.append(window_advantage.item())\n",
    "\n",
    "plt.plot(window_advantages)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that we are indeed computing the correct advantage:\n",
    "window_advantages[gate_of_interest-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clipped_advantage_intensities(txt, advantages, clip_value=1.5, **kwargs):\n",
    "    advantages = np.array(advantages)\n",
    "    advantages = np.concatenate([np.zeros(1), advantages]) # The first token is not predicted, so we set its advantage to 0\n",
    "    clipped_advantages = np.clip(advantages, -clip_value, clip_value)\n",
    "    plot_linear_intensity_scale(txt, clipped_advantages, colorbar_label=\"clipped advantage\", **kwargs)\n",
    "\n",
    "plot_clipped_advantage_intensities(test_string, window_advantages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "- Gating at the second \"Prince Harry\" is extremely important to be able to use an induction head (but potentially not as important if there's already been a couple rounds of gating)\n",
    "- Suprisingly (to me) the advantage of gating on sequence delimiters is basically null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the base model indeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_entropy_gain = full_next_token_logits.mean(dim=0) - full_early_next_token_logits[0]\n",
    "plot_clipped_advantage_intensities(test_string, mean_entropy_gain.cpu(), clip_value=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_entropy_gain.mean().item(), mean_entropy_gain.min().item(), mean_entropy_gain.max().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Analysis 3: what is the advantage of gating after a given preceding gating pattern?\n",
    "\n",
    "prediction: 50% that gating with 5 preceding zeros has a more positive advantage and gating with 2 preceding ones has a more negative advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gate_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_preceding_gates(full_gate_samples, n_preceding_ones):\n",
    "    # Use a conv1d to count the number of preceding ones.\n",
    "\n",
    "    filter = torch.ones((n_preceding_ones,))\n",
    "\n",
    "    return filter_preceding_gates(full_gate_samples, filter)\n",
    "\n",
    "def filter_preceding_gates(full_gate_samples, filter_1d):\n",
    "    # Use a conv1d to count the number of preceding ones.\n",
    "    filter_size, = filter_1d.shape\n",
    "    filter = filter_1d.unsqueeze(0).unsqueeze(0)\n",
    "    full_gate_samples_c = full_gate_samples.unsqueeze(1)\n",
    "\n",
    "    full_gate_samples_c = full_gate_samples_c.to(device=device, dtype=torch.float32)\n",
    "    filter = filter.to(device=device, dtype=full_gate_samples_c.dtype)\n",
    "\n",
    "    n_preceding_gates = F.conv1d(full_gate_samples_c, filter, padding=filter_size-1)\n",
    "    n_preceding_gates = n_preceding_gates.squeeze(1)\n",
    "    n_preceding_gates = n_preceding_gates[:, :-filter_size+1]\n",
    "    return n_preceding_gates.to(dtype=torch.int32)\n",
    "\n",
    "\n",
    "sum_preceding_2_gates = count_preceding_gates(full_gate_samples, 2)\n",
    "sum_preceding_3_gates = count_preceding_gates(full_gate_samples, 3)\n",
    "sum_preceding_2_gates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gate_samples[:5, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_preceding_3_gates[:5, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_gates = (sum_preceding_3_gates == 3)\n",
    "triple_gates.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all the examples of 2 preceding gates and 3 preceding gates. What's the advantage of 121 compared to 123?\n",
    "\n",
    "ie. $$\\mathbb{E}(\\log p(x_{i+1:i+11}) \\vert a_{i-2:i} = 11, a_{i} = 1) - \\mathbb{E}(\\log p(x_{i+1}) \\vert a_{i-2:i} = 11, a_{i} = 0)$$\n",
    "\n",
    "Where the expectation is taken over all $x$ and $a_i$. 111 can be detected with value 3 on filter 111, 110 can be detected with value 2 on filter 11-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_pattern = torch.tensor([1, 1, -1])\n",
    "counterfactual_vals = filter_preceding_gates(full_gate_samples, counterfactual_pattern)\n",
    "counterfactual_vals[:3, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_gates = (counterfactual_vals == 2)\n",
    "counterfactual_gates.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_next_token_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfull_next_token_logits\u001b[49m.shape\n",
      "\u001b[31mNameError\u001b[39m: name 'full_next_token_logits' is not defined"
     ]
    }
   ],
   "source": [
    "full_next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_preceding_gates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m extender = torch.ones((\u001b[32m10\u001b[39m,)) \u001b[38;5;66;03m# Sliding this window across the sequence allows us to average the logits over the window, so we compute the advantage.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m counteractual_gates_window = \u001b[43mfilter_preceding_gates\u001b[49m(counterfactual_gates, extender)\n\u001b[32m      4\u001b[39m triple_gates_window = filter_preceding_gates(triple_gates, extender)\n\u001b[32m      6\u001b[39m counteractual_gates_window[:\u001b[32m5\u001b[39m, :\u001b[32m20\u001b[39m], counteractual_gates_window.sum()\n",
      "\u001b[31mNameError\u001b[39m: name 'filter_preceding_gates' is not defined"
     ]
    }
   ],
   "source": [
    "extender = torch.ones((10,)) # Sliding this window across the sequence allows us to average the logits over the window, so we compute the advantage.\n",
    "\n",
    "counteractual_gates_window = filter_preceding_gates(counterfactual_gates, extender)\n",
    "triple_gates_window = filter_preceding_gates(triple_gates, extender)\n",
    "\n",
    "counteractual_gates_window[:5, :20], counteractual_gates_window.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_logit_stats(full_next_token_logits, mask):\n",
    "    \"\"\"\n",
    "    full_next_token_logits is a tensor of shape (batch_size, seq_len-1, vocab_size)\n",
    "    mask is a tensor of shape (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    next_token_mask = mask[:, :-1]\n",
    "    n_vals = next_token_mask.sum()\n",
    "\n",
    "    mean_logits = (full_next_token_logits * next_token_mask).sum() / n_vals\n",
    "    std_logits = torch.sqrt((full_next_token_logits**2 * next_token_mask).sum() / n_vals - mean_logits**2)\n",
    "    sterr_logits = std_logits / torch.sqrt(n_vals)\n",
    "\n",
    "    return mean_logits.item(), std_logits.item(), sterr_logits.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_next_token_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mean_triple, std_triple, sterr_triple = masked_logit_stats(\u001b[43mfull_next_token_logits\u001b[49m, triple_gates_window)\n\u001b[32m      2\u001b[39m mean_triple, std_triple, sterr_triple\n",
      "\u001b[31mNameError\u001b[39m: name 'full_next_token_logits' is not defined"
     ]
    }
   ],
   "source": [
    "mean_triple, std_triple, sterr_triple = masked_logit_stats(full_next_token_logits, triple_gates_window)\n",
    "mean_triple, std_triple, sterr_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_next_token_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mean_counterfactual, std_counterfactual, sterr_counterfactual = masked_logit_stats(\u001b[43mfull_next_token_logits\u001b[49m, counteractual_gates_window)\n\u001b[32m      2\u001b[39m mean_counterfactual, std_counterfactual, sterr_counterfactual\n",
      "\u001b[31mNameError\u001b[39m: name 'full_next_token_logits' is not defined"
     ]
    }
   ],
   "source": [
    "mean_counterfactual, std_counterfactual, sterr_counterfactual = masked_logit_stats(full_next_token_logits, counteractual_gates_window)\n",
    "mean_counterfactual, std_counterfactual, sterr_counterfactual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is basically no mean logit difference between being preceded with 111 and 110. Let's check what it looks like for 000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_zero_gates = (sum_preceding_3_gates == 0)\n",
    "triple_zero_gates_window = filter_preceding_gates(triple_zero_gates, extender)\n",
    "mean_triple_zero, std_triple_zero, sterr_triple_zero = masked_logit_stats(full_next_token_logits, triple_zero_gates_window)\n",
    "mean_triple_zero, std_triple_zero, sterr_triple_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, no measurable difference (todo: improve and validate standard deviation calculation, update: these aren't perfect but for some reason the triple zero seems to win)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Let's retry this analysis but at the fine-grained token level. What we'll do is intervene on the preceding two bits and see how this impacts the logit diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_gate_samples[:5, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_logits_by_pattern(full_gate_samples, full_next_token_logits, gates_of_interest, patterns):\n",
    "    \"\"\"\n",
    "    Split logits by multiple patterns\n",
    "    \n",
    "    full_gate_samples is a tensor of shape (batch_size, seq_len)\n",
    "    full_next_token_logits is a tensor of shape (batch_size, seq_len-1)\n",
    "    gates_of_interest is a tensor of shape (pattern_size,)\n",
    "    patterns is a tensor of shape (n_patterns, pattern_size)\n",
    "    \n",
    "    Returns a list of tensors each of shape (split_size, seq_len), one for each pattern which matches the patterns\n",
    "    \"\"\"\n",
    "\n",
    "    n_patterns, _ = patterns.shape\n",
    "\n",
    "    gate_samples_oi = full_gate_samples.index_select(dim=1, index=gates_of_interest) # shape (batch_size, pattern_size)\n",
    "\n",
    "    # Match the pattern to the gate samples\n",
    "    gate_samples_oi = gate_samples_oi.unsqueeze(1) # shape (batch_size, 1, pattern_size)\n",
    "    patterns = patterns.unsqueeze(0)               # shape (1, n_patterns, pattern_size)\n",
    "    matches = (patterns == gate_samples_oi)        # shape (batch_size, n_patterns, pattern_size)\n",
    "    matches = matches.all(dim=2)                   # shape (batch_size, n_patterns)\n",
    "\n",
    "    pattern_matches = [\n",
    "        full_next_token_logits[matches[:, i]] for i in range(n_patterns)\n",
    "    ]\n",
    "\n",
    "    return pattern_matches\n",
    "\n",
    "# Just to check that the above works:\n",
    "test = False\n",
    "if test:\n",
    "    gates_of_interest = torch.tensor([3, 4]).to(device=device)\n",
    "\n",
    "    patterns = torch.tensor([\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ]).to(device=device)\n",
    "\n",
    "    fake_logits = torch.arange(25).reshape(5, 5).to(device=device)\n",
    "    fake_gate_samples = torch.tensor([\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [1, 1, 1, 0, 1],\n",
    "        [0, 0, 0, 1, 1],\n",
    "        [0, 1, 1, 0, 1]\n",
    "    ]).to(device=device)\n",
    "\n",
    "    pattern_matches = split_logits_by_pattern(fake_gate_samples, fake_logits, gates_of_interest, patterns)\n",
    "    print(*pattern_matches, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "gates_of_interest = torch.tensor([40, 41]).to(device=device)\n",
    "\n",
    "pattern_list = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "]\n",
    "\n",
    "patterns = torch.tensor(pattern_list).to(device=device)\n",
    "\n",
    "pattern_matches = split_logits_by_pattern(full_gate_samples, full_next_token_logits, gates_of_interest, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take means \\& standard deviations over the batch dimension.\n",
    "pattern_means = [p.mean(dim=0) for p in pattern_matches]\n",
    "pattern_stds = [p.std(dim=0) for p in pattern_matches]\n",
    "pattern_stderrs = [p.std(dim=0) / np.sqrt(p.shape[0]) for p in pattern_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_to_str(pattern):\n",
    "    return \"\".join(str(p) for p in pattern)\n",
    "\n",
    "base_pattern_mean = pattern_means[0]\n",
    "base_pattern_str = pattern_to_str(pattern_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for pattern, pattern_mean, pattern_stderr in zip(pattern_list, pattern_means, pattern_stderrs):\n",
    "    pattern_str = pattern_to_str(pattern)\n",
    "    visualize_logit_diffs(pattern_mean - base_pattern_mean, pattern_stderr, label=f\"gating pattern {pattern_str}\")\n",
    "\n",
    "min_goi = gates_of_interest.min().item()\n",
    "max_goi = gates_of_interest.max().item()\n",
    "\n",
    "plt.xlim(30, 200)\n",
    "plt.axvline(x=min_goi, color='r', linestyle='--', label=f'Gates of interest (position {min_goi})')\n",
    "plt.axvline(x=max_goi, color='r', linestyle='--', label=f'Gates of interest (position {max_goi})')\n",
    "plt.title(f'Next Token Logit Differences (pattern - {base_pattern_str})')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Logit Difference')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_string[:min_goi], test_string[min_goi:max_goi+1], test_string[max_goi+1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Okay, but what if we marginalise over all prior gates and bytes $x$, and simply ask about what this value looks like:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\log p(x_{i+r} \\vert a_{i-3:i})\n",
    "$$\n",
    "\n",
    "TODO: continute this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_matches_mask(full_gate_samples, pattern):\n",
    "    \"\"\"\n",
    "    returns mask, a tensor of shape (batch_size, seq_len-pattern_size+1) where the ith entry is 1 if the pattern matches gates i, i+1, ..., i+pattern_size-1.\n",
    "    Args:\n",
    "        full_gate_samples: (batch_size, seq_len)\n",
    "        pattern: (pattern_size,)\n",
    "    Returns:\n",
    "        mask: (batch_size, seq_len-pattern_size+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    pattern_size, = pattern.shape\n",
    "\n",
    "    filter_1d = 2*pattern - 1 # This filter can detect the pattern as its max value when slid over the gates.\n",
    "    filter_val = (filter_1d * pattern).sum()\n",
    "\n",
    "    mask = filter_preceding_gates(full_gate_samples, filter_1d)\n",
    "    mask = mask == filter_val\n",
    "    mask = mask[:, pattern_size-1:]\n",
    "\n",
    "    return mask\n",
    "\n",
    "test = True\n",
    "if test:\n",
    "    my_pattern = torch.tensor([1, 0, 0, 0, 1])\n",
    "    my_pattern_mask = pattern_matches_mask(full_gate_samples, my_pattern)\n",
    "    print(f\"{full_gate_samples.shape=}\")\n",
    "    print(full_gate_samples[:5, :20])\n",
    "    print(f\"{my_pattern_mask.shape=}\")\n",
    "    print(my_pattern_mask[:5, :20].int())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "boi = my_pattern_mask.unsqueeze(1) \n",
    "# bruh = full_next_token_logits * boi\n",
    "# bruh.shape\n",
    "boi.shape, full_next_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_logits_following_mean(full_next_token_logits, pattern_mask, offset=0):\n",
    "    \"\"\"\n",
    "    full_next_token_logits is a tensor of shape (batch_size, seq_len-1)\n",
    "    pattern_matches_mask is a tensor of shape (batch_size, seq_len-pattern_size+1)\n",
    "    Returns the mean of the next token logits at i + offset, where the pattern matches i, i+1, ..., i+pattern_size-1.\n",
    "    \"\"\"\n",
    "    seq_len = full_next_token_logits.shape[1] + 1\n",
    "    offset_token_logits = full_next_token_logits[:, offset:]\n",
    "    min_sequence_dim = min(offset_token_logits.shape[1], pattern_mask.shape[1])\n",
    "\n",
    "    offset_token_logits = offset_token_logits[:, :min_sequence_dim]\n",
    "    pattern_mask = pattern_mask[:, :min_sequence_dim]\n",
    "\n",
    "    # TODO: maybe consider comparing to the diff of the mean within a given logit. this could maybe make the output less noisy.\n",
    "\n",
    "    return( (offset_token_logits * pattern_mask).sum() / pattern_mask.sum()).item()\n",
    "\n",
    "def pattern_logits_followning_mean_list(full_next_token_logits, pattern_mask, window_of_interest):\n",
    "    return [\n",
    "        pattern_logits_following_mean(full_next_token_logits, pattern_mask, i)\n",
    "        for i in range(window_of_interest)\n",
    "    ]\n",
    "\n",
    "window_of_interest = 100\n",
    "\n",
    "my_pattern = torch.tensor([1, 0, 0, 0, 1])\n",
    "following_mean_logits = pattern_logits_followning_mean_list(full_next_token_logits, my_pattern_mask, window_of_interest)\n",
    "plt.plot(following_mean_logits)\n",
    "plt.axvline(x=0, color='r', linestyle='--', label=f'Gates of interest (position {min_goi})')\n",
    "plt.axvline(x=my_pattern.shape[0]-1, color='r', linestyle='--', label=f'Gates of interest (position {max_goi})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: maybe consider comparing to the diff of the mean within a given logit. This could maybe make the output less noisy (?).\n",
    "\n",
    "This looks somewhat promising, like the first $1.$ boosts the predictive power and then it decays until the second $1$. Super noisy though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "TODO question: has our model learned induction heads?\n",
    "\n",
    "- Need to analyse the in context learning score to see if there is an improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Analysis of total number of gates vs log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gates = full_gate_samples.sum(dim=1).cpu().numpy()\n",
    "total_logits = full_next_token_logits.sum(dim=1).cpu().numpy()\n",
    "\n",
    "# Compute correlation between total gates and total logits\n",
    "from scipy import stats\n",
    "correlation, p_value = stats.pearsonr(total_gates, total_logits)\n",
    "# Get one-sided p-value for positive correlation\n",
    "one_sided_p_value = p_value / 2 if correlation > 0 else 1 - (p_value / 2)\n",
    "print(f\"Correlation between total gates and total log-likelihood: {correlation:.4f}\")\n",
    "print(f\"P-value for positive correlation: {one_sided_p_value:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.scatter(total_gates, total_logits)\n",
    "plt.xlabel('Total number of gates')\n",
    "plt.ylabel('Total log-likelihood (nats)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
