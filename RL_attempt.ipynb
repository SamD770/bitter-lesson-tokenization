{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and learn a tokenizer with RL : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to count the number of parameters in a torch.nn.Module\n",
    "def count_parameters(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "def parameter_count_string(module):\n",
    "    n_params = count_parameters(module)\n",
    "    if n_params > 10**6:\n",
    "        return f\"{n_params/10**6:.1f}M\"\n",
    "    elif n_params > 10**3:\n",
    "        return f\"{n_params/10**3:.1f}k\"\n",
    "    else:\n",
    "        return f\"{n_params}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/sdauncey/tokenizer_training'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "username = \"sdauncey\"\n",
    "scratch_dir = f\"/scratch/{username}/tokenizer_training\"\n",
    "\n",
    "if not os.path.exists(scratch_dir):\n",
    "    os.makedirs(scratch_dir)\n",
    "\n",
    "scratch_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "train_prompts = [s['prompt'] for s in ds['train']]\n",
    "len(train_prompts)\n",
    "train_prompts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "# # Download a portion of OpenWebText dataset\n",
    "# # This will download a subset of the OpenWebText corpus\n",
    "# print(\"Downloading OpenWebText dataset...\")\n",
    "\n",
    "# # Load a small portion of OpenWebText (1% of the dataset)\n",
    "# openwebtext_8k = datasets.load_dataset(\n",
    "#     \"openwebtext\",\n",
    "#     split=\"train[:1%]\",  # Using only 1% samples of the dataset for now.\n",
    "#     cache_dir=os.path.join(scratch_dir, \"openwebtext_8k_cache\"),\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# print(f\"Downloaded {len(openwebtext_8k)} examples from OpenWebText\")\n",
    "\n",
    "# # Display a sample\n",
    "# print(\"\\nSample text from OpenWebText:\")\n",
    "# print(openwebtext_8k[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BitterLLM(nn.Module):\n",
    "    # Maps bytes to bytes, slowly merging and then unmerging tokens layer-by-layer\n",
    "    # with a context window inversely proportional to the number of tokens in the sequence.#\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_heads: int, dropout: float, downsampling_rates: List[float]):\n",
    "        \n",
    "        # Initialize a standard transformer decoder architecture.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(embedding_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.mid_layer = nn.TransformerDecoderLayer(embedding_dim, num_heads, dropout=dropout)\n",
    "\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(embedding_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # Initialize a gate for each layer.\n",
    "        layer_gate_init = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        # Copy the gate for each layer. \n",
    "        # Initializing by copying inductively biases the model to tokenize in a later layer if the gate is high but the model chose not to.\n",
    "        self.layer_gates = nn.ModuleList([\n",
    "            deepcopy(layer_gate_init) for _ in range(num_layers*2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input is a tensor of shape (batch_size, sequence_length), \n",
    "        gets transformed into a tensor of shape (batch_size, n_tokens, embedding_dim).\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        hidden_states = [x]\n",
    "        merge_destinations = []\n",
    "\n",
    "        for layer, gate in zip(self.down_layers, self.layer_gates):\n",
    "            # TODO: make the context window inversely proportional to the number of tokens in the sequence.\n",
    "            x = layer(x)\n",
    "\n",
    "            # Gate the layer.\n",
    "            gate_logits = gate(x)\n",
    "            gate_probs = F.sigmoid(gate_logits)\n",
    "\n",
    "            if self.training:\n",
    "                # Re-scale the gate probabilities to control the downsampling rate.\n",
    "                ...\n",
    "\n",
    "            gate_samples = torch.bernoulli(gate_probs)\n",
    "\n",
    "            # Merge the tokens where the gate is 1.\n",
    "            # Create a merge destination tensor\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            merge_dst, n_dst = get_merge_dst(gate_samples)\n",
    "            \n",
    "            y = torch.zeros(batch_size, n_dst, self.embedding_dim, dtype=torch.float32)\n",
    "            x = torch.scatter_reduce(y, dim=1, index=merge_dst, src=x, reduce=\"mean\", include_self=False)\n",
    "\n",
    "            hidden_states.append((x, gate_samples, gate_probs))\n",
    "\n",
    "        y = x\n",
    "        for up_layer, down_hidden_state, merge_dst in zip(self.up_layers, reversed(hidden_states), reversed(merge_destinations)):\n",
    "            pass\n",
    "\n",
    "        # TODO: finish implementation.\n",
    "\n",
    "        return self.output_layer(x), hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 797])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = byte5_tokenizer(train_prompts[:5], return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(229))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.min(), test_batch.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '</s>': 1,\n",
       " '<unk>': 2,\n",
       " '\\x00': 3,\n",
       " '\\x01': 4,\n",
       " '\\x02': 5,\n",
       " '\\x03': 6,\n",
       " '\\x04': 7,\n",
       " '\\x05': 8,\n",
       " '\\x06': 9,\n",
       " '\\x07': 10,\n",
       " '\\x08': 11,\n",
       " '\\t': 12,\n",
       " '\\n': 13,\n",
       " '\\x0b': 14,\n",
       " '\\x0c': 15,\n",
       " '\\r': 16,\n",
       " '\\x0e': 17,\n",
       " '\\x0f': 18,\n",
       " '\\x10': 19,\n",
       " '\\x11': 20,\n",
       " '\\x12': 21,\n",
       " '\\x13': 22,\n",
       " '\\x14': 23,\n",
       " '\\x15': 24,\n",
       " '\\x16': 25,\n",
       " '\\x17': 26,\n",
       " '\\x18': 27,\n",
       " '\\x19': 28,\n",
       " '\\x1a': 29,\n",
       " '\\x1b': 30,\n",
       " '\\x1c': 31,\n",
       " '\\x1d': 32,\n",
       " '\\x1e': 33,\n",
       " '\\x1f': 34,\n",
       " ' ': 35,\n",
       " '!': 36,\n",
       " '\"': 37,\n",
       " '#': 38,\n",
       " '$': 39,\n",
       " '%': 40,\n",
       " '&': 41,\n",
       " \"'\": 42,\n",
       " '(': 43,\n",
       " ')': 44,\n",
       " '*': 45,\n",
       " '+': 46,\n",
       " ',': 47,\n",
       " '-': 48,\n",
       " '.': 49,\n",
       " '/': 50,\n",
       " '0': 51,\n",
       " '1': 52,\n",
       " '2': 53,\n",
       " '3': 54,\n",
       " '4': 55,\n",
       " '5': 56,\n",
       " '6': 57,\n",
       " '7': 58,\n",
       " '8': 59,\n",
       " '9': 60,\n",
       " ':': 61,\n",
       " ';': 62,\n",
       " '<': 63,\n",
       " '=': 64,\n",
       " '>': 65,\n",
       " '?': 66,\n",
       " '@': 67,\n",
       " 'A': 68,\n",
       " 'B': 69,\n",
       " 'C': 70,\n",
       " 'D': 71,\n",
       " 'E': 72,\n",
       " 'F': 73,\n",
       " 'G': 74,\n",
       " 'H': 75,\n",
       " 'I': 76,\n",
       " 'J': 77,\n",
       " 'K': 78,\n",
       " 'L': 79,\n",
       " 'M': 80,\n",
       " 'N': 81,\n",
       " 'O': 82,\n",
       " 'P': 83,\n",
       " 'Q': 84,\n",
       " 'R': 85,\n",
       " 'S': 86,\n",
       " 'T': 87,\n",
       " 'U': 88,\n",
       " 'V': 89,\n",
       " 'W': 90,\n",
       " 'X': 91,\n",
       " 'Y': 92,\n",
       " 'Z': 93,\n",
       " '[': 94,\n",
       " '\\\\': 95,\n",
       " ']': 96,\n",
       " '^': 97,\n",
       " '_': 98,\n",
       " '`': 99,\n",
       " 'a': 100,\n",
       " 'b': 101,\n",
       " 'c': 102,\n",
       " 'd': 103,\n",
       " 'e': 104,\n",
       " 'f': 105,\n",
       " 'g': 106,\n",
       " 'h': 107,\n",
       " 'i': 108,\n",
       " 'j': 109,\n",
       " 'k': 110,\n",
       " 'l': 111,\n",
       " 'm': 112,\n",
       " 'n': 113,\n",
       " 'o': 114,\n",
       " 'p': 115,\n",
       " 'q': 116,\n",
       " 'r': 117,\n",
       " 's': 118,\n",
       " 't': 119,\n",
       " 'u': 120,\n",
       " 'v': 121,\n",
       " 'w': 122,\n",
       " 'x': 123,\n",
       " 'y': 124,\n",
       " 'z': 125,\n",
       " '{': 126,\n",
       " '|': 127,\n",
       " '}': 128,\n",
       " '~': 129,\n",
       " '\\x7f': 130,\n",
       " '\\x80': 131,\n",
       " '\\x81': 132,\n",
       " '\\x82': 133,\n",
       " '\\x83': 134,\n",
       " '\\x84': 135,\n",
       " '\\x85': 136,\n",
       " '\\x86': 137,\n",
       " '\\x87': 138,\n",
       " '\\x88': 139,\n",
       " '\\x89': 140,\n",
       " '\\x8a': 141,\n",
       " '\\x8b': 142,\n",
       " '\\x8c': 143,\n",
       " '\\x8d': 144,\n",
       " '\\x8e': 145,\n",
       " '\\x8f': 146,\n",
       " '\\x90': 147,\n",
       " '\\x91': 148,\n",
       " '\\x92': 149,\n",
       " '\\x93': 150,\n",
       " '\\x94': 151,\n",
       " '\\x95': 152,\n",
       " '\\x96': 153,\n",
       " '\\x97': 154,\n",
       " '\\x98': 155,\n",
       " '\\x99': 156,\n",
       " '\\x9a': 157,\n",
       " '\\x9b': 158,\n",
       " '\\x9c': 159,\n",
       " '\\x9d': 160,\n",
       " '\\x9e': 161,\n",
       " '\\x9f': 162,\n",
       " '\\xa0': 163,\n",
       " '¡': 164,\n",
       " '¢': 165,\n",
       " '£': 166,\n",
       " '¤': 167,\n",
       " '¥': 168,\n",
       " '¦': 169,\n",
       " '§': 170,\n",
       " '¨': 171,\n",
       " '©': 172,\n",
       " 'ª': 173,\n",
       " '«': 174,\n",
       " '¬': 175,\n",
       " '\\xad': 176,\n",
       " '®': 177,\n",
       " '¯': 178,\n",
       " '°': 179,\n",
       " '±': 180,\n",
       " '²': 181,\n",
       " '³': 182,\n",
       " '´': 183,\n",
       " 'µ': 184,\n",
       " '¶': 185,\n",
       " '·': 186,\n",
       " '¸': 187,\n",
       " '¹': 188,\n",
       " 'º': 189,\n",
       " '»': 190,\n",
       " '¼': 191,\n",
       " '½': 192,\n",
       " '¾': 193,\n",
       " '¿': 194,\n",
       " 'À': 195,\n",
       " 'Á': 196,\n",
       " 'Â': 197,\n",
       " 'Ã': 198,\n",
       " 'Ä': 199,\n",
       " 'Å': 200,\n",
       " 'Æ': 201,\n",
       " 'Ç': 202,\n",
       " 'È': 203,\n",
       " 'É': 204,\n",
       " 'Ê': 205,\n",
       " 'Ë': 206,\n",
       " 'Ì': 207,\n",
       " 'Í': 208,\n",
       " 'Î': 209,\n",
       " 'Ï': 210,\n",
       " 'Ð': 211,\n",
       " 'Ñ': 212,\n",
       " 'Ò': 213,\n",
       " 'Ó': 214,\n",
       " 'Ô': 215,\n",
       " 'Õ': 216,\n",
       " 'Ö': 217,\n",
       " '×': 218,\n",
       " 'Ø': 219,\n",
       " 'Ù': 220,\n",
       " 'Ú': 221,\n",
       " 'Û': 222,\n",
       " 'Ü': 223,\n",
       " 'Ý': 224,\n",
       " 'Þ': 225,\n",
       " 'ß': 226,\n",
       " 'à': 227,\n",
       " 'á': 228,\n",
       " 'â': 229,\n",
       " 'ã': 230,\n",
       " 'ä': 231,\n",
       " 'å': 232,\n",
       " 'æ': 233,\n",
       " 'ç': 234,\n",
       " 'è': 235,\n",
       " 'é': 236,\n",
       " 'ê': 237,\n",
       " 'ë': 238,\n",
       " 'ì': 239,\n",
       " 'í': 240,\n",
       " 'î': 241,\n",
       " 'ï': 242,\n",
       " 'ð': 243,\n",
       " 'ñ': 244,\n",
       " 'ò': 245,\n",
       " 'ó': 246,\n",
       " 'ô': 247,\n",
       " 'õ': 248,\n",
       " 'ö': 249,\n",
       " '÷': 250,\n",
       " 'ø': 251,\n",
       " 'ù': 252,\n",
       " 'ú': 253,\n",
       " 'û': 254,\n",
       " 'ü': 255,\n",
       " 'ý': 256,\n",
       " 'þ': 257,\n",
       " 'ÿ': 258,\n",
       " '<extra_id_0>': 259,\n",
       " '<extra_id_1>': 260,\n",
       " '<extra_id_2>': 261,\n",
       " '<extra_id_3>': 262,\n",
       " '<extra_id_4>': 263,\n",
       " '<extra_id_5>': 264,\n",
       " '<extra_id_6>': 265,\n",
       " '<extra_id_7>': 266,\n",
       " '<extra_id_8>': 267,\n",
       " '<extra_id_9>': 268,\n",
       " '<extra_id_10>': 269,\n",
       " '<extra_id_11>': 270,\n",
       " '<extra_id_12>': 271,\n",
       " '<extra_id_13>': 272,\n",
       " '<extra_id_14>': 273,\n",
       " '<extra_id_15>': 274,\n",
       " '<extra_id_16>': 275,\n",
       " '<extra_id_17>': 276,\n",
       " '<extra_id_18>': 277,\n",
       " '<extra_id_19>': 278,\n",
       " '<extra_id_20>': 279,\n",
       " '<extra_id_21>': 280,\n",
       " '<extra_id_22>': 281,\n",
       " '<extra_id_23>': 282,\n",
       " '<extra_id_24>': 283,\n",
       " '<extra_id_25>': 284,\n",
       " '<extra_id_26>': 285,\n",
       " '<extra_id_27>': 286,\n",
       " '<extra_id_28>': 287,\n",
       " '<extra_id_29>': 288,\n",
       " '<extra_id_30>': 289,\n",
       " '<extra_id_31>': 290,\n",
       " '<extra_id_32>': 291,\n",
       " '<extra_id_33>': 292,\n",
       " '<extra_id_34>': 293,\n",
       " '<extra_id_35>': 294,\n",
       " '<extra_id_36>': 295,\n",
       " '<extra_id_37>': 296,\n",
       " '<extra_id_38>': 297,\n",
       " '<extra_id_39>': 298,\n",
       " '<extra_id_40>': 299,\n",
       " '<extra_id_41>': 300,\n",
       " '<extra_id_42>': 301,\n",
       " '<extra_id_43>': 302,\n",
       " '<extra_id_44>': 303,\n",
       " '<extra_id_45>': 304,\n",
       " '<extra_id_46>': 305,\n",
       " '<extra_id_47>': 306,\n",
       " '<extra_id_48>': 307,\n",
       " '<extra_id_49>': 308,\n",
       " '<extra_id_50>': 309,\n",
       " '<extra_id_51>': 310,\n",
       " '<extra_id_52>': 311,\n",
       " '<extra_id_53>': 312,\n",
       " '<extra_id_54>': 313,\n",
       " '<extra_id_55>': 314,\n",
       " '<extra_id_56>': 315,\n",
       " '<extra_id_57>': 316,\n",
       " '<extra_id_58>': 317,\n",
       " '<extra_id_59>': 318,\n",
       " '<extra_id_60>': 319,\n",
       " '<extra_id_61>': 320,\n",
       " '<extra_id_62>': 321,\n",
       " '<extra_id_63>': 322,\n",
       " '<extra_id_64>': 323,\n",
       " '<extra_id_65>': 324,\n",
       " '<extra_id_66>': 325,\n",
       " '<extra_id_67>': 326,\n",
       " '<extra_id_68>': 327,\n",
       " '<extra_id_69>': 328,\n",
       " '<extra_id_70>': 329,\n",
       " '<extra_id_71>': 330,\n",
       " '<extra_id_72>': 331,\n",
       " '<extra_id_73>': 332,\n",
       " '<extra_id_74>': 333,\n",
       " '<extra_id_75>': 334,\n",
       " '<extra_id_76>': 335,\n",
       " '<extra_id_77>': 336,\n",
       " '<extra_id_78>': 337,\n",
       " '<extra_id_79>': 338,\n",
       " '<extra_id_80>': 339,\n",
       " '<extra_id_81>': 340,\n",
       " '<extra_id_82>': 341,\n",
       " '<extra_id_83>': 342,\n",
       " '<extra_id_84>': 343,\n",
       " '<extra_id_85>': 344,\n",
       " '<extra_id_86>': 345,\n",
       " '<extra_id_87>': 346,\n",
       " '<extra_id_88>': 347,\n",
       " '<extra_id_89>': 348,\n",
       " '<extra_id_90>': 349,\n",
       " '<extra_id_91>': 350,\n",
       " '<extra_id_92>': 351,\n",
       " '<extra_id_93>': 352,\n",
       " '<extra_id_94>': 353,\n",
       " '<extra_id_95>': 354,\n",
       " '<extra_id_96>': 355,\n",
       " '<extra_id_97>': 356,\n",
       " '<extra_id_98>': 357,\n",
       " '<extra_id_99>': 358,\n",
       " '<extra_id_100>': 359,\n",
       " '<extra_id_101>': 360,\n",
       " '<extra_id_102>': 361,\n",
       " '<extra_id_103>': 362,\n",
       " '<extra_id_104>': 363,\n",
       " '<extra_id_105>': 364,\n",
       " '<extra_id_106>': 365,\n",
       " '<extra_id_107>': 366,\n",
       " '<extra_id_108>': 367,\n",
       " '<extra_id_109>': 368,\n",
       " '<extra_id_110>': 369,\n",
       " '<extra_id_111>': 370,\n",
       " '<extra_id_112>': 371,\n",
       " '<extra_id_113>': 372,\n",
       " '<extra_id_114>': 373,\n",
       " '<extra_id_115>': 374,\n",
       " '<extra_id_116>': 375,\n",
       " '<extra_id_117>': 376,\n",
       " '<extra_id_118>': 377,\n",
       " '<extra_id_119>': 378,\n",
       " '<extra_id_120>': 379,\n",
       " '<extra_id_121>': 380,\n",
       " '<extra_id_122>': 381,\n",
       " '<extra_id_123>': 382,\n",
       " '<extra_id_124>': 383}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte5_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_model has 4.8M parameters\n"
     ]
    }
   ],
   "source": [
    "def get_merge_dst(gate_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns (merge_dst, dst_idx) the merge destination for each token in the sequence and the number of unique merge destinations.\n",
    "    For now, has a janky python for-loop implementation.\n",
    "    Input is a tensor of shape (batch_size, sequence_length) with 0 tokens are merged into the next 1 token.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = gate_samples.shape\n",
    "    merge_dst = torch.zeros_like(gate_samples, dtype=torch.long)\n",
    "    n_dst = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    # Process each batch separately\n",
    "    for b in range(batch_size):\n",
    "        dst_idx = 0\n",
    "        for i in range(seq_len):\n",
    "            merge_dst[b, i] = dst_idx\n",
    "            if gate_samples[b, i] == 1 and i < seq_len - 1:\n",
    "                # If previous position had gate=1, keep the same destination\n",
    "                dst_idx += 1\n",
    "\n",
    "        n_dst[b] = dst_idx + 1\n",
    "\n",
    "    return merge_dst, n_dst\n",
    "\n",
    "\n",
    "class MiniBitterLLM(nn.Module):\n",
    "    # A mini BitterLLM with 2 down, 4 mid, and 2 up layers. As a vibe check on the idea.\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, num_heads: int, dropout: float=0.01, downsample_rate: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, dropout=dropout, batch_first=True) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.mid_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, dropout=dropout, batch_first=True) for _ in range(4)\n",
    "        ])\n",
    "\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, dropout=dropout, batch_first=True) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        # Initialize a gate for each layer.\n",
    "        layer_gate_init = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        # Copy the gate for each layer. \n",
    "        # Initializing by copying inductively biases the model to tokenize in a later layer if the gate is high but the model chose not to.\n",
    "        self.down_layer_gate = deepcopy(layer_gate_init)\n",
    "        self.up_layer_gate = deepcopy(layer_gate_init)\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "    def apply_local_layers(self, layers, x: torch.Tensor, context_window_length) -> torch.Tensor:\n",
    "        \"\"\"Again a janky python for-loop implementation that re-constructs the causal mask for each layer.\"\"\"\n",
    "        _, seq_len, _ = x.shape\n",
    "\n",
    "        # Create causal mask for context length of 64\n",
    "        mask = torch.ones(seq_len, seq_len) * float('-inf')\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # Allow attention to self and previous window_size tokens\n",
    "            start_idx = max(0, i - context_window_length + 1)\n",
    "            mask[i, start_idx:i+1] = 0.0\n",
    "        \n",
    "        # Process through down layers with the specified context length\n",
    "        for layer in layers:\n",
    "            x = layer(x, src_mask=mask, is_causal=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        batch_size, _ = x.shape\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Apply down layers  byte tokens        \n",
    "        x = self.apply_local_layers(self.down_layers, x, 64)\n",
    "\n",
    "        # Sample gating binary variables for each token.\n",
    "        gate_logits = self.down_layer_gate(x)\n",
    "        gate_probs = F.sigmoid(gate_logits)\n",
    "\n",
    "        if self.training:\n",
    "            # Re-scale the gate probabilities to control the downsampling rate.\n",
    "            true_gate_probs = gate_probs * (self.downsample_rate / gate_probs.mean())\n",
    "\n",
    "        gate_samples = torch.bernoulli(true_gate_probs)\n",
    "\n",
    "        # Merge the tokens into the next token where the gate is 1.\n",
    "        gate_samples = gate_samples.squeeze(-1)\n",
    "        merge_dst, n_dst = get_merge_dst(gate_samples)\n",
    "        merge_dst = merge_dst.unsqueeze(-1).expand(-1, -1, self.embedding_dim)\n",
    "\n",
    "        x_downsampled = torch.zeros(batch_size, n_dst.max(), self.embedding_dim, dtype=torch.float32)\n",
    "        x_downsampled = torch.scatter_reduce(x_downsampled, dim=1, index=merge_dst, src=x, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        # Apply mid layers to merged tokens and compute the deviation\n",
    "        y_downsampled = self.apply_local_layers(self.mid_layers, x_downsampled, 64*4)\n",
    "        deviation = y_downsampled - x_downsampled\n",
    "\n",
    "        # Add the upsampled deviation to the input to the middle layers\n",
    "        upsampled_deviation = torch.gather(deviation, dim=1, index=merge_dst)\n",
    "        y = x + upsampled_deviation\n",
    "\n",
    "        # Apply up layers to byte tokens\n",
    "        y = self.apply_local_layers(self.up_layers, y, 64)\n",
    "\n",
    "        # Map residual stream to logits\n",
    "        logits = self.output_layer(y)\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        out = {\n",
    "            \"logits\": logits,\n",
    "            \"gate_probs\": true_gate_probs.squeeze(-1),\n",
    "            \"gate_samples\": gate_samples.to(dtype=torch.long),\n",
    "            \"merge_dst\": merge_dst[:, :, 0], # This dimension is repeated.\n",
    "            \"n_dst\": n_dst,\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "my_model = MiniBitterLLM(vocab_size=byte5_tokenizer.vocab_size, embedding_dim=128, num_heads=4, downsample_rate=0.25)\n",
    "print(f\"my_model has {parameter_count_string(my_model)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k='logits' v.shape=torch.Size([5, 797, 256]), v.dtype=torch.float32\n",
      "k='gate_probs' v.shape=torch.Size([5, 797]), v.dtype=torch.float32\n",
      "k='gate_samples' v.shape=torch.Size([5, 797]), v.dtype=torch.int64\n",
      "k='merge_dst' v.shape=torch.Size([5, 797]), v.dtype=torch.int64\n",
      "k='n_dst' v.shape=torch.Size([5]), v.dtype=torch.int64\n"
     ]
    }
   ],
   "source": [
    "out = my_model(test_batch)\n",
    "\n",
    "for k, v in out.items():\n",
    "    print(f\"{k=} {v.shape=}, {v.dtype=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = my_model(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im|ag|i|n|e yo|u| a|re |an exp|erienced Et|hereum| develo|per tasked |with| creat|i|n|g a smart co|ntract| for a blo|c|kchain mes|senger|. |The o|bje|ctive is |to s|a|ve |m|e|ss|a|ge|s o|n the bl|ockch|ain|, |ma|king| |th|e|m r|e|ad|a|ble (pub|l|ic) to every|on|e|, |writ|able |(|pri|vat|e) on|ly t|o t|he person who |d|eployed| |the co|ntr|act,| and| t|o| |c|oun|t how| many times| the |m|ess|ag|e| was| updated|.| Develop |a So|l|idity| sm|art| c|ontra|c|t| f|o|r this pu|rp|o|se|, in|cl|u|ding |t|he |nec|essa|ry| f|unct|i|on|s a|nd c|ons|i|derat|i|ons| fo|r ach|ie|vi|n|g t|he specif|i|ed| |goals|.| |Ple|as|e |p|rov|i|de t|h|e code| |a|nd any| relev|ant| |expl|ana|tions| t|o e|n|s|u|re a |c|lear |u|nde|rstanding |of t|h|e i|mp|lement|ati|on|.</s><pad><pad><pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad>|<pad>|<pad><pad><pad><pad><pad><pad><pad>|<pad><pad><pad>|<pad><pad><pad>|<pad><pad><pad><pad><pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>|<pad><pad><pad>|<pad><pad><pad><pad><pad><pad>|<pad><pad><pad><pad><pad>|<pad><pad><pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad>|<pad>|<pad>|<pad><pad><pad><pad><pad><pad><pad><pad><pad>|<pad><pad>|<pad>|<pad><pad><pad>|<pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad>|<pad>|<pad><pad>|<pad><pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>|<pad><pad><pad><pad>|<pad>|<pad><pad>|<pad><pad><pad><pad>|<pad><pad>|<pad>|<pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad><pad>|<pad>|<pad><pad><pad><pad><pad><pad><pad>|<pad><pad><pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>|<pad><pad><pad><pad>|<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>|<pad><pad><pad>|<pad><pad><pad>|<pad>|<pad><pad><pad><pad><pad><pad><pad><pad>|<pad><pad><pad><pad>|<pad><pad><pad><pad><pad>|<pad><pad>"
     ]
    }
   ],
   "source": [
    "def display_gating(tokens_ids, merge_dst):\n",
    "    \"\"\"Display how a SmallBitterLLM merges a sequence. token_ids and merge_dst are tensors of shape (sequence_length,).\"\"\"\n",
    "    previous_merge_dst = 0\n",
    "    for t_id, merge_destinantion in zip(tokens_ids, merge_dst):\n",
    "        merge_destinantion = merge_destinantion.item()\n",
    "        \n",
    "        if merge_destinantion != previous_merge_dst:\n",
    "            print(f\"|\", end=\"\")\n",
    "            previous_merge_dst = merge_destinantion\n",
    "        \n",
    "        t_txt = byte5_tokenizer.decode(t_id)\n",
    "        print(f\"{t_txt}\", end=\"\")\n",
    "\n",
    "display_gating(test_batch[0], out[\"merge_dst\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar_loss': 2.5473217964172363,\n",
       " 'gating_loss': 0.30918932459530757,\n",
       " 'total_loss': 2.856511121012544}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_discounted_rewards(rewards, discount):\n",
    "    \"\"\"\n",
    "    Assumes that rewards is a numpy array of shape (n_episodes, n_timesteps). Returns tensor of same shape.\n",
    "    credit to: https://stackoverflow.com/questions/47970683/vectorize-a-numpy-discount-calculation/47971187#47971187,\n",
    "    minor modifications made to vectorise computation.\n",
    "    C[i] = R[i] + discount * C[i+1]\n",
    "    signal.lfilter(b, a, x, axis=-1, zi=None)\n",
    "    a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
    "                          - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
    "    \"\"\"\n",
    "    r = rewards[:, ::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = lfilter(b, a, x=r)\n",
    "    return y[:, ::-1]\n",
    "\n",
    "\n",
    "def discounted_rewards_torch(rewards, discount):\n",
    "    \"\"\"torch wrapper for compute_discounted_rewards. Warning: does _not_ allow for backprop through the rewards, which is fine for policy gradients.\"\"\"\n",
    "    rewards = rewards.detach().numpy()\n",
    "    discounted_rewards = compute_discounted_rewards(rewards, discount)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards.copy()) # Copy as torch doesn't like converting negatively strided arrays\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def bitter_tokenizer_training_step(model, batch):\n",
    "    \"\"\"\n",
    "    Assume that batch is torch.tensor of token ids of shape (batch, sequence_length). returns a dict of floats of the training losses for the batch.\n",
    "    \"\"\"\n",
    "    batch_size, _ = batch.shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(batch)\n",
    "    logits = out[\"logits\"]\n",
    "    gate_samples = out[\"gate_samples\"]\n",
    "    gate_probs = out[\"gate_probs\"]\n",
    "\n",
    "\n",
    "    # Compute autoregressive loss: log probability of next token.\n",
    "    next_token_ids = batch[:, 1:]\n",
    "    current_token_logits = logits[:, :-1]\n",
    "    next_token_logits = F.cross_entropy(current_token_logits.transpose(1, 2), next_token_ids, reduction=\"none\") # Transpose as F.cross_entropy wants shape [batch, classes, ...]\n",
    "    ar_loss = next_token_logits.mean()\n",
    "\n",
    "    # Compute gating loss: discounted log probabilities of following token(s).\n",
    "    discount_rate = 0.95\n",
    "    next_token_logits_padded = torch.cat([next_token_logits, torch.zeros(batch_size, 1)], dim=-1) # Pad the last reward as zero\n",
    "    discounted_rewards = discounted_rewards_torch(next_token_logits_padded, discount_rate)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean())\n",
    "\n",
    "    # action 0 = continue, action 1 = gate\n",
    "    action_log_probs = torch.stack([(1 - gate_probs).log() , gate_probs.log()], dim=1)\n",
    "    selected_action_log_probs = F.cross_entropy(action_log_probs, gate_samples, reduction=\"none\")\n",
    "    gating_loss = (discounted_rewards * selected_action_log_probs).mean()\n",
    "\n",
    "    # Optimizer step\n",
    "    total_loss = ar_loss + gating_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    out = {\n",
    "        \"ar_loss\": ar_loss.item(),\n",
    "        \"gating_loss\": gating_loss.item(),\n",
    "        \"total_loss\": total_loss.item()\n",
    "    }\n",
    "\n",
    "    return out\n",
    "\n",
    "bitter_tokenizer_training_step(my_model, test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
