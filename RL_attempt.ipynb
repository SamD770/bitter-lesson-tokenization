{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and learn a tokenizer with RL : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "username = \"sdauncey\"\n",
    "scratch_dir = f\"/scratch/{username}/tokenizer_training\"\n",
    "\n",
    "if not os.path.exists(scratch_dir):\n",
    "    os.makedirs(scratch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading OpenWebText dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3c5ee4e1294450bd814d6475f8c91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/21 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf20edcdf2f45b78dde3f1457e3f6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 80138 examples from OpenWebText\n",
      "\n",
      "Sample text from OpenWebText:\n",
      "Port-au-Prince, Haiti (CNN) -- Earthquake victims, writhing in pain and grasping at life, watched doctors and nurses walk away from a field hospital Friday night after a Belgian medical team evacuated the area, saying it was concerned about security.\n",
      "\n",
      "The decision left CNN Chief Medical Correspondent Sanjay Gupta as the only doctor at the hospital to get the patients through the night.\n",
      "\n",
      "CNN initially reported, based on conversations with some of the doctors, that the United Nations ordered the B...\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# Download a portion of OpenWebText dataset\n",
    "# This will download a subset of the OpenWebText corpus\n",
    "print(\"Downloading OpenWebText dataset...\")\n",
    "\n",
    "# Load a small portion of OpenWebText (1% of the dataset)\n",
    "openwebtext = datasets.load_dataset(\n",
    "    \"openwebtext\",\n",
    "    split=\"train[:1%]\",  # Using only 1% of the data to keep it manageable\n",
    "    cache_dir=os.path.join(scratch_dir, \"openwebtext_cache\"),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Downloaded {len(openwebtext)} examples from OpenWebText\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample text from OpenWebText:\")\n",
    "print(openwebtext[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 80138\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openwebtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5516, 10286, 6021)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(openwebtext[0][\"text\"]), len(openwebtext[1][\"text\"]), len(openwebtext[2][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83,\n",
       " 114,\n",
       " 117,\n",
       " 119,\n",
       " 48,\n",
       " 100,\n",
       " 120,\n",
       " 48,\n",
       " 83,\n",
       " 117,\n",
       " 108,\n",
       " 113,\n",
       " 102,\n",
       " 104,\n",
       " 47,\n",
       " 35,\n",
       " 75,\n",
       " 100,\n",
       " 108,\n",
       " 119,\n",
       " 108,\n",
       " 35,\n",
       " 43,\n",
       " 70,\n",
       " 81,\n",
       " 81,\n",
       " 44,\n",
       " 35,\n",
       " 48,\n",
       " 48,\n",
       " 35,\n",
       " 72,\n",
       " 100,\n",
       " 117,\n",
       " 119,\n",
       " 107,\n",
       " 116,\n",
       " 120,\n",
       " 100,\n",
       " 110,\n",
       " 104,\n",
       " 35,\n",
       " 121,\n",
       " 108,\n",
       " 102,\n",
       " 119,\n",
       " 108,\n",
       " 112,\n",
       " 118,\n",
       " 47,\n",
       " 35,\n",
       " 122,\n",
       " 117,\n",
       " 108,\n",
       " 119,\n",
       " 107,\n",
       " 108,\n",
       " 113,\n",
       " 106,\n",
       " 35,\n",
       " 108,\n",
       " 113,\n",
       " 35,\n",
       " 115,\n",
       " 100,\n",
       " 108,\n",
       " 113,\n",
       " 35,\n",
       " 100,\n",
       " 113,\n",
       " 103,\n",
       " 35,\n",
       " 106,\n",
       " 117,\n",
       " 100,\n",
       " 118,\n",
       " 115,\n",
       " 108,\n",
       " 113,\n",
       " 106,\n",
       " 35,\n",
       " 100,\n",
       " 119,\n",
       " 35,\n",
       " 111,\n",
       " 108,\n",
       " 105,\n",
       " 104,\n",
       " 47,\n",
       " 35,\n",
       " 122,\n",
       " 100,\n",
       " 119,\n",
       " 102,\n",
       " 107,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 103,\n",
       " 114,\n",
       " 102,\n",
       " 119,\n",
       " 114,\n",
       " 117,\n",
       " 118,\n",
       " 35,\n",
       " 100,\n",
       " 113,\n",
       " 103,\n",
       " 35,\n",
       " 113,\n",
       " 120,\n",
       " 117,\n",
       " 118,\n",
       " 104,\n",
       " 118,\n",
       " 35,\n",
       " 122,\n",
       " 100,\n",
       " 111,\n",
       " 110,\n",
       " 35,\n",
       " 100,\n",
       " 122,\n",
       " 100,\n",
       " 124,\n",
       " 35,\n",
       " 105,\n",
       " 117,\n",
       " 114,\n",
       " 112,\n",
       " 35,\n",
       " 100,\n",
       " 35,\n",
       " 105,\n",
       " 108,\n",
       " 104,\n",
       " 111,\n",
       " 103,\n",
       " 35,\n",
       " 107,\n",
       " 114,\n",
       " 118,\n",
       " 115,\n",
       " 108,\n",
       " 119,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 73,\n",
       " 117,\n",
       " 108,\n",
       " 103,\n",
       " 100,\n",
       " 124,\n",
       " 35,\n",
       " 113,\n",
       " 108,\n",
       " 106,\n",
       " 107,\n",
       " 119,\n",
       " 35,\n",
       " 100,\n",
       " 105,\n",
       " 119,\n",
       " 104,\n",
       " 117,\n",
       " 35,\n",
       " 100,\n",
       " 35,\n",
       " 69,\n",
       " 104,\n",
       " 111,\n",
       " 106,\n",
       " 108,\n",
       " 100,\n",
       " 113,\n",
       " 35,\n",
       " 112,\n",
       " 104,\n",
       " 103,\n",
       " 108,\n",
       " 102,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 119,\n",
       " 104,\n",
       " 100,\n",
       " 112,\n",
       " 35,\n",
       " 104,\n",
       " 121,\n",
       " 100,\n",
       " 102,\n",
       " 120,\n",
       " 100,\n",
       " 119,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 100,\n",
       " 117,\n",
       " 104,\n",
       " 100,\n",
       " 47,\n",
       " 35,\n",
       " 118,\n",
       " 100,\n",
       " 124,\n",
       " 108,\n",
       " 113,\n",
       " 106,\n",
       " 35,\n",
       " 108,\n",
       " 119,\n",
       " 35,\n",
       " 122,\n",
       " 100,\n",
       " 118,\n",
       " 35,\n",
       " 102,\n",
       " 114,\n",
       " 113,\n",
       " 102,\n",
       " 104,\n",
       " 117,\n",
       " 113,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 100,\n",
       " 101,\n",
       " 114,\n",
       " 120,\n",
       " 119,\n",
       " 35,\n",
       " 118,\n",
       " 104,\n",
       " 102,\n",
       " 120,\n",
       " 117,\n",
       " 108,\n",
       " 119,\n",
       " 124,\n",
       " 49,\n",
       " 13,\n",
       " 13,\n",
       " 87,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 103,\n",
       " 104,\n",
       " 102,\n",
       " 108,\n",
       " 118,\n",
       " 108,\n",
       " 114,\n",
       " 113,\n",
       " 35,\n",
       " 111,\n",
       " 104,\n",
       " 105,\n",
       " 119,\n",
       " 35,\n",
       " 70,\n",
       " 81,\n",
       " 81,\n",
       " 35,\n",
       " 70,\n",
       " 107,\n",
       " 108,\n",
       " 104,\n",
       " 105,\n",
       " 35,\n",
       " 80,\n",
       " 104,\n",
       " 103,\n",
       " 108,\n",
       " 102,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 70,\n",
       " 114,\n",
       " 117,\n",
       " 117,\n",
       " 104,\n",
       " 118,\n",
       " 115,\n",
       " 114,\n",
       " 113,\n",
       " 103,\n",
       " 104,\n",
       " 113,\n",
       " 119,\n",
       " 35,\n",
       " 86,\n",
       " 100,\n",
       " 113,\n",
       " 109,\n",
       " 100,\n",
       " 124,\n",
       " 35,\n",
       " 74,\n",
       " 120,\n",
       " 115,\n",
       " 119,\n",
       " 100,\n",
       " 35,\n",
       " 100,\n",
       " 118,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 114,\n",
       " 113,\n",
       " 111,\n",
       " 124,\n",
       " 35,\n",
       " 103,\n",
       " 114,\n",
       " 102,\n",
       " 119,\n",
       " 114,\n",
       " 117,\n",
       " 35,\n",
       " 100,\n",
       " 119,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 107,\n",
       " 114,\n",
       " 118,\n",
       " 115,\n",
       " 108,\n",
       " 119,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 119,\n",
       " 114,\n",
       " 35,\n",
       " 106,\n",
       " 104,\n",
       " 119,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 115,\n",
       " 100,\n",
       " 119,\n",
       " 108,\n",
       " 104,\n",
       " 113,\n",
       " 119,\n",
       " 118,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 117,\n",
       " 114,\n",
       " 120,\n",
       " 106,\n",
       " 107,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 113,\n",
       " 108,\n",
       " 106,\n",
       " 107,\n",
       " 119,\n",
       " 49,\n",
       " 13,\n",
       " 13,\n",
       " 70,\n",
       " 81,\n",
       " 81,\n",
       " 35,\n",
       " 108,\n",
       " 113,\n",
       " 108,\n",
       " 119,\n",
       " 108,\n",
       " 100,\n",
       " 111,\n",
       " 111,\n",
       " 124,\n",
       " 35,\n",
       " 117,\n",
       " 104,\n",
       " 115,\n",
       " 114,\n",
       " 117,\n",
       " 119,\n",
       " 104,\n",
       " 103,\n",
       " 47,\n",
       " 35,\n",
       " 101,\n",
       " 100,\n",
       " 118,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 114,\n",
       " 113,\n",
       " 35,\n",
       " 102,\n",
       " 114,\n",
       " 113,\n",
       " 121,\n",
       " 104,\n",
       " 117,\n",
       " 118,\n",
       " 100,\n",
       " 119,\n",
       " 108,\n",
       " 114,\n",
       " 113,\n",
       " 118,\n",
       " 35,\n",
       " 122,\n",
       " 108,\n",
       " 119,\n",
       " 107,\n",
       " 35,\n",
       " 118,\n",
       " 114,\n",
       " 112,\n",
       " 104,\n",
       " 35,\n",
       " 114,\n",
       " 105,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 103,\n",
       " 114,\n",
       " 102,\n",
       " 119,\n",
       " 114,\n",
       " 117,\n",
       " 118,\n",
       " 47,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 100,\n",
       " 119,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 88,\n",
       " 113,\n",
       " 108,\n",
       " 119,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 81,\n",
       " 100,\n",
       " 119,\n",
       " 108,\n",
       " 114,\n",
       " 113,\n",
       " 118,\n",
       " 35,\n",
       " 114,\n",
       " 117,\n",
       " 103,\n",
       " 104,\n",
       " 117,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 69,\n",
       " 104,\n",
       " 111,\n",
       " 106,\n",
       " 108,\n",
       " 100,\n",
       " 113,\n",
       " 35,\n",
       " 73,\n",
       " 108,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 35,\n",
       " 68,\n",
       " 108,\n",
       " 103,\n",
       " 35,\n",
       " 100,\n",
       " 113,\n",
       " 103,\n",
       " 35,\n",
       " 86,\n",
       " 120,\n",
       " 115,\n",
       " 115,\n",
       " 114,\n",
       " 117,\n",
       " 119,\n",
       " 35,\n",
       " 87,\n",
       " 104,\n",
       " 100,\n",
       " 112,\n",
       " 35,\n",
       " 119,\n",
       " 114,\n",
       " 35,\n",
       " 104,\n",
       " 121,\n",
       " 100,\n",
       " 102,\n",
       " 120,\n",
       " 100,\n",
       " 119,\n",
       " 104,\n",
       " 49,\n",
       " 35,\n",
       " 75,\n",
       " 114,\n",
       " 122,\n",
       " 104,\n",
       " 121,\n",
       " 104,\n",
       " 117,\n",
       " 47,\n",
       " 35,\n",
       " 69,\n",
       " 104,\n",
       " 111,\n",
       " 106,\n",
       " 108,\n",
       " 100,\n",
       " 113,\n",
       " 35,\n",
       " 70,\n",
       " 107,\n",
       " 108,\n",
       " 104,\n",
       " 105,\n",
       " 35,\n",
       " 70,\n",
       " 114,\n",
       " 114,\n",
       " 117,\n",
       " 103,\n",
       " 108,\n",
       " 113,\n",
       " 100,\n",
       " 119,\n",
       " 114,\n",
       " 117,\n",
       " 35,\n",
       " 74,\n",
       " 104,\n",
       " 104,\n",
       " 117,\n",
       " 119,\n",
       " 35,\n",
       " 74,\n",
       " 108,\n",
       " 109,\n",
       " 118,\n",
       " 47,\n",
       " 35,\n",
       " 100,\n",
       " 35,\n",
       " 103,\n",
       " 114,\n",
       " 102,\n",
       " 119,\n",
       " 114,\n",
       " 117,\n",
       " 35,\n",
       " 122,\n",
       " 107,\n",
       " 114,\n",
       " 35,\n",
       " 122,\n",
       " 100,\n",
       " 118,\n",
       " 35,\n",
       " 100,\n",
       " 119,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 107,\n",
       " 114,\n",
       " 118,\n",
       " 115,\n",
       " 108,\n",
       " 119,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 122,\n",
       " 108,\n",
       " 119,\n",
       " 107,\n",
       " 35,\n",
       " 57,\n",
       " 51,\n",
       " 35,\n",
       " 69,\n",
       " 104,\n",
       " 111,\n",
       " 106,\n",
       " 108,\n",
       " 100,\n",
       " 113,\n",
       " 35,\n",
       " 112,\n",
       " 104,\n",
       " 103,\n",
       " 108,\n",
       " 102,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 115,\n",
       " 104,\n",
       " 117,\n",
       " 118,\n",
       " 114,\n",
       " 113,\n",
       " 113,\n",
       " 104,\n",
       " 111,\n",
       " 47,\n",
       " 35,\n",
       " 118,\n",
       " 100,\n",
       " 108,\n",
       " 103,\n",
       " 35,\n",
       " 108,\n",
       " 119,\n",
       " 35,\n",
       " 122,\n",
       " 100,\n",
       " 118,\n",
       " 35,\n",
       " 107,\n",
       " 108,\n",
       " 118,\n",
       " 35,\n",
       " 103,\n",
       " 104,\n",
       " 102,\n",
       " 108,\n",
       " 118,\n",
       " 108,\n",
       " 114,\n",
       " 113,\n",
       " 35,\n",
       " 119,\n",
       " 114,\n",
       " 35,\n",
       " 115,\n",
       " 120,\n",
       " 111,\n",
       " 111,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 119,\n",
       " 104,\n",
       " 100,\n",
       " 112,\n",
       " 35,\n",
       " 114,\n",
       " 120,\n",
       " 119,\n",
       " 35,\n",
       " 105,\n",
       " 114,\n",
       " 117,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 113,\n",
       " 108,\n",
       " 106,\n",
       " 107,\n",
       " 119,\n",
       " 49,\n",
       " 35,\n",
       " 74,\n",
       " 108,\n",
       " 109,\n",
       " 118,\n",
       " 35,\n",
       " 118,\n",
       " 100,\n",
       " 108,\n",
       " 103,\n",
       " 35,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 117,\n",
       " 104,\n",
       " 116,\n",
       " 120,\n",
       " 104,\n",
       " 118,\n",
       " 119,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 88,\n",
       " 49,\n",
       " 81,\n",
       " 49,\n",
       " 35,\n",
       " 118,\n",
       " 104,\n",
       " 102,\n",
       " 120,\n",
       " 117,\n",
       " 108,\n",
       " 119,\n",
       " 124,\n",
       " 35,\n",
       " 115,\n",
       " 104,\n",
       " 117,\n",
       " 118,\n",
       " 114,\n",
       " 113,\n",
       " 113,\n",
       " 104,\n",
       " 111,\n",
       " 35,\n",
       " 119,\n",
       " 114,\n",
       " 35,\n",
       " 118,\n",
       " 119,\n",
       " 100,\n",
       " 105,\n",
       " 105,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 107,\n",
       " 114,\n",
       " 118,\n",
       " 115,\n",
       " 108,\n",
       " 119,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 114,\n",
       " 121,\n",
       " 104,\n",
       " 117,\n",
       " 113,\n",
       " 108,\n",
       " 106,\n",
       " 107,\n",
       " 119,\n",
       " 47,\n",
       " 35,\n",
       " 101,\n",
       " 120,\n",
       " 119,\n",
       " 35,\n",
       " 122,\n",
       " 100,\n",
       " 118,\n",
       " 35,\n",
       " 119,\n",
       " 114,\n",
       " 111,\n",
       " 103,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 100,\n",
       " 119,\n",
       " 35,\n",
       " 115,\n",
       " 104,\n",
       " 100,\n",
       " 102,\n",
       " 104,\n",
       " 110,\n",
       " 104,\n",
       " 104,\n",
       " 115,\n",
       " 104,\n",
       " 117,\n",
       " 118,\n",
       " 35,\n",
       " 122,\n",
       " 114,\n",
       " 120,\n",
       " 111,\n",
       " 103,\n",
       " 35,\n",
       " 114,\n",
       " 113,\n",
       " 111,\n",
       " 124,\n",
       " 35,\n",
       " 101,\n",
       " 104,\n",
       " 35,\n",
       " 100,\n",
       " 101,\n",
       " 111,\n",
       " 104,\n",
       " 35,\n",
       " 119,\n",
       " 114,\n",
       " 35,\n",
       " 104,\n",
       " 121,\n",
       " 100,\n",
       " 102,\n",
       " 120,\n",
       " 100,\n",
       " 119,\n",
       " 104,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 119,\n",
       " 104,\n",
       " 100,\n",
       " 112,\n",
       " 49,\n",
       " 13,\n",
       " 13,\n",
       " 75,\n",
       " 104,\n",
       " 35,\n",
       " 118,\n",
       " 100,\n",
       " 108,\n",
       " 103,\n",
       " 35,\n",
       " 108,\n",
       " 119,\n",
       " 35,\n",
       " 122,\n",
       " 100,\n",
       " 118,\n",
       " 35,\n",
       " 100,\n",
       " 35,\n",
       " 37,\n",
       " 119,\n",
       " 114,\n",
       " 120,\n",
       " 106,\n",
       " 107,\n",
       " 35,\n",
       " 103,\n",
       " 104,\n",
       " 102,\n",
       " 108,\n",
       " 118,\n",
       " 108,\n",
       " 114,\n",
       " 113,\n",
       " 37,\n",
       " 35,\n",
       " 101,\n",
       " 120,\n",
       " 119,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 100,\n",
       " 119,\n",
       " 35,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 100,\n",
       " 102,\n",
       " 102,\n",
       " 104,\n",
       " 115,\n",
       " 119,\n",
       " 104,\n",
       " 103,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " 35,\n",
       " 88,\n",
       " 49,\n",
       " 81,\n",
       " 49,\n",
       " 35,\n",
       " 114,\n",
       " 105,\n",
       " 105,\n",
       " 104,\n",
       " 117,\n",
       " 35,\n",
       " 119,\n",
       " 114,\n",
       " 35,\n",
       " 104,\n",
       " 121,\n",
       " 100,\n",
       " 102,\n",
       " 120,\n",
       " 100,\n",
       " 119,\n",
       " 104,\n",
       " 35,\n",
       " 100,\n",
       " 105,\n",
       " 119,\n",
       " 104,\n",
       " 117,\n",
       " 35,\n",
       " 100,\n",
       " 35,\n",
       " 70,\n",
       " 100,\n",
       " 113,\n",
       " 100,\n",
       " 103,\n",
       " 108,\n",
       " 100,\n",
       " 113,\n",
       " 35,\n",
       " 112,\n",
       " 104,\n",
       " 103,\n",
       " 108,\n",
       " 102,\n",
       " 100,\n",
       " 111,\n",
       " 35,\n",
       " 119,\n",
       " 104,\n",
       " 100,\n",
       " 112,\n",
       " 47,\n",
       " 35,\n",
       " 100,\n",
       " 111,\n",
       " 118,\n",
       " 114,\n",
       " 35,\n",
       " 100,\n",
       " 119,\n",
       " 35,\n",
       " 119,\n",
       " 107,\n",
       " 104,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte5_tokenizer.encode(openwebtext[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_dst(gate_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns (merge_dst, dst_idx) the merge destination for each token in the sequence and the number of unique merge destinations.\n",
    "    For now, has a janky python for-loop implementation.\n",
    "    Input is a tensor of shape (batch_size, sequence_length) with 0 tokens are merged into the next 1 token.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = gate_samples.shape\n",
    "    merge_dst = torch.zeros_like(gate_samples, dtype=torch.long)\n",
    "            \n",
    "    # Process each batch separately\n",
    "    for b in range(batch_size):\n",
    "        dst_idx = 0\n",
    "        for i in range(seq_len):\n",
    "            merge_dst[b, i] = dst_idx\n",
    "            if gate_samples[b, i] == 1 and i < seq_len - 1:\n",
    "                # If previous position had gate=1, keep the same destination\n",
    "                dst_idx += 1\n",
    "\n",
    "    n_dst = dst_idx + 1\n",
    "    return merge_dst, n_dst\n",
    "\n",
    "class BitterLLM(nn.Module):\n",
    "    # Maps bytes to bytes, slowly merging and then unmerging tokens layer-by-layer\n",
    "    # with a context window inversely proportional to the number of tokens in the sequence.#\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_heads: int, dropout: float, downsampling_rates: List[float]):\n",
    "        \n",
    "        # Initialize a standard transformer decoder architecture.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(embedding_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.mid_layer = nn.TransformerDecoderLayer(embedding_dim, num_heads, dropout=dropout)\n",
    "\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(embedding_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # Initialize a gate for each layer.\n",
    "        layer_gate_init = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        # Copy the gate for each layer. \n",
    "        # Initializing by copying inductively biases the model to tokenize in a later layer if the gate is high but the model chose not to.\n",
    "        self.layer_gates = nn.ModuleList([\n",
    "            deepcopy(layer_gate_init) for _ in range(num_layers*2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input is a tensor of shape (batch_size, sequence_length), \n",
    "        gets transformed into a tensor of shape (batch_size, n_tokens, embedding_dim).\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        hidden_states = [x]\n",
    "        merge_destinations = []\n",
    "\n",
    "        for layer, gate in zip(self.down_layers, self.layer_gates):\n",
    "            # TODO: make the context window inversely proportional to the number of tokens in the sequence.\n",
    "            x = layer(x)\n",
    "\n",
    "            # Gate the layer.\n",
    "            gate_logits = gate(x)\n",
    "            gate_probs = F.sigmoid(gate_logits)\n",
    "\n",
    "            if self.training:\n",
    "                # Re-scale the gate probabilities to control the downsampling rate.\n",
    "                ...\n",
    "\n",
    "            gate_samples = torch.bernoulli(gate_probs)\n",
    "\n",
    "            # Merge the tokens where the gate is 1.\n",
    "            # Create a merge destination tensor\n",
    "            batch_size, seq_len, _ = x.shape\n",
    "            merge_dst, n_dst = get_merge_dst(gate_samples)\n",
    "            \n",
    "            y = torch.zeros(batch_size, n_dst, self.embedding_dim, dtype=torch.float32)\n",
    "            x = torch.scatter_reduce(y, dim=1, index=merge_dst, src=x, reduce=\"mean\", include_self=False)\n",
    "\n",
    "            hidden_states.append((x, gate_samples, gate_probs))\n",
    "\n",
    "        y = x\n",
    "        for up_layer, down_hidden_state, merge_dst in zip(self.up_layers, reversed(hidden_states), reversed(merge_destinations)):\n",
    "            pass\n",
    "\n",
    "        return self.output_layer(x), hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBitterLLM(nn.Module):\n",
    "    # A mini BitterLLM with 2 down, 4 mid, and 2 up layers. As a vibe check on the idea.\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, num_heads: int, dropout: float=0.01, downsample_rate: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, dropout=dropout, batch_first=True) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.mid_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, dropout=dropout, batch_first=True) for _ in range(4)\n",
    "        ])\n",
    "\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, dropout=dropout, batch_first=True) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        # Initialize a gate for each layer.\n",
    "        layer_gate_init = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        # Copy the gate for each layer. \n",
    "        # Initializing by copying inductively biases the model to tokenize in a later layer if the gate is high but the model chose not to.\n",
    "        self.down_layer_gate = deepcopy(layer_gate_init)\n",
    "        self.up_layer_gate = deepcopy(layer_gate_init)\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "    def apply_local_layers(self, layers, x: torch.Tensor, context_window_length) -> torch.Tensor:\n",
    "        \"\"\"Again a janky python for-loop implementation that re-constructs the causal mask for each layer.\"\"\"\n",
    "        _, seq_len, _ = x.shape\n",
    "\n",
    "        # Create causal mask for context length of 64\n",
    "        mask = torch.ones(seq_len, seq_len) * float('-inf')\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # Allow attention to self and previous window_size tokens\n",
    "            start_idx = max(0, i - context_window_length + 1)\n",
    "            mask[i, start_idx:i+1] = 0.0\n",
    "        \n",
    "        # Process through down layers with the specified context length\n",
    "        for layer in layers:\n",
    "            x = layer(x, tgt_mask=mask, tgt_is_causal=True)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        # Apply transformer layers with context length of 64\n",
    "        \n",
    "        self.apply_local_layers(self.down_layers, x, 64)\n",
    "\n",
    "        # Gate the layer.\n",
    "        gate_logits = self.down_layer_gate(x)\n",
    "        gate_probs = F.sigmoid(gate_logits)\n",
    "\n",
    "        if self.training:\n",
    "            # Re-scale the gate probabilities to control the downsampling rate.\n",
    "            true_gate_probs = gate_probs * (self.downsample_rate / gate_probs.mean())\n",
    "\n",
    "        gate_samples = torch.bernoulli(true_gate_probs)\n",
    "\n",
    "        # Merge the tokens where the gate is 1.\n",
    "        # Create a merge destination tensor\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        merge_dst, n_dst = get_merge_dst(gate_samples)\n",
    "        \n",
    "        x_downsampled = torch.zeros(batch_size, n_dst, self.embedding_dim, dtype=torch.float32)\n",
    "        x_downsampled = torch.scatter_reduce(x_downsampled, dim=1, index=merge_dst, src=x, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        y_downsampled = self.apply_local_layers(self.mid_layers, x_downsampled, 64*4)\n",
    "\n",
    "        deviation = y_downsampled - x_downsampled\n",
    "        upsampled_deviation = torch.gather(deviation, dim=1, index=merge_dst)\n",
    "\n",
    "        y = x + upsampled_deviation\n",
    "        self.apply_local_layers(self.up_layers, y, 64)\n",
    "\n",
    "        print(f\"{x.shape=} {x_downsampled.shape=} {y_downsampled.shape=} {y.shape=}\")\n",
    "\n",
    "        return self.output_layer(y)\n",
    "\n",
    "\n",
    "my_model = MiniBitterLLM(vocab_size=byte5_tokenizer.vocab_size, embedding_dim=128, num_heads=4, downsample_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10397])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = byte5_tokenizer(openwebtext[\"text\"][:5], return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "test_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(229))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.min(), test_batch.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '</s>': 1,\n",
       " '<unk>': 2,\n",
       " '\\x00': 3,\n",
       " '\\x01': 4,\n",
       " '\\x02': 5,\n",
       " '\\x03': 6,\n",
       " '\\x04': 7,\n",
       " '\\x05': 8,\n",
       " '\\x06': 9,\n",
       " '\\x07': 10,\n",
       " '\\x08': 11,\n",
       " '\\t': 12,\n",
       " '\\n': 13,\n",
       " '\\x0b': 14,\n",
       " '\\x0c': 15,\n",
       " '\\r': 16,\n",
       " '\\x0e': 17,\n",
       " '\\x0f': 18,\n",
       " '\\x10': 19,\n",
       " '\\x11': 20,\n",
       " '\\x12': 21,\n",
       " '\\x13': 22,\n",
       " '\\x14': 23,\n",
       " '\\x15': 24,\n",
       " '\\x16': 25,\n",
       " '\\x17': 26,\n",
       " '\\x18': 27,\n",
       " '\\x19': 28,\n",
       " '\\x1a': 29,\n",
       " '\\x1b': 30,\n",
       " '\\x1c': 31,\n",
       " '\\x1d': 32,\n",
       " '\\x1e': 33,\n",
       " '\\x1f': 34,\n",
       " ' ': 35,\n",
       " '!': 36,\n",
       " '\"': 37,\n",
       " '#': 38,\n",
       " '$': 39,\n",
       " '%': 40,\n",
       " '&': 41,\n",
       " \"'\": 42,\n",
       " '(': 43,\n",
       " ')': 44,\n",
       " '*': 45,\n",
       " '+': 46,\n",
       " ',': 47,\n",
       " '-': 48,\n",
       " '.': 49,\n",
       " '/': 50,\n",
       " '0': 51,\n",
       " '1': 52,\n",
       " '2': 53,\n",
       " '3': 54,\n",
       " '4': 55,\n",
       " '5': 56,\n",
       " '6': 57,\n",
       " '7': 58,\n",
       " '8': 59,\n",
       " '9': 60,\n",
       " ':': 61,\n",
       " ';': 62,\n",
       " '<': 63,\n",
       " '=': 64,\n",
       " '>': 65,\n",
       " '?': 66,\n",
       " '@': 67,\n",
       " 'A': 68,\n",
       " 'B': 69,\n",
       " 'C': 70,\n",
       " 'D': 71,\n",
       " 'E': 72,\n",
       " 'F': 73,\n",
       " 'G': 74,\n",
       " 'H': 75,\n",
       " 'I': 76,\n",
       " 'J': 77,\n",
       " 'K': 78,\n",
       " 'L': 79,\n",
       " 'M': 80,\n",
       " 'N': 81,\n",
       " 'O': 82,\n",
       " 'P': 83,\n",
       " 'Q': 84,\n",
       " 'R': 85,\n",
       " 'S': 86,\n",
       " 'T': 87,\n",
       " 'U': 88,\n",
       " 'V': 89,\n",
       " 'W': 90,\n",
       " 'X': 91,\n",
       " 'Y': 92,\n",
       " 'Z': 93,\n",
       " '[': 94,\n",
       " '\\\\': 95,\n",
       " ']': 96,\n",
       " '^': 97,\n",
       " '_': 98,\n",
       " '`': 99,\n",
       " 'a': 100,\n",
       " 'b': 101,\n",
       " 'c': 102,\n",
       " 'd': 103,\n",
       " 'e': 104,\n",
       " 'f': 105,\n",
       " 'g': 106,\n",
       " 'h': 107,\n",
       " 'i': 108,\n",
       " 'j': 109,\n",
       " 'k': 110,\n",
       " 'l': 111,\n",
       " 'm': 112,\n",
       " 'n': 113,\n",
       " 'o': 114,\n",
       " 'p': 115,\n",
       " 'q': 116,\n",
       " 'r': 117,\n",
       " 's': 118,\n",
       " 't': 119,\n",
       " 'u': 120,\n",
       " 'v': 121,\n",
       " 'w': 122,\n",
       " 'x': 123,\n",
       " 'y': 124,\n",
       " 'z': 125,\n",
       " '{': 126,\n",
       " '|': 127,\n",
       " '}': 128,\n",
       " '~': 129,\n",
       " '\\x7f': 130,\n",
       " '\\x80': 131,\n",
       " '\\x81': 132,\n",
       " '\\x82': 133,\n",
       " '\\x83': 134,\n",
       " '\\x84': 135,\n",
       " '\\x85': 136,\n",
       " '\\x86': 137,\n",
       " '\\x87': 138,\n",
       " '\\x88': 139,\n",
       " '\\x89': 140,\n",
       " '\\x8a': 141,\n",
       " '\\x8b': 142,\n",
       " '\\x8c': 143,\n",
       " '\\x8d': 144,\n",
       " '\\x8e': 145,\n",
       " '\\x8f': 146,\n",
       " '\\x90': 147,\n",
       " '\\x91': 148,\n",
       " '\\x92': 149,\n",
       " '\\x93': 150,\n",
       " '\\x94': 151,\n",
       " '\\x95': 152,\n",
       " '\\x96': 153,\n",
       " '\\x97': 154,\n",
       " '\\x98': 155,\n",
       " '\\x99': 156,\n",
       " '\\x9a': 157,\n",
       " '\\x9b': 158,\n",
       " '\\x9c': 159,\n",
       " '\\x9d': 160,\n",
       " '\\x9e': 161,\n",
       " '\\x9f': 162,\n",
       " '\\xa0': 163,\n",
       " '¡': 164,\n",
       " '¢': 165,\n",
       " '£': 166,\n",
       " '¤': 167,\n",
       " '¥': 168,\n",
       " '¦': 169,\n",
       " '§': 170,\n",
       " '¨': 171,\n",
       " '©': 172,\n",
       " 'ª': 173,\n",
       " '«': 174,\n",
       " '¬': 175,\n",
       " '\\xad': 176,\n",
       " '®': 177,\n",
       " '¯': 178,\n",
       " '°': 179,\n",
       " '±': 180,\n",
       " '²': 181,\n",
       " '³': 182,\n",
       " '´': 183,\n",
       " 'µ': 184,\n",
       " '¶': 185,\n",
       " '·': 186,\n",
       " '¸': 187,\n",
       " '¹': 188,\n",
       " 'º': 189,\n",
       " '»': 190,\n",
       " '¼': 191,\n",
       " '½': 192,\n",
       " '¾': 193,\n",
       " '¿': 194,\n",
       " 'À': 195,\n",
       " 'Á': 196,\n",
       " 'Â': 197,\n",
       " 'Ã': 198,\n",
       " 'Ä': 199,\n",
       " 'Å': 200,\n",
       " 'Æ': 201,\n",
       " 'Ç': 202,\n",
       " 'È': 203,\n",
       " 'É': 204,\n",
       " 'Ê': 205,\n",
       " 'Ë': 206,\n",
       " 'Ì': 207,\n",
       " 'Í': 208,\n",
       " 'Î': 209,\n",
       " 'Ï': 210,\n",
       " 'Ð': 211,\n",
       " 'Ñ': 212,\n",
       " 'Ò': 213,\n",
       " 'Ó': 214,\n",
       " 'Ô': 215,\n",
       " 'Õ': 216,\n",
       " 'Ö': 217,\n",
       " '×': 218,\n",
       " 'Ø': 219,\n",
       " 'Ù': 220,\n",
       " 'Ú': 221,\n",
       " 'Û': 222,\n",
       " 'Ü': 223,\n",
       " 'Ý': 224,\n",
       " 'Þ': 225,\n",
       " 'ß': 226,\n",
       " 'à': 227,\n",
       " 'á': 228,\n",
       " 'â': 229,\n",
       " 'ã': 230,\n",
       " 'ä': 231,\n",
       " 'å': 232,\n",
       " 'æ': 233,\n",
       " 'ç': 234,\n",
       " 'è': 235,\n",
       " 'é': 236,\n",
       " 'ê': 237,\n",
       " 'ë': 238,\n",
       " 'ì': 239,\n",
       " 'í': 240,\n",
       " 'î': 241,\n",
       " 'ï': 242,\n",
       " 'ð': 243,\n",
       " 'ñ': 244,\n",
       " 'ò': 245,\n",
       " 'ó': 246,\n",
       " 'ô': 247,\n",
       " 'õ': 248,\n",
       " 'ö': 249,\n",
       " '÷': 250,\n",
       " 'ø': 251,\n",
       " 'ù': 252,\n",
       " 'ú': 253,\n",
       " 'û': 254,\n",
       " 'ü': 255,\n",
       " 'ý': 256,\n",
       " 'þ': 257,\n",
       " 'ÿ': 258,\n",
       " '<extra_id_0>': 259,\n",
       " '<extra_id_1>': 260,\n",
       " '<extra_id_2>': 261,\n",
       " '<extra_id_3>': 262,\n",
       " '<extra_id_4>': 263,\n",
       " '<extra_id_5>': 264,\n",
       " '<extra_id_6>': 265,\n",
       " '<extra_id_7>': 266,\n",
       " '<extra_id_8>': 267,\n",
       " '<extra_id_9>': 268,\n",
       " '<extra_id_10>': 269,\n",
       " '<extra_id_11>': 270,\n",
       " '<extra_id_12>': 271,\n",
       " '<extra_id_13>': 272,\n",
       " '<extra_id_14>': 273,\n",
       " '<extra_id_15>': 274,\n",
       " '<extra_id_16>': 275,\n",
       " '<extra_id_17>': 276,\n",
       " '<extra_id_18>': 277,\n",
       " '<extra_id_19>': 278,\n",
       " '<extra_id_20>': 279,\n",
       " '<extra_id_21>': 280,\n",
       " '<extra_id_22>': 281,\n",
       " '<extra_id_23>': 282,\n",
       " '<extra_id_24>': 283,\n",
       " '<extra_id_25>': 284,\n",
       " '<extra_id_26>': 285,\n",
       " '<extra_id_27>': 286,\n",
       " '<extra_id_28>': 287,\n",
       " '<extra_id_29>': 288,\n",
       " '<extra_id_30>': 289,\n",
       " '<extra_id_31>': 290,\n",
       " '<extra_id_32>': 291,\n",
       " '<extra_id_33>': 292,\n",
       " '<extra_id_34>': 293,\n",
       " '<extra_id_35>': 294,\n",
       " '<extra_id_36>': 295,\n",
       " '<extra_id_37>': 296,\n",
       " '<extra_id_38>': 297,\n",
       " '<extra_id_39>': 298,\n",
       " '<extra_id_40>': 299,\n",
       " '<extra_id_41>': 300,\n",
       " '<extra_id_42>': 301,\n",
       " '<extra_id_43>': 302,\n",
       " '<extra_id_44>': 303,\n",
       " '<extra_id_45>': 304,\n",
       " '<extra_id_46>': 305,\n",
       " '<extra_id_47>': 306,\n",
       " '<extra_id_48>': 307,\n",
       " '<extra_id_49>': 308,\n",
       " '<extra_id_50>': 309,\n",
       " '<extra_id_51>': 310,\n",
       " '<extra_id_52>': 311,\n",
       " '<extra_id_53>': 312,\n",
       " '<extra_id_54>': 313,\n",
       " '<extra_id_55>': 314,\n",
       " '<extra_id_56>': 315,\n",
       " '<extra_id_57>': 316,\n",
       " '<extra_id_58>': 317,\n",
       " '<extra_id_59>': 318,\n",
       " '<extra_id_60>': 319,\n",
       " '<extra_id_61>': 320,\n",
       " '<extra_id_62>': 321,\n",
       " '<extra_id_63>': 322,\n",
       " '<extra_id_64>': 323,\n",
       " '<extra_id_65>': 324,\n",
       " '<extra_id_66>': 325,\n",
       " '<extra_id_67>': 326,\n",
       " '<extra_id_68>': 327,\n",
       " '<extra_id_69>': 328,\n",
       " '<extra_id_70>': 329,\n",
       " '<extra_id_71>': 330,\n",
       " '<extra_id_72>': 331,\n",
       " '<extra_id_73>': 332,\n",
       " '<extra_id_74>': 333,\n",
       " '<extra_id_75>': 334,\n",
       " '<extra_id_76>': 335,\n",
       " '<extra_id_77>': 336,\n",
       " '<extra_id_78>': 337,\n",
       " '<extra_id_79>': 338,\n",
       " '<extra_id_80>': 339,\n",
       " '<extra_id_81>': 340,\n",
       " '<extra_id_82>': 341,\n",
       " '<extra_id_83>': 342,\n",
       " '<extra_id_84>': 343,\n",
       " '<extra_id_85>': 344,\n",
       " '<extra_id_86>': 345,\n",
       " '<extra_id_87>': 346,\n",
       " '<extra_id_88>': 347,\n",
       " '<extra_id_89>': 348,\n",
       " '<extra_id_90>': 349,\n",
       " '<extra_id_91>': 350,\n",
       " '<extra_id_92>': 351,\n",
       " '<extra_id_93>': 352,\n",
       " '<extra_id_94>': 353,\n",
       " '<extra_id_95>': 354,\n",
       " '<extra_id_96>': 355,\n",
       " '<extra_id_97>': 356,\n",
       " '<extra_id_98>': 357,\n",
       " '<extra_id_99>': 358,\n",
       " '<extra_id_100>': 359,\n",
       " '<extra_id_101>': 360,\n",
       " '<extra_id_102>': 361,\n",
       " '<extra_id_103>': 362,\n",
       " '<extra_id_104>': 363,\n",
       " '<extra_id_105>': 364,\n",
       " '<extra_id_106>': 365,\n",
       " '<extra_id_107>': 366,\n",
       " '<extra_id_108>': 367,\n",
       " '<extra_id_109>': 368,\n",
       " '<extra_id_110>': 369,\n",
       " '<extra_id_111>': 370,\n",
       " '<extra_id_112>': 371,\n",
       " '<extra_id_113>': 372,\n",
       " '<extra_id_114>': 373,\n",
       " '<extra_id_115>': 374,\n",
       " '<extra_id_116>': 375,\n",
       " '<extra_id_117>': 376,\n",
       " '<extra_id_118>': 377,\n",
       " '<extra_id_119>': 378,\n",
       " '<extra_id_120>': 379,\n",
       " '<extra_id_121>': 380,\n",
       " '<extra_id_122>': 381,\n",
       " '<extra_id_123>': 382,\n",
       " '<extra_id_124>': 383}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte5_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerEncoderLayer.forward() got an unexpected keyword argument 'tgt_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_model(test_batch)\n",
      "File \u001b[0;32m/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[70], line 51\u001b[0m, in \u001b[0;36mMiniBitterLLM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Apply transformer layers with context length of 64\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_local_layers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_layers, x, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Gate the layer.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m gate_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_layer_gate(x)\n",
      "Cell \u001b[0;32mIn[70], line 43\u001b[0m, in \u001b[0;36mMiniBitterLLM.apply_local_layers\u001b[0;34m(self, layers, x, context_window_length)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Process through down layers with the specified context length\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, tgt_mask\u001b[38;5;241m=\u001b[39mmask, tgt_is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerEncoderLayer.forward() got an unexpected keyword argument 'tgt_mask'"
     ]
    }
   ],
   "source": [
    "my_model(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 1, 2, 2, 2, 3, 3, 3, 3]]), 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = torch.tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "merge_dst, n_dst = get_merge_dst(test_samples)\n",
    "merge_dst, n_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [-inf, 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf, 0., 0., 0., 0., -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create causal mask for context length of 64\n",
    "# Create a sliding window attention mask with window size 2\n",
    "window_size = 4\n",
    "seq_len = 10\n",
    "mask = torch.ones(seq_len, seq_len) * float('-inf')\n",
    "\n",
    "for i in range(seq_len):\n",
    "    # Allow attention to self and previous window_size tokens\n",
    "    start_idx = max(0, i - window_size + 1)\n",
    "    mask[i, start_idx:i+1] = 0.0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000, 0.3333, 0.2500]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(1, n_dst, dtype=torch.float32)\n",
    "y = y.scatter_reduce(dim=1, index=merge_dst, src=test_samples, reduce=\"mean\", include_self=False)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 1.0000, 0.3333, 0.3333, 0.3333, 0.2500, 0.2500, 0.2500,\n",
       "         0.2500]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_upsampled = torch.gather(y, dim=1, index=merge_dst)\n",
    "y_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
