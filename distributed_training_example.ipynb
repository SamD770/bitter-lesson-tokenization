{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch import nn\n",
    "from typing import List\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "from transformers.models.gemma2.modeling_gemma2 import Gemma2DecoderLayer, Gemma2Config, Gemma2Attention, Gemma2Model\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Helper functions to count the number of parameters in a torch.nn.Module\n",
    "def count_parameters(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "def display_gpu_memory():\n",
    "    # torch can give a more accurate memory usage than nvidia-smi\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total_memory_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        allocated_memory_gb = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "        free_memory_gb = torch.cuda.mem_get_info(i)[0] / (1024**3)\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"  Total GPU memory: {total_memory_gb:.1f} GB\")\n",
    "        print(f\"  Free GPU memory: {free_memory_gb:.1f} GB\")\n",
    "        print(f\"  Allocated GPU memory: {allocated_memory_gb:.1f} GB\")\n",
    "\n",
    "\n",
    "def parameter_count_string(module):\n",
    "    n_params = count_parameters(module)\n",
    "    if n_params > 10**6:\n",
    "        return f\"{n_params/10**6:.1f}M\"\n",
    "    elif n_params > 10**3:\n",
    "        return f\"{n_params/10**3:.1f}k\"\n",
    "    else:\n",
    "        return f\"{n_params}\" \n",
    "    \n",
    "\n",
    "\n",
    "class TokenDownsampler(nn.Module):\n",
    "    def __init__(self, downsample_rate: float):\n",
    "        super().__init__()\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "\n",
    "class TokenUpsampler(nn.Module):\n",
    "    def __init__(self, upsample_rate: float):\n",
    "        super().__init__()\n",
    "        self.upsample_rate = upsample_rate\n",
    "\n",
    "\n",
    "def get_merge_dst(gate_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns (merge_dst, dst_idx) the merge destination for each token in the sequence and the number of unique merge destinations.\n",
    "    For now, has a janky python for-loop implementation.\n",
    "    Input is a tensor of shape (batch_size, sequence_length) with 0 tokens are merged into the next 1 token.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = gate_samples.shape\n",
    "    merge_dst = torch.zeros_like(gate_samples, dtype=torch.long)\n",
    "    n_dst = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    # Process each batch separately\n",
    "    for b in range(batch_size):\n",
    "        dst_idx = 0\n",
    "        for i in range(seq_len):\n",
    "            merge_dst[b, i] = dst_idx\n",
    "            if gate_samples[b, i] == 1 and i < seq_len - 1:\n",
    "                # If previous position had gate=1, keep the same destination\n",
    "                dst_idx += 1\n",
    "\n",
    "        n_dst[b] = dst_idx + 1\n",
    "\n",
    "    return merge_dst, n_dst\n",
    "\n",
    "\n",
    "class AverageTokenDownsampler():\n",
    "    def forward(self, x: torch.Tensor, gate_samples: torch.Tensor, position_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        1 2 3 4 5\n",
    "        1 0 0 1 1\n",
    "        ->\n",
    "        1 3 5\n",
    "        inputs:\n",
    "        x.shape = (batch_size, seq_len, embedding_dim)\n",
    "        gate_samples.shape = (batch_size, seq_len)\n",
    "        position_ids.shape = (batch_size, seq_len)\n",
    "        returns:\n",
    "        x_downsampled.shape = (batch_size, n_dst, embedding_dim)\n",
    "        position_ids_downsampled.shape = (batch_size, n_dst)\n",
    "        \"\"\"\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        # Merge the tokens into the next token where the gate is 1.\n",
    "        gate_samples = gate_samples.squeeze(-1)\n",
    "        down_merge_dst, n_dst = get_merge_dst(gate_samples)\n",
    "\n",
    "        # Also merge the position ids.\n",
    "        position_ids_downsampled = torch.zeros(batch_size, n_dst.max(), dtype=x.dtype).to(x.device)\n",
    "        position_ids_downsampled = torch.scatter_reduce(position_ids_downsampled, dim=1, index=down_merge_dst, src=position_ids, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        # Merge the downsampled tokens.\n",
    "        down_merge_dst = down_merge_dst.unsqueeze(-1).expand(-1, -1, self.embedding_dim)\n",
    "\n",
    "        x_downsampled = torch.zeros(batch_size, n_dst.max(), self.embedding_dim, dtype=x.dtype).to(x.device)\n",
    "        x_downsampled = torch.scatter_reduce(x_downsampled, dim=1, index=down_merge_dst, src=x, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        return x_downsampled, position_ids_downsampled\n",
    "\n",
    "\n",
    "class DistributeTokenUpsampler():\n",
    "    def forward(self, x: torch.Tensor, gate_samples: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        1 2 3\n",
    "        1 0 0 1 1\n",
    "        ->\n",
    "        1 1 1 2 3\n",
    "        inputs:\n",
    "        x.shape = (batch_size, shortened_seq_len, embedding_dim)\n",
    "        gate_samples.shape = (batch_size, seq_len)\n",
    "        returns:\n",
    "        x_upsampled.shape = (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        _, _, embedding_dim = x.shape\n",
    "\n",
    "        # Get the merge destination for each token\n",
    "        up_merge_dst, _ = get_merge_dst(gate_samples)\n",
    "        up_merge_dst = up_merge_dst.unsqueeze(-1).expand(-1, -1, embedding_dim)\n",
    "\n",
    "        # Add the upsampled deviation to the input to the middle layers\n",
    "        x_upsampled = torch.gather(x, dim=1, index=up_merge_dst)\n",
    "\n",
    "        return x_upsampled\n",
    "\n",
    "\n",
    "def get_gate_indices(gate_samples: torch.Tensor, n_dst_max) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns the indices of the tokens that are gated merged. For now, has a janky python for-loop implementation.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = gate_samples.shape\n",
    "    gate_indices = torch.zeros(batch_size, n_dst_max, dtype=torch.long)\n",
    "\n",
    "    # Process each batch separately\n",
    "    for b in range(batch_size):\n",
    "        dst_idx = 0\n",
    "        for i, _ in enumerate(gate_samples[b]):\n",
    "            if gate_samples[b, i] == 1:\n",
    "                gate_indices[b, dst_idx] = i\n",
    "                dst_idx += 1\n",
    "\n",
    "    return gate_indices\n",
    "\n",
    "\n",
    "class SelectTokenDownsampler():\n",
    "    def forward(self, x: torch.Tensor, gate_samples: torch.Tensor, position_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        1 2 3 4 5\n",
    "        1 0 0 1 1\n",
    "        ->\n",
    "        1 4 5\n",
    "        inputs:\n",
    "        x.shape = (batch_size, seq_len, embedding_dim)\n",
    "        gate_samples.shape = (batch_size, seq_len)\n",
    "        position_ids.shape = (batch_size, seq_len)\n",
    "        returns:\n",
    "        x_downsampled.shape = (batch_size, n_dst, embedding_dim)\n",
    "        position_ids_downsampled.shape = (batch_size, n_dst)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Merge the tokens into the next token where the gate is 1.\n",
    "        gate_samples = gate_samples.squeeze(-1)\n",
    "        n_dst = gate_samples.sum(dim=1)\n",
    "\n",
    "        selected_indices = get_gate_indices(gate_samples, n_dst.max())\n",
    "        position_ids_downsampled = position_ids.gather(dim=1, index=selected_indices)\n",
    "\n",
    "        selected_indices = selected_indices.unsqueeze(-1).expand(-1, -1, x.shape[-1])\n",
    "        x_downsampled = x.gather(dim=1, index=selected_indices)\n",
    "\n",
    "        return x_downsampled, position_ids_downsampled\n",
    "\n",
    "\n",
    "def compute_discounted_rewards(rewards, discount):\n",
    "    \"\"\"\n",
    "    Assumes that rewards is a numpy array of shape (n_episodes, n_timesteps). Returns tensor of same shape.\n",
    "    credit to: https://stackoverflow.com/questions/47970683/vectorize-a-numpy-discount-calculation/47971187#47971187,\n",
    "    minor modifications made to vectorise computation.\n",
    "    C[i] = R[i] + discount * C[i+1]\n",
    "    signal.lfilter(b, a, x, axis=-1, zi=None)\n",
    "    a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
    "                          - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
    "    \"\"\"\n",
    "    r = rewards[:, ::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = lfilter(b, a, x=r)\n",
    "    return y[:, ::-1]\n",
    "\n",
    "\n",
    "def discounted_rewards_torch(rewards, discount):\n",
    "    \"\"\"torch wrapper for compute_discounted_rewards. Warning: does _not_ allow for backprop through the rewards, which is fine for policy gradients.\"\"\"\n",
    "    rewards_device = rewards.device\n",
    "    rewards = rewards.detach().cpu().numpy()\n",
    "    discounted_rewards = compute_discounted_rewards(rewards, discount)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards.copy(), device=rewards_device) # Copy as torch doesn't like converting negatively strided arrays\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "class DownGater(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, downsample_rate: float):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "    def gate_samples(self, down_gate_probs: torch.Tensor) -> torch.Tensor:\n",
    "        gate_samples = torch.bernoulli(down_gate_probs)\n",
    "        return gate_samples\n",
    "\n",
    "\n",
    "class LinearGater(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, downsample_rate: float):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.linear = nn.Linear(embedding_dim, 1)\n",
    "        self.downsample_rate = downsample_rate\n",
    "        self.downsample_rate_scale = 5.\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        down_gate_logits = self.linear(x)\n",
    "        down_gate_probs = F.sigmoid(down_gate_logits)\n",
    "        return down_gate_logits, down_gate_probs # We need to return the logits for stable backprop\n",
    "    \n",
    "\n",
    "class RandomGater(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, downsample_rate: float):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        gate_probs = torch.ones(batch_size, seq_len, 1, dtype=x.dtype, device=x.device) * self.downsample_rate\n",
    "        gate_logits = torch.log(gate_probs / (1 - gate_probs))\n",
    "        return gate_logits, gate_probs\n",
    "\n",
    "\n",
    "class EquidistantGater(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, downsample_rate: float):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.downsample_rate = downsample_rate\n",
    "        self.gate_every = round(1 / downsample_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        gate_probs = torch.zeros(batch_size, seq_len, 1, dtype=x.dtype, device=x.device) \n",
    "        gate_probs[:, ::self.gate_every] = 1.\n",
    "        gate_logits = gate_probs * 40. - 20. # Avoid Nans\n",
    "        return gate_logits, gate_probs\n",
    "\n",
    "\n",
    "def create_gemma2DecoderLayer(config: Gemma2Config, layer_idx: int):\n",
    "    # Gemma2Attention.__init__ overrides config.sliding_window with None if layer_idx % 2 == 0.\n",
    "    # This is a hack to get the sliding window for even layers indices.\n",
    "    layer = Gemma2DecoderLayer(config, layer_idx)\n",
    "    layer.self_attn.sliding_window = config.sliding_window\n",
    "    layer.is_sliding = config.sliding_window is not None\n",
    "    return layer\n",
    "\n",
    "\n",
    "def get_gemma2_attention_mask(batch_size, seq_len, device, dtype):\n",
    "    \n",
    "\n",
    "    cache_position = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "\n",
    "    my_attention_mask = Gemma2Model._prepare_4d_causal_attention_mask_with_cache_position(\n",
    "        None,\n",
    "        seq_len,\n",
    "        seq_len,\n",
    "        dtype,\n",
    "        device,\n",
    "        cache_position,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    return cache_position, my_attention_mask\n",
    "\n",
    "\n",
    "class CausalGemmaMiniBitterLLM(nn.Module):\n",
    "    # A mini BitterLLM with 2 down, 4 mid, and 2 up layers. As a vibe check on the idea.\n",
    "    # Use Gemma2DecoderLayer as a drop in replacement for the TransformerEncoderLayer, with RoPE and sliding window pre-implemented.\n",
    "    # Also uses a causal mask.\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, num_heads: int, downsample_rate: float = 0.25, sliding_window = 64, GaterClass=LinearGater):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.byte_layer_config = Gemma2Config(\n",
    "            head_dim=head_dim,\n",
    "            query_pre_attn_scalar=head_dim, \n",
    "            sliding_window=sliding_window,\n",
    "            intermediate_size=embedding_dim,\n",
    "            hidden_size=embedding_dim,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_key_value_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        self.deep_layer_config = Gemma2Config(\n",
    "            head_dim=head_dim,\n",
    "            query_pre_attn_scalar=head_dim, \n",
    "            sliding_window=None,\n",
    "            intermediate_size=embedding_dim * 4, # dim_feedforward should scale inversely with the number of tokens in the sequence.\n",
    "            hidden_size=embedding_dim,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_key_value_heads=num_heads\n",
    "        )\n",
    "\n",
    "        n_down_layers = 2\n",
    "        n_mid_layers = 2\n",
    "        n_up_layers = 2\n",
    "\n",
    "        # Layer idx=0 is necessary for the sliding window to be applied.\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            create_gemma2DecoderLayer(self.byte_layer_config, layer_idx=i) for i in range(n_down_layers)\n",
    "        ])\n",
    "\n",
    "        self.mid_layers = nn.ModuleList([\n",
    "            create_gemma2DecoderLayer(self.deep_layer_config, layer_idx=i+n_down_layers) for i in range(n_mid_layers) \n",
    "        ])\n",
    "\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            create_gemma2DecoderLayer(self.byte_layer_config, layer_idx=i+n_down_layers+n_mid_layers) for i in range(n_up_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        self.down_layer_gate = GaterClass(embedding_dim, downsample_rate)\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids: torch.Tensor, \n",
    "            position_ids: torch.Tensor=None        \n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        batch_size, max_seq_len = input_ids.shape\n",
    "\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(max_seq_len, dtype=x.dtype).unsqueeze(0).expand(batch_size, -1).to(x.device)      \n",
    "        \n",
    "        # Position_ids are used for RoPE\n",
    "        # cache_position is used for the cache.update() function which retrieves relevant kvs\n",
    "        byte_cache_position, byte_attention_mask = get_gemma2_attention_mask(batch_size, max_seq_len, x.device, x.dtype)\n",
    "\n",
    "        # Apply down layers to byte tokens        \n",
    "        for layer in self.down_layers:\n",
    "            x = layer(x, \n",
    "                attention_mask=byte_attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                cache_position=byte_cache_position,\n",
    "            )[0]\n",
    "\n",
    "        # Sample gating binary variables for each token.\n",
    "        down_gate_logits, down_gate_probs = self.down_layer_gate(x)\n",
    "        down_gate_samples = torch.bernoulli(down_gate_probs)\n",
    "\n",
    "        # Hack: ensure for now that we always gate on the first token:\n",
    "        down_gate_samples[:, 0] = 1.\n",
    "\n",
    "        # Merge the tokens into the next token where the gate is 1.\n",
    "        down_gate_samples = down_gate_samples.squeeze(-1)\n",
    "        down_merge_dst, n_dst = get_merge_dst(down_gate_samples)\n",
    "        max_n_dst = n_dst.max()\n",
    "\n",
    "        # Also merge the position ids.\n",
    "        position_ids_downsampled = torch.zeros(batch_size, max_n_dst, dtype=x.dtype).to(x.device)\n",
    "        position_ids_downsampled = torch.scatter_reduce(position_ids_downsampled, dim=1, index=down_merge_dst, src=position_ids, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        # Merge the downsampled tokens.\n",
    "        down_merge_dst = down_merge_dst.unsqueeze(-1).expand(-1, -1, self.embedding_dim)\n",
    "\n",
    "        x_downsampled = torch.zeros(batch_size, max_n_dst, self.embedding_dim, dtype=x.dtype).to(x.device)\n",
    "        x_downsampled = torch.scatter_reduce(x_downsampled, dim=1, index=down_merge_dst, src=x, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        # Apply mid layers to merged tokens and compute the deviation\n",
    "        downsampled_cache_position, downsampled_attention_mask = get_gemma2_attention_mask(batch_size, max_n_dst, x.device, x.dtype)\n",
    "\n",
    "        y_downsampled = x_downsampled\n",
    "\n",
    "        for layer in self.mid_layers:\n",
    "            y_downsampled = layer(\n",
    "                y_downsampled, \n",
    "                attention_mask=downsampled_attention_mask,\n",
    "                position_ids=position_ids_downsampled,\n",
    "                cache_position=downsampled_cache_position,\n",
    "            )[0]\n",
    "        \n",
    "        deviation = y_downsampled - x_downsampled        \n",
    "\n",
    "        # Upsample by removing the first token merge group, shifting all token groups down and adding another one token group at the end.\n",
    "        up_gate_samples = down_gate_samples[:, 1:]\n",
    "        up_gate_samples = torch.cat([up_gate_samples, torch.ones(batch_size, 1, dtype=up_gate_samples.dtype).to(up_gate_samples.device)], dim=1)\n",
    "        up_merge_dst, _ = get_merge_dst(up_gate_samples)\n",
    "        up_merge_dst = up_merge_dst.unsqueeze(-1).expand(-1, -1, self.embedding_dim)\n",
    "\n",
    "        # Add the upsampled deviation to the input to the middle layers\n",
    "        upsampled_deviation = torch.gather(deviation, dim=1, index=up_merge_dst)\n",
    "        y = x + upsampled_deviation\n",
    "\n",
    "        # Apply up layers to byte tokens\n",
    "        for layer in self.up_layers:\n",
    "            y = layer(\n",
    "                y, \n",
    "                attention_mask=byte_attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                cache_position=byte_cache_position,\n",
    "            )[0]\n",
    "\n",
    "        # Map residual stream to logits\n",
    "        logits = self.output_layer(y)\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        out = {\n",
    "            \"logits\": logits,\n",
    "            \"down_gate_probs\": down_gate_probs.squeeze(-1),\n",
    "            \"down_gate_logits\": down_gate_logits.squeeze(-1),\n",
    "            \"down_gate_samples\": down_gate_samples.to(dtype=torch.long),\n",
    "            \"down_merge_dst\": down_merge_dst[:, :, 0], # This dimension is repeated.\n",
    "            \"up_merge_dst\": up_merge_dst[:, :, 0],\n",
    "            \"n_dst\": n_dst,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"key_values\": None\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def bitter_tokenizer_training_step(model, batch, optimizer, learn_gating=True):\n",
    "    \"\"\"\n",
    "    Assume that batch is torch.tensor of token ids of shape (batch, sequence_length). returns a dict of floats of the training losses for the batch.\n",
    "    \"\"\"\n",
    "    batch_size, _ = batch.shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(batch)\n",
    "    logits = out[\"logits\"]\n",
    "    down_gate_samples = out[\"down_gate_samples\"]\n",
    "    down_gate_probs = out[\"down_gate_probs\"]\n",
    "    down_gate_logits = out[\"down_gate_logits\"]\n",
    "    \n",
    "    # Compute autoregressive loss: log probability of next token.\n",
    "    next_token_ids = batch[:, 1:]\n",
    "    current_token_logits = logits[:, :-1]\n",
    "    next_token_logits = F.cross_entropy(current_token_logits.transpose(1, 2), next_token_ids, reduction=\"none\") # Transpose as F.cross_entropy wants shape [batch, classes, ...]\n",
    "    ar_loss = next_token_logits.mean()\n",
    "    true_downsample_rate = down_gate_probs.mean()\n",
    "\n",
    "    if learn_gating:\n",
    "        # Compute gating loss: discounted log probabilities of following token(s).\n",
    "        discount_rate = 0.99\n",
    "        next_token_logits_padded = torch.cat([next_token_logits, torch.zeros(batch_size, 1, device=next_token_logits.device)], dim=-1) # Pad the last reward as zero\n",
    "        discounted_rewards = discounted_rewards_torch(next_token_logits_padded, discount_rate)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean(dim=0)) # Simple estimate of the advantage\n",
    "\n",
    "        # action 0 = continue, action 1 = gate\n",
    "        action_log_probs = torch.stack([torch.zeros_like(down_gate_logits), down_gate_logits], dim=1) # As a sigmoid is equivalent to having one logit as 0.\n",
    "        selected_action_log_probs = F.cross_entropy(action_log_probs, down_gate_samples, reduction=\"none\")\n",
    "        gating_loss = - (discounted_rewards * selected_action_log_probs).mean() # Negative as we want to maximise the reward.\n",
    "\n",
    "        # Hacky additional consistency loss: make the downsampling rate match the training gating.\n",
    "        down_gate_rate_loss =  2.*(model.downsample_rate - true_downsample_rate) **2\n",
    "\n",
    "        total_loss = ar_loss + gating_loss + down_gate_rate_loss\n",
    "    else:\n",
    "        selected_action_log_probs = torch.tensor(0.0)\n",
    "        gating_loss = torch.tensor(0.0)\n",
    "        down_gate_rate_loss = torch.tensor(0.0) # For logging purposes.\n",
    "        total_loss = ar_loss\n",
    "\n",
    "    # Optimizer step\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    out = {\n",
    "        \"ar_loss\": ar_loss.item(),\n",
    "        \"gating_loss\": gating_loss.item(),\n",
    "        \"true_downsample_rate\": true_downsample_rate.item(),\n",
    "        \"rate_consistency_loss\": down_gate_rate_loss.item(),\n",
    "        \"total_loss\": total_loss.item(),\n",
    "        \"selected_action_ce\": selected_action_log_probs.mean().item()\n",
    "    }\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def bitter_tokenizer_training_step_profile(model, batch, optimizer, learn_gating=True):\n",
    "    \"\"\"\n",
    "    Assume that batch is torch.tensor of token ids of shape (batch, sequence_length). returns a dict of floats of the training losses for the batch.\n",
    "    \"\"\"\n",
    "    batch_size, _ = batch.shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with record_function(\"model_forward\"):\n",
    "        out = model(batch)\n",
    "\n",
    "    logits = out[\"logits\"]\n",
    "    down_gate_samples = out[\"down_gate_samples\"]\n",
    "    down_gate_probs = out[\"down_gate_probs\"]\n",
    "    down_gate_logits = out[\"down_gate_logits\"]\n",
    "    \n",
    "    # Compute autoregressive loss: log probability of next token.\n",
    "    next_token_ids = batch[:, 1:]\n",
    "    current_token_logits = logits[:, :-1]\n",
    "    next_token_logits = F.cross_entropy(current_token_logits.transpose(1, 2), next_token_ids, reduction=\"none\") # Transpose as F.cross_entropy wants shape [batch, classes, ...]\n",
    "    ar_loss = next_token_logits.mean()\n",
    "    true_downsample_rate = down_gate_probs.mean()\n",
    "\n",
    "    if learn_gating:\n",
    "        # Compute gating loss: discounted log probabilities of following token(s).\n",
    "        discount_rate = 0.99\n",
    "        next_token_logits_padded = torch.cat([next_token_logits, torch.zeros(batch_size, 1, device=next_token_logits.device)], dim=-1) # Pad the last reward as zero\n",
    "        discounted_rewards = discounted_rewards_torch(next_token_logits_padded, discount_rate)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean(dim=0)) # Simple estimate of the advantage\n",
    "\n",
    "        # action 0 = continue, action 1 = gate\n",
    "        action_log_probs = torch.stack([torch.zeros_like(down_gate_logits), down_gate_logits], dim=1) # As a sigmoid is equivalent to having one logit as 0.\n",
    "        selected_action_log_probs = F.cross_entropy(action_log_probs, down_gate_samples, reduction=\"none\")\n",
    "        gating_loss = - (discounted_rewards * selected_action_log_probs).mean() # Negative as we want to maximise the reward.\n",
    "\n",
    "        # Hacky additional consistency loss: make the downsampling rate match the training gating.\n",
    "        down_gate_rate_loss =  2.*(model.downsample_rate - true_downsample_rate) **2\n",
    "\n",
    "        total_loss = ar_loss + gating_loss + down_gate_rate_loss\n",
    "    else:\n",
    "        selected_action_log_probs = torch.tensor(0.0)\n",
    "        gating_loss = torch.tensor(0.0)\n",
    "        down_gate_rate_loss = torch.tensor(0.0) # For logging purposes.\n",
    "        total_loss = ar_loss\n",
    "\n",
    "    with record_function(\"backward\"):\n",
    "        # Optimizer step\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    out = {\n",
    "        \"ar_loss\": ar_loss.item(),\n",
    "        \"gating_loss\": gating_loss.item(),\n",
    "        \"true_downsample_rate\": true_downsample_rate.item(),\n",
    "        \"rate_consistency_loss\": down_gate_rate_loss.item(),\n",
    "        \"total_loss\": total_loss.item(),\n",
    "        \"selected_action_ce\": selected_action_log_probs.mean().item()\n",
    "    }\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "byte5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")\n",
    "\n",
    "\n",
    "def display_gating(tokens_ids, merge_dst):\n",
    "    \"\"\"Display how a SmallBitterLLM merges a sequence. token_ids and merge_dst are tensors of shape (sequence_length,).\"\"\"\n",
    "    previous_merge_dst = 0\n",
    "    for t_id, merge_destinantion in zip(tokens_ids, merge_dst):\n",
    "        merge_destinantion = merge_destinantion.item()\n",
    "        \n",
    "        if merge_destinantion != previous_merge_dst:\n",
    "            print(f\"|\", end=\"\")\n",
    "            previous_merge_dst = merge_destinantion\n",
    "        \n",
    "        t_txt = byte5_tokenizer.decode(t_id)\n",
    "        print(f\"{t_txt.replace('\\n', '\\\\n')}\", end=\"\")\n",
    "\n",
    "    print()\n",
    "        \n",
    "\n",
    "\n",
    "def bitter_tokenizer_training_loop(model, train_dataset, batch_print_every=10, num_epochs=1, batch_size=128, batch_limit=None, learn_gating=True):\n",
    "    # Initialize distributed training\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(backend='nccl')\n",
    "    \n",
    "    local_rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "    # Wrap model in DDP\n",
    "    model = model.to(device)\n",
    "    model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "    \n",
    "    # Create distributed sampler and data loader\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=local_rank,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # See how the model merges a sequence (only on rank 0)\n",
    "    if local_rank == 0:\n",
    "        test_string = train_dataset[-1][\"text\"][:200]\n",
    "        test_batch = byte5_tokenizer.encode(test_string, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    train_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        model = model.to(device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, GPU usage:\")\n",
    "        display_gpu_memory()\n",
    "\n",
    "        for batch_count, batch in enumerate(train_loader):\n",
    "\n",
    "            batch = batch[\"text\"]\n",
    "            batch = byte5_tokenizer(batch, return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "            batch = batch[:, :1024]  # Truncate to maximum length of 4096 to save GPU memory.\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            loss_dict = bitter_tokenizer_training_step(model, batch, optimizer, learn_gating=learn_gating)\n",
    "            train_losses.append(loss_dict)\n",
    "\n",
    "            # Memory tracking for each batch\n",
    "            if batch_count % batch_print_every == 0:\n",
    "                print(f\"Batch {batch_count} ar train loss: {loss_dict['ar_loss']} nats/token selected action ce: {loss_dict['selected_action_ce']}\")\n",
    "                with torch.no_grad():\n",
    "                    out = model(test_batch)\n",
    "\n",
    "                gate_samples = out[\"down_gate_samples\"]\n",
    "                merge_dst = out[\"down_merge_dst\"]\n",
    "                true_rate = gate_samples.float().mean().item()\n",
    "                implied_iid_ce = -true_rate * np.log(true_rate) - (1 - true_rate) * np.log(1 - true_rate)\n",
    "                print(f\"Downsample rate: {true_rate:4f} implied iid ce: {implied_iid_ce:4f}\")\n",
    "                display_gating(test_batch[0], merge_dst[0])\n",
    "\n",
    "                if batch_limit is not None and batch_count > batch_limit:\n",
    "                    break\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train loss: {np.mean([l['total_loss'] for l in train_losses]):.4f}\")\n",
    "    \n",
    "    train_losses = pd.DataFrame(train_losses)\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "    return train_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 27 19:34:58 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:39:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              45W / 300W |      3MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000000:3C:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              45W / 300W |      3MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
