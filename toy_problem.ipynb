{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtune in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (0.6.1)\n",
      "Requirement already satisfied: torchdata==0.11.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (0.11.0)\n",
      "Requirement already satisfied: datasets in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (2.21.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (0.26.2)\n",
      "Requirement already satisfied: safetensors in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (0.4.5)\n",
      "Requirement already satisfied: kagglehub in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (0.3.11)\n",
      "Requirement already satisfied: sentencepiece in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (0.6.0)\n",
      "Requirement already satisfied: blobfile>=2 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (2.1.1)\n",
      "Requirement already satisfied: tokenizers in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (0.20.3)\n",
      "Requirement already satisfied: numpy in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (4.66.5)\n",
      "Requirement already satisfied: omegaconf in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: psutil in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (5.9.0)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchtune) (10.4.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.2.3)\n",
      "Requirement already satisfied: requests in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torchdata==0.11.0->torchtune) (2.4.1)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.21.0)\n",
      "Requirement already satisfied: lxml~=4.9 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from blobfile>=2->torchtune) (4.9.4)\n",
      "Requirement already satisfied: filelock~=3.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->torchtune) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (3.10.5)\n",
      "Requirement already satisfied: packaging in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from requests->torchdata==0.11.0->torchtune) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from requests->torchdata==0.11.0->torchtune) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from requests->torchdata==0.11.0->torchtune) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.2)\n",
      "Requirement already satisfied: networkx in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from torch>=2->torchdata==0.11.0->torchtune) (72.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages (from sympy->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torchtune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchao\n",
      "  Downloading torchao-0.10.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
      "Downloading torchao-0.10.0-cp39-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchao\n",
      "Successfully installed torchao-0.10.0\n"
     ]
    }
   ],
   "source": [
    "! pip install torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/bin/tune\", line 5, in <module>\n",
      "    from torchtune._cli.tune import main\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/__init__.py\", line 38, in <module>\n",
      "    from torchtune import datasets, generation, models, modules, utils\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/datasets/__init__.py\", line 7, in <module>\n",
      "    from torchtune.datasets import multimodal\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/datasets/multimodal/__init__.py\", line 7, in <module>\n",
      "    from ._llava_instruct import llava_instruct_dataset\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/datasets/multimodal/_llava_instruct.py\", line 10, in <module>\n",
      "    from torchtune.data._messages import ShareGPTToMessages\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/data/__init__.py\", line 7, in <module>\n",
      "    from torchtune.data._collate import (\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/data/_collate.py\", line 12, in <module>\n",
      "    from torchtune.modules.attention_utils import packed_block_causal_mask\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/modules/__init__.py\", line 7, in <module>\n",
      "    from .attention import MultiHeadAttention  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/modules/attention.py\", line 12, in <module>\n",
      "    from torchtune.modules.attention_utils import _MaskType, _sdpa_or_flex_attention\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/modules/attention_utils.py\", line 13, in <module>\n",
      "    from torchtune.utils._import_guard import _SUPPORTS_FLEX_ATTENTION\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/utils/__init__.py\", line 7, in <module>\n",
      "    from ._device import (\n",
      "  File \"/itet-stor/sdauncey/net_scratch/conda_envs/geometric_diffusers/lib/python3.12/site-packages/torchtune/utils/_device.py\", line 19, in <module>\n",
      "    from torch.nn.attention.flex_attention import BlockMask\n",
      "ModuleNotFoundError: No module named 'torch.nn.attention.flex_attention'\n"
     ]
    }
   ],
   "source": [
    "! tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.version import __version__\n",
    "import torch\n",
    "\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE implementation borrowed from Gemma https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/modeling_gemma.py\n",
    "class GemmaRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids, seq_len=None):\n",
    "        # position_ids: [bs, seq_len]\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        # returns cos: [bs, seq_len, head_size] where cos[b, i, j] = cos(pos_ids[b, i] * inv_freq[j])\n",
    "        # returns sin: [bs, seq_len, head_size] where sin[b, i, j] = sin(pos_ids[b, i] * inv_freq[j])\n",
    "        self.inv_freq.to(x.device)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        print(position_ids_expanded.shape, inv_freq_expanded.shape)\n",
    "\n",
    "        # Force float32 since bfloat16 loses precision on long contexts\n",
    "        # See https://github.com/huggingface/transformers/pull/29285\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    # if x[b, i] = ((1, 2, 3, 4, 5, 6..., a + 1, a+2, a+3..., 2a)) \n",
    "    # the rotate_half(x)[b, i] = ((-a-1, -a-2, -a-3..., -2a, 1, 2, 3, 4, 5, 6..., a))\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    # The key is that (to translate the paper notation to code notation) \n",
    "    # x_j = x[..., (j+1)//2] if j is odd\n",
    "    # x_j = x[..., (head_dim + j)//2] if j is even\n",
    "    # and cos[b, i, j] = cos[b, i , j + head_dim//2]\n",
    "\n",
    "    # Such that, for j < head_dim // 2,\n",
    "    # q_embed[b, i, j] = q[b, i, j] * cos[b, i, j] - q[b, i, j + head_dim // 2] * sin[b, i, j]\n",
    "    # = x_j cos(theta_j) - x_{j+1} sin(theta_{j})\n",
    "\n",
    "    # and for j >= head_dim // 2,\n",
    "    # q_embed[b, i, j] = q[b, i, j] * cos[b, i, j] + q[b, i, j - head_dim // 2] * sin[b, i, j]\n",
    "    # = x_j cos(theta_j) + x_{j-1} sin(theta_{j-1})\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gemma2.modeling_gemma2 import Gemma2DecoderLayer, Gemma2Config, Gemma2Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,\n",
       " Gemma2DecoderLayer(\n",
       "   (self_attn): Gemma2Attention(\n",
       "     (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "     (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "     (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "     (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "     (rotary_emb): Gemma2RotaryEmbedding()\n",
       "   )\n",
       "   (mlp): Gemma2MLP(\n",
       "     (gate_proj): Linear(in_features=128, out_features=512, bias=False)\n",
       "     (up_proj): Linear(in_features=128, out_features=512, bias=False)\n",
       "     (down_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "     (act_fn): PytorchGELUTanh()\n",
       "   )\n",
       "   (input_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "   (pre_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "   (post_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "   (post_attention_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       " ))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_sliding_window = True\n",
    "gemma_layer_idx = int(not use_sliding_window)\n",
    "\n",
    "l = Gemma2DecoderLayer(config, layer_idx=gemma_layer_idx) \n",
    "l.sliding_window, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.1135,  0.4218,  0.7886,  ..., -0.0882, -1.3569, -1.2370],\n",
       "          [-1.3356,  1.5032, -0.7773,  ...,  0.7668, -0.4292, -1.3472],\n",
       "          [ 0.9520, -1.5788, -0.5847,  ...,  3.9367,  0.0774,  0.3537],\n",
       "          ...,\n",
       "          [-0.3520, -0.2929, -1.6333,  ..., -0.3577, -0.2004, -1.2158],\n",
       "          [ 1.4927, -1.0173,  0.4418,  ..., -0.3577, -1.7583,  0.2881],\n",
       "          [-0.0309,  2.7137,  0.8338,  ...,  1.4638, -1.0924, -0.7616]]],\n",
       "        grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 10, 128)\n",
    "l(x, position_ids=torch.arange(10).unsqueeze(0).expand(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "\n",
    "def get_merge_dst(gate_samples: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns (merge_dst, dst_idx) the merge destination for each token in the sequence and the number of unique merge destinations.\n",
    "    For now, has a janky python for-loop implementation.\n",
    "    Input is a tensor of shape (batch_size, sequence_length) with 0 tokens are merged into the next 1 token.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = gate_samples.shape\n",
    "    merge_dst = torch.zeros_like(gate_samples, dtype=torch.long)\n",
    "    n_dst = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "    # Process each batch separately\n",
    "    for b in range(batch_size):\n",
    "        dst_idx = 0\n",
    "        for i in range(seq_len):\n",
    "            merge_dst[b, i] = dst_idx\n",
    "            if gate_samples[b, i] == 1 and i < seq_len - 1:\n",
    "                # If previous position had gate=1, keep the same destination\n",
    "                dst_idx += 1\n",
    "\n",
    "        n_dst[b] = dst_idx + 1\n",
    "\n",
    "    return merge_dst, n_dst\n",
    "\n",
    "\n",
    "class GemmaMiniBitterLLM(nn.Module):\n",
    "    # A mini BitterLLM with 2 down, 4 mid, and 2 up layers. As a vibe check on the idea.\n",
    "    # Use Gemma2DecoderLayer as a drop in replacement for the TransformerEncoderLayer, with RoPE and sliding window pre-implemented.\n",
    "    # Also uses \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, num_heads: int, dropout: float=0.01, downsample_rate: float = 0.25, sliding_window = 64):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        head_dim = embedding_dim // num_heads\n",
    "\n",
    "\n",
    "        byte_layer_config = Gemma2Config(\n",
    "            head_dim=head_dim,\n",
    "            query_pre_attn_scalar=head_dim, \n",
    "            sliding_window=sliding_window,\n",
    "            intermediate_size=embedding_dim,\n",
    "            hidden_size=embedding_dim,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_key_value_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        deep_layer_config = Gemma2Config(\n",
    "            head_dim=head_dim,\n",
    "            query_pre_attn_scalar=head_dim, \n",
    "            sliding_window=None,\n",
    "            intermediate_size=embedding_dim * 4, # dim_feedforward should scale inversely with the number of tokens in the sequence.\n",
    "            hidden_size=embedding_dim,\n",
    "            num_attention_heads=num_heads,\n",
    "            num_key_value_heads=num_heads\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Layer idx=0 is necessary for the sliding window to be applied.\n",
    "        self.down_layers = nn.ModuleList([\n",
    "            Gemma2DecoderLayer(byte_layer_config, layer_idx=0) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.mid_layers = nn.ModuleList([\n",
    "            Gemma2DecoderLayer(byte_layer_config, layer_idx=1) for _ in range(2) \n",
    "        ])\n",
    "\n",
    "        self.up_layers = nn.ModuleList([\n",
    "            Gemma2DecoderLayer(byte_layer_config, layer_idx=0) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        # Initialize a gate for each layer.\n",
    "        layer_gate_init = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        # Copy the gate for each layer. \n",
    "        # Initializing by copying inductively biases the model to tokenize in a later layer if the gate is high but the model chose not to.\n",
    "        self.down_layer_gate = deepcopy(layer_gate_init)\n",
    "        self.downsample_rate = downsample_rate\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        batch_size, max_seq_len = x.shape\n",
    "\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        position_ids = torch.arange(max_seq_len, dtype=x.dtype).unsqueeze(0).expand(batch_size, -1).to(x.device)\n",
    "\n",
    "        # Apply down layers to byte tokens        \n",
    "        for layer in self.down_layers:\n",
    "            x = layer(x, position_ids=position_ids)[0]\n",
    "\n",
    "        # Sample gating binary variables for each token.\n",
    "        down_gate_logits = self.down_layer_gate(x)\n",
    "        down_gate_probs = F.sigmoid(down_gate_logits)\n",
    "        down_gate_samples = torch.bernoulli(down_gate_probs)\n",
    "\n",
    "        # Hack: ensure for now that we always gate on the first token:\n",
    "        down_gate_samples[:, 0] = 1.\n",
    "\n",
    "        # Merge the tokens into the next token where the gate is 1.\n",
    "        down_gate_samples = down_gate_samples.squeeze(-1)\n",
    "        down_merge_dst, n_dst = get_merge_dst(down_gate_samples)\n",
    "\n",
    "        # Also merge the position ids.\n",
    "        position_ids_downsampled = torch.zeros(batch_size, n_dst.max(), dtype=x.dtype).to(x.device)\n",
    "        position_ids_downsampled = torch.scatter_reduce(position_ids_downsampled, dim=1, index=down_merge_dst, src=position_ids, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        # Merge the downsampled tokens.\n",
    "        down_merge_dst = down_merge_dst.unsqueeze(-1).expand(-1, -1, self.embedding_dim)\n",
    "\n",
    "        x_downsampled = torch.zeros(batch_size, n_dst.max(), self.embedding_dim, dtype=x.dtype).to(x.device)\n",
    "        x_downsampled = torch.scatter_reduce(x_downsampled, dim=1, index=down_merge_dst, src=x, reduce=\"mean\", include_self=False)\n",
    "\n",
    "        # Apply mid layers to merged tokens and compute the deviation\n",
    "        for layer in self.mid_layers:\n",
    "            y_downsampled = layer(x_downsampled, position_ids=position_ids_downsampled)[0]\n",
    "            deviation = y_downsampled - x_downsampled        \n",
    "\n",
    "        # Upsample by removing the first token merge group, shifting all token groups down and adding another one token group at the end.\n",
    "        up_gate_samples = down_gate_samples[:, 1:]\n",
    "        up_gate_samples = torch.cat([up_gate_samples, torch.ones(batch_size, 1, dtype=up_gate_samples.dtype).to(up_gate_samples.device)], dim=1)\n",
    "        up_merge_dst, _ = get_merge_dst(up_gate_samples)\n",
    "        up_merge_dst = up_merge_dst.unsqueeze(-1).expand(-1, -1, self.embedding_dim)\n",
    "\n",
    "        # Add the upsampled deviation to the input to the middle layers\n",
    "        upsampled_deviation = torch.gather(deviation, dim=1, index=up_merge_dst)\n",
    "        y = x + upsampled_deviation\n",
    "\n",
    "        # Apply up layers to byte tokens\n",
    "        for layer in self.up_layers:\n",
    "            y = layer(y, position_ids=position_ids)[0]\n",
    "\n",
    "        # Map residual stream to logits\n",
    "        logits = self.output_layer(y)\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        out = {\n",
    "            \"logits\": logits,\n",
    "            \"down_gate_probs\": down_gate_probs.squeeze(-1),\n",
    "            \"down_gate_logits\": down_gate_logits.squeeze(-1),\n",
    "            \"down_gate_samples\": down_gate_samples.to(dtype=torch.long),\n",
    "            \"down_merge_dst\": down_merge_dst[:, :, 0], # This dimension is repeated.\n",
    "            \"up_merge_dst\": up_merge_dst[:, :, 0],\n",
    "            \"n_dst\": n_dst,\n",
    "        }\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaMiniBitterLLM(\n",
       "  (embedding): Embedding(5, 128)\n",
       "  (down_layers): ModuleList(\n",
       "    (0-1): 2 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (up_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (down_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (mid_layers): ModuleList(\n",
       "    (0-1): 2 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (up_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (down_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (up_layers): ModuleList(\n",
       "    (0-1): 2 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (up_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (down_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((128,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (down_layer_gate): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = GemmaMiniBitterLLM(vocab_size=5, embedding_dim=128, num_heads=8, dropout=0.01, downsample_rate=0.25, sliding_window=64)\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[[-3.1004, -4.3510, -4.5187, -1.1112, -0.5075],\n",
       "          [-5.9664, -3.6229, -3.1658, -3.0314, -0.1275],\n",
       "          [-3.4827, -3.4981, -4.3503, -0.5938, -0.9838],\n",
       "          [-2.4602, -1.1707, -1.6812, -3.2660, -0.9672],\n",
       "          [-2.2847, -1.9154, -1.8480, -3.0081, -0.6089],\n",
       "          [-2.2779, -1.9298, -1.7474, -2.8227, -0.6565],\n",
       "          [-3.3053, -1.1156, -4.2962, -1.3617, -1.0059],\n",
       "          [-3.3878, -1.3458, -4.7451, -1.9585, -0.5868],\n",
       "          [-6.1343, -3.6274, -3.0678, -2.8861, -0.1405],\n",
       "          [-6.0930, -3.6900, -2.9890, -2.7109, -0.1555]]],\n",
       "        grad_fn=<LogSoftmaxBackward0>),\n",
       " 'down_gate_probs': tensor([[0.1863, 0.8490, 0.1945, 0.5522, 0.5443, 0.4933, 0.3663, 0.3715, 0.8474,\n",
       "          0.8448]], grad_fn=<SqueezeBackward1>),\n",
       " 'down_gate_logits': tensor([[-1.4739,  1.7266, -1.4213,  0.2094,  0.1775, -0.0267, -0.5482, -0.5256,\n",
       "           1.7142,  1.6941]], grad_fn=<SqueezeBackward1>),\n",
       " 'down_gate_samples': tensor([[1, 1, 0, 0, 1, 0, 0, 1, 1, 1]]),\n",
       " 'down_merge_dst': tensor([[0, 1, 2, 2, 2, 3, 3, 3, 4, 5]]),\n",
       " 'up_merge_dst': tensor([[0, 1, 1, 1, 2, 2, 2, 3, 4, 5]]),\n",
       " 'n_dst': tensor([6])}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.randint(0, 5, (1, 10))\n",
    "my_model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBAABABAABBAABBBAABABAA', 'ABAAABAABBAABABAA']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_patterns = [\"10110\", \"0010\"]\n",
    "lower_patterns = [\"ABAA\", \"BBAAB\"]\n",
    "\n",
    "imputed_upper_patterns = [\n",
    "    p.replace(\"0\", lower_patterns[0]).replace(\"1\", lower_patterns[1]) for p in upper_patterns\n",
    "]\n",
    "imputed_upper_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(n_upper):\n",
    "    choices = [random.randint(0, 1) for _ in range(n_upper)]\n",
    "\n",
    "    x = \"\".join(\n",
    "        imputed_upper_patterns[c] for c in choices\n",
    "    )\n",
    "    return x, choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BBAABABAABBAABBBAABABAAABAAABAABBAABABAAABAAABAABBAABABAAABAAABAABBAABABAAABAAABAABBAABABAAABAAABAABBAABABAABBAABABAABBAABBBAABABAABBAABABAABBAABBBAABABAABBAABABAABBAABBBAABABAAABAAABAABBAABABAA',\n",
       " [0, 1, 1, 1, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: force MiniBitterLM to learn these strings, with a context windom of 4 for the initial tokens.\n",
    "# In theory, there should be less than 2 / (6 + 4) * (4 + 6) = 1/50 bits per character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = [sample(10) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_discounted_rewards(rewards, discount):\n",
    "    \"\"\"\n",
    "    Assumes that rewards is a numpy array of shape (n_episodes, n_timesteps). Returns tensor of same shape.\n",
    "    credit to: https://stackoverflow.com/questions/47970683/vectorize-a-numpy-discount-calculation/47971187#47971187,\n",
    "    minor modifications made to vectorise computation.\n",
    "    C[i] = R[i] + discount * C[i+1]\n",
    "    signal.lfilter(b, a, x, axis=-1, zi=None)\n",
    "    a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
    "                          - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
    "    \"\"\"\n",
    "    r = rewards[:, ::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = lfilter(b, a, x=r)\n",
    "    return y[:, ::-1]\n",
    "\n",
    "\n",
    "def discounted_rewards_torch(rewards, discount):\n",
    "    \"\"\"torch wrapper for compute_discounted_rewards. Warning: does _not_ allow for backprop through the rewards, which is fine for policy gradients.\"\"\"\n",
    "    rewards_device = rewards.device\n",
    "    rewards = rewards.detach().cpu().numpy()\n",
    "    discounted_rewards = compute_discounted_rewards(rewards, discount)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards.copy(), device=rewards_device) # Copy as torch doesn't like converting negatively strided arrays\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def bitter_tokenizer_training_step(model, batch, optimizer):\n",
    "    \"\"\"\n",
    "    Assume that batch is torch.tensor of token ids of shape (batch, sequence_length). returns a dict of floats of the training losses for the batch.\n",
    "    \"\"\"\n",
    "    batch_size, _ = batch.shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(batch)\n",
    "    logits = out[\"logits\"]\n",
    "    down_gate_samples = out[\"down_gate_samples\"]\n",
    "    down_gate_probs = out[\"down_gate_probs\"]\n",
    "    down_gate_logits = out[\"down_gate_logits\"]\n",
    "    \n",
    "    # Compute autoregressive loss: log probability of next token.\n",
    "    next_token_ids = batch[:, 1:]\n",
    "    current_token_logits = logits[:, :-1]\n",
    "    next_token_logits = F.cross_entropy(current_token_logits.transpose(1, 2), next_token_ids, reduction=\"none\") # Transpose as F.cross_entropy wants shape [batch, classes, ...]\n",
    "    ar_loss = next_token_logits.mean()\n",
    "\n",
    "    # Compute gating loss: discounted log probabilities of following token(s).\n",
    "    discount_rate = 0.95\n",
    "    next_token_logits_padded = torch.cat([next_token_logits, torch.zeros(batch_size, 1, device=next_token_logits.device)], dim=-1) # Pad the last reward as zero\n",
    "    discounted_rewards = discounted_rewards_torch(next_token_logits_padded, discount_rate)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) # Simple estimate of the advantage\n",
    "\n",
    "    # action 0 = continue, action 1 = gate\n",
    "    action_log_probs = torch.stack([torch.zeros_like(down_gate_logits), down_gate_logits], dim=1) # As a sigmoid is equivalent to having one logit as 0.\n",
    "    selected_action_log_probs = F.cross_entropy(action_log_probs, down_gate_samples, reduction=\"none\")\n",
    "    gating_loss = - (discounted_rewards * selected_action_log_probs).mean() # Negative as we want to maximise the reward.\n",
    "\n",
    "    # Hacky additional consistency loss: make the downsampling rate match the training gating.\n",
    "    down_gate_rate_loss =  5.*(model.downsample_rate - down_gate_probs.mean()) **2\n",
    "\n",
    "    # Optimizer step\n",
    "    total_loss = ar_loss + gating_loss + down_gate_rate_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    out = {\n",
    "        \"ar_loss\": ar_loss.item(),\n",
    "        \"gating_loss\": gating_loss.item(),\n",
    "        \"rate_consistency_loss\": down_gate_rate_loss.item(),\n",
    "        \"total_loss\": total_loss.item(),\n",
    "        \"selected_action_ce\": selected_action_log_probs.mean().item()\n",
    "    }\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def display_gating(tokens_ids, merge_dst):\n",
    "    \"\"\"Display how a SmallBitterLLM merges a sequence. token_ids and merge_dst are tensors of shape (sequence_length,).\"\"\"\n",
    "    previous_merge_dst = 0\n",
    "    for t_id, merge_destinantion in zip(tokens_ids, merge_dst):\n",
    "        merge_destinantion = merge_destinantion.item()\n",
    "        \n",
    "        t_txt = byte5_tokenizer.decode(t_id)\n",
    "        print(f\"{t_txt.replace('\\n', '\\\\n')}\", end=\"\")\n",
    "\n",
    "        if merge_destinantion != previous_merge_dst:\n",
    "            print(f\"|\", end=\"\")\n",
    "            previous_merge_dst = merge_destinantion\n",
    "\n",
    "    print()\n",
    "        \n",
    "\n",
    "\n",
    "def bitter_tokenizer_training_loop(model, train_dataset):\n",
    "    # TODO: validation dataset\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # See how the model merges a sequence.\n",
    "    test_string = openwebtext_8k[-1][\"text\"][:200]\n",
    "    print(f\"{test_string.replace('\\n', '\\\\n')}\")\n",
    "    test_batch = byte5_tokenizer.encode(test_string, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    # Start memory tracking\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 2\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        model = model.to(device)\n",
    "        train_losses = []\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, GPU usage:\")\n",
    "        display_gpu_memory()\n",
    "        \n",
    "        # CPU memory tracking\n",
    "        process = psutil.Process()\n",
    "        print(f\"CPU Memory before epoch: {process.memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "        batch_count = 0\n",
    "        for batch in train_loader:\n",
    "\n",
    "            batch = batch[\"text\"]\n",
    "            batch = byte5_tokenizer(batch, return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "            batch = batch[:, :4096]  # Truncate to maximum length of 4096 to save GPU memory.\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            loss_dict = bitter_tokenizer_training_step(model, batch, optimizer)\n",
    "            train_losses.append(loss_dict)\n",
    "\n",
    "            # Memory tracking for each batch\n",
    "            if batch_count % 10 == 0:\n",
    "                # print(f\"CPU Memory at batch {batch_count}: {process.memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "                # current, peak = tracemalloc.get_traced_memory()\n",
    "                # print(f\"Current memory usage: {current / 10**6:.2f} MB; Peak: {peak / 10**6:.2f} MB\")\n",
    "                \n",
    "                # # Force garbage collection to see if memory is being properly released\n",
    "                # gc.collect()\n",
    "                # print(f\"After GC: {process.memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "                print(f\"Batch {batch_count} ar train loss: {loss_dict['ar_loss']} nats/token\")\n",
    "                with torch.no_grad():\n",
    "                    out = model(test_batch)\n",
    "                display_gating(test_batch[0], out[\"down_merge_dst\"][0])\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train loss: {np.mean([l['total_loss'] for l in train_losses]):.4f}\")\n",
    "        \n",
    "        # Memory snapshot at end of epoch\n",
    "        print(f\"CPU Memory after epoch: {process.memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "        snapshot = tracemalloc.take_snapshot()\n",
    "        top_stats = snapshot.statistics('lineno')\n",
    "        print(\"[ Top 10 memory consumers ]\")\n",
    "        for stat in top_stats[:10]:\n",
    "            print(stat)\n",
    "    \n",
    "    # Stop memory tracking\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    train_losses = pd.DataFrame(train_losses)\n",
    "\n",
    "    return train_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
