{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from `shortening.py` at https://github.com/PiotrNawrot/dynamic-pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def final(foo,\n",
    "          upsample):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            B x L x S\n",
    "    \"\"\"\n",
    "    autoregressive = foo != 0\n",
    "    lel = 1 - foo\n",
    "\n",
    "    lel[autoregressive] = 0\n",
    "\n",
    "    dim = 2 if upsample else 1\n",
    "\n",
    "    lel = lel / (lel.sum(dim=dim, keepdim=True) + 1e-9)\n",
    "\n",
    "    return lel\n",
    "\n",
    "\n",
    "def common(boundaries, upsample=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    boundaries = boundaries.clone()\n",
    "\n",
    "    n_segments = boundaries.sum(dim=-1).max().item()\n",
    "\n",
    "    if upsample:\n",
    "        n_segments += 1\n",
    "\n",
    "    if n_segments == 0:\n",
    "        return None\n",
    "\n",
    "    tmp = torch.zeros_like(\n",
    "        boundaries\n",
    "    ).unsqueeze(2) + torch.arange(\n",
    "        start=0,\n",
    "        end=n_segments,\n",
    "        device=boundaries.device\n",
    "    )\n",
    "\n",
    "    hh1 = boundaries.cumsum(1)\n",
    "\n",
    "    if not upsample:\n",
    "        hh1 -= boundaries\n",
    "\n",
    "    foo = tmp - hh1.unsqueeze(-1)\n",
    "\n",
    "    return foo\n",
    "\n",
    "\n",
    "def downsample(boundaries, hidden, null_group):\n",
    "    \"\"\"\n",
    "        Downsampling\n",
    "\n",
    "        - The first element of boundaries tensor is always 0 and doesn't matter\n",
    "        - 1 starts a new group\n",
    "        - We append an extra \"null\" group at the beginning\n",
    "        - We discard last group because it won't be used (in terms of upsampling)\n",
    "\n",
    "        Input:\n",
    "            boundaries: B x L\n",
    "            hidden: L x B x D\n",
    "        Output:\n",
    "            shortened_hidden: S x B x D\n",
    "    \"\"\"\n",
    "\n",
    "    foo = common(boundaries, upsample=False)  # B x L x S\n",
    "\n",
    "    if foo is None:\n",
    "        return null_group.repeat(1, hidden.size(1), 1)\n",
    "    else:\n",
    "        bar = final(foo=foo, upsample=False)  # B x L x S\n",
    "\n",
    "        shortened_hidden = torch.einsum('lbd,bls->sbd', hidden, bar)\n",
    "        shortened_hidden = torch.cat(\n",
    "            [null_group.repeat(1, hidden.size(1), 1), shortened_hidden], dim=0\n",
    "        )\n",
    "\n",
    "        return shortened_hidden\n",
    "\n",
    "\n",
    "def upsample(boundaries, shortened_hidden):\n",
    "    \"\"\"\n",
    "        Upsampling\n",
    "\n",
    "        - The first element of boundaries tensor is always 0 and doesn't matter\n",
    "        - 1 starts a new group\n",
    "        - i-th group can be upsampled only to the tokens from (i+1)-th group, otherwise there's a leak\n",
    "\n",
    "        Input:\n",
    "            boundaries: B x L\n",
    "            shortened_hidden: S x B x D\n",
    "        Output:\n",
    "            upsampled_hidden: L x B x D\n",
    "    \"\"\"\n",
    "\n",
    "    foo = common(boundaries, upsample=True)  # B x L x S\n",
    "    bar = final(foo, upsample=True)  # B x L x S\n",
    "\n",
    "    return torch.einsum('sbd,bls->lbd', shortened_hidden, bar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line `shortened_hidden = torch.einsum('lbd,bls->sbd', hidden, bar)` is where the predicted boundaries from the MLP and the hidden states re-join on the computation graph. In the forward pass, the multiplication by `bar` acts an average pooling operation, with the `boundaries` variable being used to construct the matrix `bar`. In order to train the MLP, gradients need to flow backward through the construction of the matrix `bar`, here I analyse the derivative of one value of bar with respect to the boundaries is implicitly defined by this construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_hard_boundaries = torch.tensor([[0, 0, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=torch.float32, requires_grad=True)\n",
    "my_hard_boundaries.retain_grad()\n",
    "my_hard_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 4]),\n",
       " tensor([[[0.3333, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3333, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3333, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = common(my_hard_boundaries, upsample=False)\n",
    "bar = final(foo, upsample=False)\n",
    "bar.shape, bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = bar[0, 6, 3]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 1.]], requires_grad=True),\n",
       " tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1875, -0.1250,\n",
       "          -0.0625,  0.0000]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.backward()\n",
    "my_hard_boundaries, my_hard_boundaries.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping this in a for loop, we can compute the whole jacobian of the non-zero value in each row of bar with respect to the boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_values = []\n",
    "\n",
    "for i in range(10):\n",
    "    my_hard_boundaries = torch.tensor([[0, 0, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=torch.float32, requires_grad=True)\n",
    "    my_hard_boundaries.retain_grad()\n",
    "    foo = common(my_hard_boundaries, upsample=False)\n",
    "    bar = final(foo, upsample=False)\n",
    "    l = bar[0, i, :].sum() # We can safely sum as the 0. values in bar are disconnected from the computation graph.\n",
    "    l.backward()\n",
    "    grad_values.append(my_hard_boundaries.grad)\n",
    "    my_hard_boundaries.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2222, -0.1111,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.1111, -0.1111,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.1111,  0.2222,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2500,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1875, -0.1250,\n",
       "         -0.0625,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0625, -0.1250,\n",
       "         -0.0625,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0625,  0.1250,\n",
       "         -0.0625,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0625,  0.1250,\n",
       "          0.1875,  0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian = torch.cat(grad_values, dim=0)\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2222, -0.1111,  0.0000],\n",
       "         [ 0.1111, -0.1111,  0.0000],\n",
       "         [ 0.1111,  0.2222,  0.0000]]),\n",
       " tensor([[0.]]),\n",
       " tensor([[-0.2500,  0.0000],\n",
       "         [ 0.2500,  0.0000]]),\n",
       " tensor([[-0.1875, -0.1250, -0.0625,  0.0000],\n",
       "         [ 0.0625, -0.1250, -0.0625,  0.0000],\n",
       "         [ 0.0625,  0.1250, -0.0625,  0.0000],\n",
       "         [ 0.0625,  0.1250,  0.1875,  0.0000]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian[:3, :3], jacobian[3:4, 3:4], jacobian[4:6, 4:6], jacobian[6:10, 6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_hard_boundaries = torch.tensor([[0, 0, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=torch.float32, requires_grad=True)\n",
    "my_hard_boundaries.retain_grad()\n",
    "my_hard_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3333, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "         [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "         [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "         [0.0000, 0.0000, 0.0000, 0.2500]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the effect of multiplying foo by -2. does not effect the forward pass:\n",
    "\n",
    "foo_modified = common(my_hard_boundaries, upsample=False)\n",
    "foo_modified = -2. * foo_modified\n",
    "bar_modified = final(foo_modified, upsample=False)\n",
    "\n",
    "foo_origonal = common(my_hard_boundaries, upsample=False)\n",
    "bar_origonal = final(foo_origonal, upsample=False)\n",
    "\n",
    "bar_modified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(bar_modified, bar_origonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_values_modified = []\n",
    "\n",
    "for i in range(10):\n",
    "    my_hard_boundaries = torch.tensor([[0, 0, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=torch.float32, requires_grad=True)\n",
    "    my_hard_boundaries.retain_grad()\n",
    "    foo_modified = common(my_hard_boundaries, upsample=False)\n",
    "    foo_modified = -2. * foo_modified\n",
    "    bar_modified = final(foo_modified, upsample=False)\n",
    "    l = bar_modified[0, i, :].sum() # We can safely sum as the 0. values in bar are disconnected from the computation graph.\n",
    "    l.backward()\n",
    "    grad_values_modified.append(my_hard_boundaries.grad)\n",
    "    my_hard_boundaries.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4444,  0.2222,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.2222,  0.2222,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.2222, -0.4444,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.5000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.3750,  0.2500,\n",
       "          0.1250,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1250,  0.2500,\n",
       "          0.1250,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1250, -0.2500,\n",
       "          0.1250,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1250, -0.2500,\n",
       "         -0.3750,  0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_modified = torch.cat(grad_values_modified, dim=0)\n",
    "jacobian_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4444,  0.2222,  0.0000],\n",
       "         [-0.2222,  0.2222,  0.0000],\n",
       "         [-0.2222, -0.4444,  0.0000]]),\n",
       " tensor([[0.]]),\n",
       " tensor([[ 0.5000,  0.0000],\n",
       "         [-0.5000,  0.0000]]),\n",
       " tensor([[ 0.3750,  0.2500,  0.1250,  0.0000],\n",
       "         [-0.1250,  0.2500,  0.1250,  0.0000],\n",
       "         [-0.1250, -0.2500,  0.1250,  0.0000],\n",
       "         [-0.1250, -0.2500, -0.3750,  0.0000]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_modified[:3, :3], jacobian_modified[3:4, 3:4], jacobian_modified[4:6, 4:6], jacobian_modified[6:10, 6:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
