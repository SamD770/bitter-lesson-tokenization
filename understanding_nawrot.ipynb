{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryPredictor(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, activation_function,\n",
    "                 temp, prior, bp_type, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.prior = prior\n",
    "        self.bp_type = bp_type\n",
    "        self.threshold = threshold\n",
    "\n",
    "        if activation_function == 'relu':\n",
    "            activation_fn = nn.ReLU(inplace=True)\n",
    "        elif activation_function == 'gelu':\n",
    "            activation_fn = torch.nn.GELU()\n",
    "\n",
    "        self.boundary_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_inner),\n",
    "            activation_fn,\n",
    "            nn.Linear(d_inner, 1),\n",
    "        )\n",
    "\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        # Hidden is of shape [seq_len x bs x d_model]\n",
    "        # Boundaries we return are [bs x seq_len]\n",
    "        boundary_logits = self.boundary_predictor(hidden).squeeze(-1).transpose(0, 1)\n",
    "        boundary_probs = torch.sigmoid(boundary_logits)\n",
    "\n",
    "        if self.bp_type == 'gumbel':\n",
    "            bernoulli = torch.distributions.relaxed_bernoulli.RelaxedBernoulli(\n",
    "                temperature=self.temp,\n",
    "                probs=boundary_probs,\n",
    "            )\n",
    "\n",
    "            soft_boundaries = bernoulli.rsample()\n",
    "\n",
    "            hard_boundaries = (soft_boundaries > self.threshold).float()\n",
    "            hard_boundaries = (\n",
    "                hard_boundaries - soft_boundaries.detach() + soft_boundaries\n",
    "            )\n",
    "        elif self.bp_type in ['entropy', 'unigram']:\n",
    "            soft_boundaries = boundary_probs\n",
    "            hard_boundaries = (soft_boundaries > self.threshold).float()\n",
    "\n",
    "        return soft_boundaries, hard_boundaries\n",
    "\n",
    "    def calc_loss(self, preds, gt):\n",
    "        # B x T\n",
    "        if self.bp_type in ['entropy', 'unigram']:\n",
    "            assert preds is not None and gt is not None\n",
    "            return self.loss(preds, gt.float())\n",
    "        elif self.bp_type in ['gumbel']:\n",
    "            assert gt is None\n",
    "            binomial = torch.distributions.binomial.Binomial(\n",
    "                preds.size(-1),\n",
    "                probs=torch.Tensor([self.prior]).to(preds.device)\n",
    "            )\n",
    "            loss_boundaries = -binomial.log_prob(\n",
    "                preds.sum(dim=-1)\n",
    "            ).mean() / preds.size(-1)\n",
    "\n",
    "            return loss_boundaries\n",
    "\n",
    "    def calc_stats(self, preds, gt):\n",
    "        # B x T\n",
    "        preds, gt = preds.bool(), gt.bool()\n",
    "        TP = ((preds == gt) & preds).sum().item()\n",
    "        FP = ((preds != gt) & preds).sum().item()\n",
    "        FN = ((preds != gt) & (~preds)).sum().item()\n",
    "\n",
    "        acc = (preds == gt).sum().item() / gt.numel()\n",
    "\n",
    "        if TP == 0:\n",
    "            precision, recall = 0, 0\n",
    "        else:\n",
    "            precision = TP / (TP + FP)\n",
    "            recall = TP / (TP + FN)\n",
    "\n",
    "        stats = {\n",
    "            'acc': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelBoundaryPredictor(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, activation_function,\n",
    "                 temp, prior, bp_type, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.prior = prior\n",
    "        self.bp_type = bp_type\n",
    "        self.threshold = threshold\n",
    "\n",
    "        if activation_function == 'relu':\n",
    "            activation_fn = nn.ReLU(inplace=True)\n",
    "        elif activation_function == 'gelu':\n",
    "            activation_fn = torch.nn.GELU()\n",
    "\n",
    "        self.boundary_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_inner),\n",
    "            activation_fn,\n",
    "            nn.Linear(d_inner, 1),\n",
    "        )\n",
    "\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        # Hidden is of shape [seq_len x bs x d_model]\n",
    "        # Boundaries we return are [bs x seq_len]\n",
    "        boundary_logits = self.boundary_predictor(hidden).squeeze(-1).transpose(0, 1)\n",
    "        boundary_probs = torch.sigmoid(boundary_logits)\n",
    "\n",
    "        bernoulli = torch.distributions.relaxed_bernoulli.RelaxedBernoulli(\n",
    "            temperature=self.temp,\n",
    "            probs=boundary_probs,\n",
    "        )\n",
    "\n",
    "        soft_boundaries = bernoulli.rsample()\n",
    "\n",
    "        hard_boundaries = (soft_boundaries > self.threshold).float()\n",
    "        hard_boundaries = (\n",
    "            hard_boundaries - soft_boundaries.detach() + soft_boundaries\n",
    "        )\n",
    "\n",
    "        return soft_boundaries, hard_boundaries\n",
    "\n",
    "    def calc_loss(self, preds, gt):\n",
    "        # B x T\n",
    "        # Regularization: binomial log probability of the number of boundaries\n",
    "        binomial = torch.distributions.binomial.Binomial(\n",
    "            preds.size(-1),\n",
    "            probs=torch.Tensor([self.prior]).to(preds.device)\n",
    "        )\n",
    "        loss_boundaries = -binomial.log_prob(\n",
    "            preds.sum(dim=-1)\n",
    "        ).mean() / preds.size(-1)\n",
    "\n",
    "        return loss_boundaries\n",
    "\n",
    "    def calc_stats(self, preds, gt):\n",
    "        # B x T\n",
    "        preds, gt = preds.bool(), gt.bool()\n",
    "        TP = ((preds == gt) & preds).sum().item()\n",
    "        FP = ((preds != gt) & preds).sum().item()\n",
    "        FN = ((preds != gt) & (~preds)).sum().item()\n",
    "\n",
    "        acc = (preds == gt).sum().item() / gt.numel()\n",
    "\n",
    "        if TP == 0:\n",
    "            precision, recall = 0, 0\n",
    "        else:\n",
    "            precision = TP / (TP + FP)\n",
    "            recall = TP / (TP + FN)\n",
    "\n",
    "        stats = {\n",
    "            'acc': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def final(foo,\n",
    "          upsample):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            B x L x S\n",
    "    \"\"\"\n",
    "    autoregressive = foo != 0\n",
    "    lel = 1 - foo\n",
    "\n",
    "    lel[autoregressive] = 0\n",
    "\n",
    "    dim = 2 if upsample else 1\n",
    "\n",
    "    lel = lel / (lel.sum(dim=dim, keepdim=True) + 1e-9)\n",
    "\n",
    "    return lel\n",
    "\n",
    "\n",
    "def common(boundaries, upsample=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    boundaries = boundaries.clone()\n",
    "\n",
    "    n_segments = boundaries.sum(dim=-1).max().item()\n",
    "\n",
    "    if upsample:\n",
    "        n_segments += 1\n",
    "\n",
    "    if n_segments == 0:\n",
    "        return None\n",
    "\n",
    "    tmp = torch.zeros_like(\n",
    "        boundaries\n",
    "    ).unsqueeze(2) + torch.arange(\n",
    "        start=0,\n",
    "        end=n_segments,\n",
    "        device=boundaries.device\n",
    "    )\n",
    "\n",
    "    hh1 = boundaries.cumsum(1)\n",
    "\n",
    "    if not upsample:\n",
    "        hh1 -= boundaries\n",
    "\n",
    "    foo = tmp - hh1.unsqueeze(-1)\n",
    "\n",
    "    return foo\n",
    "\n",
    "\n",
    "def downsample(boundaries, hidden, null_group):\n",
    "    \"\"\"\n",
    "        Downsampling\n",
    "\n",
    "        - The first element of boundaries tensor is always 0 and doesn't matter\n",
    "        - 1 starts a new group\n",
    "        - We append an extra \"null\" group at the beginning\n",
    "        - We discard last group because it won't be used (in terms of upsampling)\n",
    "\n",
    "        Input:\n",
    "            boundaries: B x L\n",
    "            hidden: L x B x D\n",
    "        Output:\n",
    "            shortened_hidden: S x B x D\n",
    "    \"\"\"\n",
    "\n",
    "    foo = common(boundaries, upsample=False)  # B x L x S\n",
    "\n",
    "    if foo is None:\n",
    "        return null_group.repeat(1, hidden.size(1), 1)\n",
    "    else:\n",
    "        bar = final(foo=foo, upsample=False)  # B x L x S\n",
    "\n",
    "        shortened_hidden = torch.einsum('lbd,bls->sbd', hidden, bar)\n",
    "        shortened_hidden = torch.cat(\n",
    "            [null_group.repeat(1, hidden.size(1), 1), shortened_hidden], dim=0\n",
    "        )\n",
    "\n",
    "        return shortened_hidden\n",
    "\n",
    "\n",
    "def upsample(boundaries, shortened_hidden):\n",
    "    \"\"\"\n",
    "        Upsampling\n",
    "\n",
    "        - The first element of boundaries tensor is always 0 and doesn't matter\n",
    "        - 1 starts a new group\n",
    "        - i-th group can be upsampled only to the tokens from (i+1)-th group, otherwise there's a leak\n",
    "\n",
    "        Input:\n",
    "            boundaries: B x L\n",
    "            shortened_hidden: S x B x D\n",
    "        Output:\n",
    "            upsampled_hidden: L x B x D\n",
    "    \"\"\"\n",
    "\n",
    "    foo = common(boundaries, upsample=True)  # B x L x S\n",
    "    bar = final(foo, upsample=True)  # B x L x S\n",
    "\n",
    "    return torch.einsum('sbd,bls->lbd', shortened_hidden, bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GumbelBoundaryPredictor(\n",
       "  (boundary_predictor): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 3\n",
    "d_inner = 3\n",
    "my_bp = GumbelBoundaryPredictor(d_model=d_model, d_inner=d_inner, activation_function='gelu', temp=1.0, prior=0.5, bp_type='gumbel', threshold=0.5)\n",
    "my_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_hidden_states = torch.randn(10, 1, d_model)\n",
    "# soft_boundaries, hard_boundaries = my_bp(my_hidden_states)\n",
    "# hard_boundaries.shape, soft_boundaries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 1., 1., 1., 0., 1.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hard_boundaries.retain_grad()\n",
    "# hard_boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_hard_boundaries = torch.tensor([[0, 0, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=torch.float32, requires_grad=True)\n",
    "my_hard_boundaries.retain_grad()\n",
    "my_hard_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 4]),\n",
       " tensor([[[0.3333, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3333, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3333, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 1.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.5000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500],\n",
       "          [0.0000, 0.0000, 0.0000, 0.2500]]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = common(my_hard_boundaries, upsample=False)\n",
    "bar = final(foo, upsample=False)\n",
    "bar.shape, bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = bar[0, 6, 3]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 1.]], requires_grad=True),\n",
       " tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1875, -0.1250,\n",
       "          -0.0625,  0.0000]]))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.backward()\n",
    "my_hard_boundaries, my_hard_boundaries.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_values = []\n",
    "\n",
    "for i in range(10):\n",
    "    my_hard_boundaries = torch.tensor([[0, 0, 1, 1, 0, 1, 0, 0, 0, 1]], dtype=torch.float32, requires_grad=True)\n",
    "    my_hard_boundaries.retain_grad()\n",
    "    foo = common(my_hard_boundaries, upsample=False)\n",
    "    bar = final(foo, upsample=False)\n",
    "    l = bar[0, i, :].sum() # We can safely sum as the 0. values in bar are disconnected from the computation graph.count\n",
    "    l.backward()\n",
    "    grad_values.append(my_hard_boundaries.grad)\n",
    "    my_hard_boundaries.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2222, -0.1111,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.1111, -0.1111,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.1111,  0.2222,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.2500,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2500,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1875, -0.1250,\n",
       "         -0.0625,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0625, -0.1250,\n",
       "         -0.0625,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0625,  0.1250,\n",
       "         -0.0625,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0625,  0.1250,\n",
       "          0.1875,  0.0000]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian = torch.cat(grad_values, dim=0)\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2222, -0.1111,  0.0000],\n",
       "         [ 0.1111, -0.1111,  0.0000],\n",
       "         [ 0.1111,  0.2222,  0.0000]]),\n",
       " tensor([[0.]]),\n",
       " tensor([[-0.2500,  0.0000],\n",
       "         [ 0.2500,  0.0000]]),\n",
       " tensor([[-0.1875, -0.1250, -0.0625,  0.0000],\n",
       "         [ 0.0625, -0.1250, -0.0625,  0.0000],\n",
       "         [ 0.0625,  0.1250, -0.0625,  0.0000],\n",
       "         [ 0.0625,  0.1250,  0.1875,  0.0000]]))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian[:3, :3], jacobian[3:4, 3:4], jacobian[4:6, 4:6], jacobian[6:10, 6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boundary_predictor.0.weight: param.grad=tensor([[ 0.0017,  0.0109, -0.0136],\n",
      "        [-0.0021, -0.0129,  0.0162],\n",
      "        [ 0.0012,  0.0077, -0.0096]])\n",
      "boundary_predictor.0.bias: param.grad=tensor([-0.0136,  0.0161, -0.0096])\n",
      "boundary_predictor.2.weight: param.grad=tensor([[-0.0021,  0.0033,  0.0351]])\n",
      "boundary_predictor.2.bias: param.grad=tensor([0.0616])\n"
     ]
    }
   ],
   "source": [
    "for n, param in my_bp.named_parameters():\n",
    "    print(f\"{n}: {param.grad=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_segments = my_hard_boundaries.sum(dim=-1).max().item()\n",
    "\n",
    "n_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.]]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.zeros_like(\n",
    "    my_hard_boundaries\n",
    ").unsqueeze(2) + torch.arange(\n",
    "    start=0,\n",
    "    end=n_segments,\n",
    "    device=my_hard_boundaries.device\n",
    ")\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 2.]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_preceding_boundaries = my_hard_boundaries.cumsum(1)\n",
    "n_preceding_boundaries -= my_hard_boundaries\n",
    "\n",
    "n_preceding_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.],\n",
       "         [ 0.,  1.],\n",
       "         [-1.,  0.],\n",
       "         [-2., -1.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = tmp - n_preceding_boundaries.unsqueeze(-1)\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True],\n",
       "         [False,  True],\n",
       "         [ True, False],\n",
       "         [ True,  True]]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoregressive = foo != 0\n",
    "autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 0.]]], grad_fn=<IndexPutBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lel = 1 - foo\n",
    "lel[autoregressive] = 0\n",
    "lel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.0000],\n",
       "         [0.5000, 0.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 0.0000]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lel = lel / (lel.sum(dim=1, keepdim=True) + 1e-9)\n",
    "lel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
